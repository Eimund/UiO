\documentclass[11pt,english,a4paper]{article}
\include{mitt_oppsett}
\usepackage{fullpage}
\setcounter{MaxMatrixCols}{20}

\renewcommand\title{FYS4150 - Computational Physics - Project 5}

\renewcommand\author{Candidate number: 68}
%\newcommand\adress{}
\renewcommand\date{\today}
\newcommand\email{\href{mailto:eimundsm@fys.uio.no}{eimundsm@fys.uio.no}}

%\lstset{language=[Visual]C++,caption={Descriptive Caption Text},label=DescriptiveLabel}
\lstset{language=c++}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{black}\bfseries}
\lstset{commentstyle=\itshape\color{black}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\lstset{tabsize=2}

\begin{document}
\maketitle
\begin{flushleft}

\begin{abstract}

\end{abstract}

\section{Diffusion of neurotransmitters}

I will study diffusion as a transport process for neurotransmitters  across synaptic cleft separating the cell membrane of two neurons, for more detail see \cite{project4}. The diffusion equation is the partial differential equation 

\begin{align*}
\frac{\partial u\p{\textbf{x},t}}{\partial t} = \nabla \cdot\p{D\p{\textbf{x},t}\boldsymbol{\nabla} u\p{\textbf{x},t}}\,,
\end{align*}

where $u$ is the concentration of particular neurotransmitters at location $\textbf{x}$ and time $t$ with the diffusion coefficient $D$. In this study I consider the diffusion coefficient as constant, which simplify the diffusion equation to the heat equation

\begin{align*}
\frac{\partial u\p{\textbf{x},t}}{\partial t} = D\nabla^2 u\p{\textbf{x},t}\,.
\end{align*}

I will look at the concentration of neurotransmitter $u$ in two dimensions with $x_1$ parallel with the direction between the presynaptic to the postsynaptic across the synaptic cleft, and $x_2$ is parallel with both presynaptic to the postsynaptic. Hence we have the differential equation

\begin{align}
\frac{\partial u\p{\kr{x_i}_{i=1}^2,t}}{\partial t} = D\sum_{j=1}^2\frac{\partial^2 u\p{\kr{x_i}_{i=1}^2,t}}{\partial {x_j}^2}\,,
\label{eq_1}
\end{align}

where $\kr{x_i}_{i=1}^2 = \p{x_1, x_2} = \textbf{x}$. The boundary and initial condition that I'm going to study is 

\begin{align}
&\exists \kr{d, w} \subseteq \mathbb{R}_{0+}\exists\kr{w_i}_{i=1}^2 \subseteq \mathbb{R}_{0+}^{w-}\left(\forall t \in\mathbb{R}_{0}:\forall x_2 \in \mathbb{R}_{w_1}^{w_2} : u\p{0,x_2,t} = u_0  \right.
\nonumber\\
& \quad \wedge \forall  t \in\mathbb{R} \left(\forall x_2\in\mathbb{R}_{0+}^{w-} : u\p{d, x_2, t} = 0 
\wedge \forall x_1\in\mathbb{R}_{0}^d : \left(u\p{x_1,0, t} = 0 \wedge u\p{x_1,w, t} = 0\right) \right)
\nonumber\\
& \quad \left.\wedge
\forall x_1\in\mathbb{R}_{0+}^{d-}\forall x_2\in\mathbb{R}_{0+}^{w-} :u\p{\kr{x_i}_{i=1}^2,0}=0 \wedge \forall x_2 \in \mathbb{R}_0^w \setminus \mathbb{R}_{w_1}^{w_2} : u\p{0,x_2,0} = 0 \right)
\label{eq_2}
\end{align}

where $d$ is the distance between the presynaptic and the postsynaptic, and $w$ is the width of the presynaptic and postsynaptic. Note that the notation $\forall x\in\mathbb{R}_{a+}^{b-} \Leftrightarrow a < x < b$, where as $\forall x\in\mathbb{R}_{a}^{b} \Leftrightarrow a \leq x \leq b$. Note also that these boundary conditions implies that the neurotransmitters are transmitted from presynaptic at $x_1=0$ and $w_1 \leq x_2\leq w_2$ with constant concentration $u_0$; the neurotransmitters are  immediately absorbed at the postsynaptic $x_1=d$; there are no neurotransmitters at boundary width $x_2=0$ and $x_2 = w$ of the synaptic cleft; and we have the initial condition at $t=0$ where there are no neurotransmitters between the pre- and postsynaptic as well on the side of the synaptic vesicles $x_1=0$, $0\leq x_2 < w_1$ and $w_2 < x_2 \leq w$. \linebreak

\figur{0.8}{thompsonB2000-p39.eps}{Left: Schematic drawing of the process of vesicle release from the axon terminal and release of transmitter molecules into the synaptic cleft. (From Thompson: \anf{The Brain}, Worth Publ., 2000). Right: Molecular structure of the two important neurotransmitters glutamate and GABA.}{fig1} 

To solve the differential equation \eqref{eq_1} with the boundary and initial condition \eqref{eq_2} we make an ansatz that the solution is unique, which is the case for a deterministic system. We recognize the heat equation as part of the class of partial differential equation spanned by the Poisson's equation for each time instance. The Uniqueness theorem for the Poisson's equation $\nabla^2 u = f$ \cite{Uniqueness} says that the Poisson's equation has a unique solution with the Dirichlet boundary condition, where Dirichlet boundary condition is here defined as a boundary that specifies the values the solution must have at the boundary.  \linebreak

Unfortunately the boundary condition in \eqref{eq_2} is not a Dirichlet boundary, since the boundary is not specified on the side of the synaptic vesicles $x_1=0$, $0\leq x_2 < w_1$ and $w_2 < x_2 \leq w$, and closer investigation will show that the boundary condition in \eqref{eq_2} does not provide a unique solution. So we need to add further condition to make the solution unique, and I make an assumption that the total concentration $u$ in an infinitesimal area has uniform concentration per length $u_1$ and $u_2$ in each direction $x_1$ and $x_2$ accordingly. Hence $u=u_1 u_2$ at every point and therefore we can write

\begin{align}
\forall x_1 \in \mathbb{R}_0^d \forall x_2 \in \mathbb{R}_0^w \forall t \in \mathbb{R}_{0}: u\p{\kr{x_i}_{i=1}^2,t} = u_1\p{x_1,t} u_2\p{x_2,t} \,.
\label{eq_3}
\end{align}

Putting this into the heat equation \eqref{eq_1} we get

\begin{align*}
u_2\p{x_2,t}\frac{\partial u_1\p{x_1,t}}{\partial t} + u_1\p{x_1,t}\frac{\partial u_2\p{x_2,t}}{\partial t} = D\p{u_2\p{x_2,t}\frac{\partial^2 u_1\p{x_1,t}}{\partial {x_1}^2} + u_1\p{x_1,t}\frac{\partial^2 u_2\p{x_2,t}}{\partial {x_2}^2}} \,,
\end{align*}

which can be written as two heat equations

\begin{align}
\forall i \in \mathbb{N}_1^2 : \frac{\partial u_i\p{x_i,t}}{\partial t}  = D \frac{\partial^2 u_i\p{x_i,t}}{\partial {x_i}^2} \,.
\label{eq_4}
\end{align}

I make another assumption that the the boundary at $u\p{0,x_2,t}$ determined by $u_2\p{x_2,t}$ alone, and by satisfying initial condition $u\p{0,x_2,0}$ for $u_2$

\begin{align}
\forall t \in \mathbb{R}_0 \p{: u_2\p{0,t} = u_2\p{w,t} = 0 \wedge \forall x_2\in\mathbb{R}_{w_1}^{w_2}: u_2\p{x_2,t} = u_0} \wedge \forall x_2 \in\mathbb{R}_{0}^{w} \setminus \mathbb{R}_{w_1}^{w_2}: u_2\p{x_2,0} = 0
\label{eq_5}
\end{align}

we have now determined the values on all the boundaries have therefore Dirichlet boundary condition, and therefore a unique solution of $u$. We found the analytical solution to the heat equation in \eqref{eq_4} for $i=2$ with similar boundary and initial condition, and I will therefore not show the derivation here, but just state the solutions

\begin{align}
u_2\p{x_2,t} = u_0 \begin{cases} \frac{x_2}{w_1} - \sum_{n=1} \frac{2}{n \pi} \sin\p{n\pi\p{1-\frac{x_2}{w_1}}}\exp\p{-D\p{\frac{n\pi}{w_1}}^2 t} &: x_2\in\mathbb{R}_0^{w_1-}\\ 1 &: x_2 \in\mathbb{R}_{w_1}^{w_2} \\  \frac{w-x_2}{w-w_2} - \sum_{n=1} \frac{2}{n \pi} \sin\p{n\pi\frac{x_2-w_2}{w-w_2}}\exp\p{-D\p{\frac{n\pi}{w-w_2}}^2 t} & : x_2\in\mathbb{R}_{w_2+}^w \,.\end{cases}
\label{eq_6}
\end{align}

Since I have established that the boundary at $u\p{0,x_2,t}$ determined by $u_2\p{x_2,t}$ alone, means that $u_1$ has the following boundary and initial condition

\begin{align}
\forall t \in \mathbb{R}_0\p{: u_1\p{0,t} = 1 \wedge u_1\p{d,t} = 0} \wedge  \forall x_1 \in \mathbb{R}_{0+}^{d-} : u_1\p{x_1,0}=0 \,.  
\label{eq_7}
\end{align}

And using the analytical solution from project 4 to heat equation \eqref{eq_4} for $i=1$ with boundary and initial condition in \eqref{eq_7}, we have 

\begin{align}
u_1\p{x_1,t} = 1-\frac{x_1}{d} - \sum_{n=1} \frac{2}{n \pi} \sin\p{n\pi\frac{x_1}{d}}\exp\p{-D\p{\frac{n\pi}{d}}^2 t} \,.
\label{eq_8}
\end{align}

To summarize the solution to the concentration $u$ is given by \eqref{eq_3} with \eqref{eq_6} and \eqref{eq_8}. 


\section{Numerical methods}

\subsection{The $\theta$-rule}

The Taylor expansion is given by

\begin{align}
u\p{x} = \sum_{n=0} \frac{\T{u}{(n)}\p{x_0}}{n!}\p{x-x_0}^n
\label{eq_9}
\end{align}

where $\T{u}{(n)} = \frac{\mathrm{d}^n u}{\mathrm{d}t^n}$ and $x_0$ is a initial value where we step from to $x$ . If we now use the first order approximation

\begin{align*}
u\p{x} \approx u\p{x_0} + \T{u}{(1)}\p{x_0}\p{x-x_0} \,.
\end{align*}

The first order differential equation $\T{u}{(1)}\p{x} = f\p{x}$ is determined when we have the initial condition $u\p{x_0}$, however $\T{u}{(1)}\p{x_0}$ is not an initial condition, and it depends on how we calculate it numerically from the initial condition. Now note that $\T{u}{(1)}\p{x_0}$ is the same for different values of $x$ in the approximation above and lets say that we calculate it as given from the approximation above;

\begin{align}
\T{u}{(1)}\p{x_0} \approx \frac{u\p{x}-u\p{x_0}}{x-x_0}\,.
\label{eq_10}
\end{align}

So now use this in another point $x_{\theta} = \theta x + \p{1-\theta}x_0$ which we also approximate to the first order, and if we use the expression above for $\T{u}{(1)}\p{x_0}$ we get

\begin{align}
u\p{x_\theta} &\approx u\p{x_0} + \T{u}{(1)}\p{x_0}\p{x_{\theta}-x_0} = u\p{x_0} + \theta\,\T{u}{(1)}\p{x_0}\p{x-x_0} 
\nonumber\\
&\approx u\p{x_0} + \frac{u\p{x}-u\p{x_0}}{x-x_0}\theta\p{x-x_0} = \theta u\p{x} + \p{1-\theta}u\p{x_0} \,,
\label{eq_11}
\end{align}

this is known as the $\theta$-rule. The $\theta$-rule can be used to approximate the solution of the following first order differential equation

\begin{align}
\T{u}{(1)}\p{x} = f\p{u\p{x}} \,,
\label{eq_12}
\end{align}

where we use \eqref{eq_10} to approximate the expression $\T{u}{(1)}\p{x}$ and given an even better or worse approximation to the solution $u\p{x}$ by approximating $f\p{x}\approx f\p{x_{\theta}}$;

\begin{align*}
\frac{u\p{x}-u\p{x_0}}{x-x_0} \approx f\p{u\p{x_{\theta}}} = f\p{\theta u\p{x} + \p{1-\theta}u\p{x_0}}\,,
\end{align*}

which discretize to

\begin{align}
\frac{u_{i+1}-u_i}{x_{i+1}-x_i} = f\p{\theta u_{i+1} + \p{1-\theta}u_i} \qquad \text{where } i\in\mathbb{N}_0 \text{ and $u_0$ is an initial contidion.} 
\label{eq_13}
\end{align}

We can find the the next step in the numerical solution to \eqref{eq_12} by solving this difference equation with regard to $u_{i+1}$. Note the above discretization is known as Forward Euler scheme (Explicit) when $\theta = 0$, Backward Euler scheme (Implicit) when $\theta = 1$ and Crank-Nicolson scheme when $\theta = \frac{1}{2}$.

\subsection{Second order derivative}

We approximated the first order derivative in \eqref{eq_10}, but we need to approximate the second order derivative to be able to solve the diffusion in \eqref{eq_1}. I start by expanding the Taylor series in \eqref{eq_9} around the point $x_0\pm\Delta x$ accordingly;

\begin{align}
u\p{x_0\pm\Delta x} = \sum_{n=0} \frac{\T{u}{(n)}\p{x_0}}{n!}\p{\pm\Delta x}^n\,.
\label{eq_14}
\end{align}

Now adding these two expansions

\begin{align*}
u\p{x_0+\Delta x} + u\p{x_0-\Delta x} = 2\sum_{n=0} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2n} = 2 u\p{x_0} + \T{u}{(2)}\p{x_0}\Delta x^2 + 2\sum_{n=2} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2n}\,,
\end{align*}

and solve it with

\begin{align*}
\T{u}{(2)}\p{x_0} &= \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2} - 2\sum_{n=2} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2(n-1)}
\\ &=  \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2} + \mathcal{O}\p{\Delta x^2}\,,
\end{align*}

So the second order derivative can be approximated with

\begin{align}
\T{u}{(2)}\p{x_0} \approx   \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2}
\label{eq_15}
\end{align}

with the local truncation error $\mathcal{O}\p{\Delta x^2}$.

\subsection{The heat equation} \label{heat_equation}

We want to discretize the dimensionless heat equation from \eqref{eq_1}, where we use $D=1$, $u_0=1$ and $d=1$, 

\begin{align*}
\frac{\partial u\p{\kr{x_i}_{i=1}^2,t}}{\partial t} = \sum_{\ell=1}^2\frac{\partial^2 u\p{\kr{x_i}_{i=1}^2,t}}{\partial {x_\ell}^2}\,,
\end{align*}

to numerically solve diffusion of neurotransmitters. First we do the $\theta$-rule discretization in \eqref{eq_13} 

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^2}-u_{i\kr{j_k}_{k=1}^2}}{\Delta t} 
&= \sum_{\ell = 1}^2\frac{\partial^2 u_{(i+\theta)\kr{j_k}_{k=1}^2}}{\partial {x_\ell}^2} 
= \sum_{\ell=1}^2\frac{\partial^2 \p{\theta u_{(i+1)\kr{j_k}_{k=1}^2}+\p{1-\theta}u_{i\kr{j_k}_{k=1}^2}}}{\partial {x_\ell}^2} 
\\
&= \sum_{\ell=1}^2\p{\theta \frac{\partial u_{(i+1)\kr{j_k}_{k=1}^2}}{\partial {x_\ell}^2} + \p{1-\theta}\frac{\partial u_{i\kr{j_k}_{k=1}^2}}{\partial {x_\ell}^2}}
\end{align*}

where index $i$ is stepping of $t$ and $j_k$ are stepping of $x_k$. Note also that the following notation expand accordingly $u_{i\kr{j_k}_{k=1}^2} = u_{ij_1 j_2}$, which becomes a more elegant notation for larger $n$ in $u_{i\kr{j_k}_{k=1}^n} = u_{ij_1j_2\ldots j_n}$. Now we implement the discretization of the second order in \eqref{eq_15}

\begin{align}
\frac{u_{(i+1)\kr{j_k}_{k=1}^2}-u_{i\kr{j_k}_{k=1}^2}}{\Delta t} =& \sum_{\ell=1}^2 \left(\frac{\theta}{\Delta {x_\ell}^2}\p{u_{(i+1)\kr{j_k+\delta_{k\ell}}_{k=1}^2} - 2 u_{(i+1)\kr{j_k}_{k=1}^2} + u_{(i+1)\kr{j_k-\delta_{k\ell}}_{k=1}^2}} \right.
\nonumber\\
&\quad \left. + \frac{1-\theta}{\Delta {x_\ell}^2}\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} - 2u_{i\kr{j_k}_{k=1}^2} +u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}}\right) \,,
\label{eq_16}
\end{align}

where $\delta_{k\ell}$ is the Kronecker delta, and we now clearly see the elegance of the notation $u_{i\kr{j_k}_{k=1}^n}$. \linebreak

The dimensionless initial condition from \eqref{eq_4} gives us

\begin{align*}
u_{0\kr{j_k}_{k=1}^2} = \begin{cases} 1 & : j_1 = 0 \text{ and } x_{2 j_2} \in \mathbb{R}_{w_1}^{w_2} \\ 0 & :\text{elsewhere,} \end{cases} 
\end{align*}

where $x_{2 j_2} = x_{2 0}  + \frac{j_2}{\Delta x_2}$. For the explicit scheme $\theta = 0$ we get

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^2} = u_{i\kr{j_k}_{k=1}^2} + \sum_{\ell=1}^2 \alpha_\ell\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} - 2u_{i\kr{j_k}_{k=1}^2} +u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}}
\label{eq_17}
\end{align}

where

\begin{align*}
\alpha_\ell = \frac{\Delta t}{\Delta {x_\ell}^2} \qquad \text{and} \qquad n_\ell = \frac{1}{\Delta x_\ell} \,.
\end{align*}

\subsection{Jacobi's iterative method}

When we have $\theta\neq 0$ in \eqref{eq_16} we have implicit schemes, and I rewrite \eqref{eq_16} with unknowns on the left side and knowns on the right side;

\begin{align*}
\forall k\in\mathbb{N}_1^2 \forall j_k \in \mathbb{N}_1^{n_k-2} : &\p{1+2\theta\sum_{\ell=1}^2 \alpha_\ell} u_{(i+1)\kr{j_k}_{k=1}^2} - \theta\sum_{\ell = 1}^2\alpha_\ell \p{u_{(i+1)\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{(i+1)\kr{j_k-\delta_{k\ell}}_{k=1}^2}} 
\\
&\quad = \p{1-2\p{1-\theta}\sum_{\ell = 1}^2 \alpha_\ell} u_{i\kr{j_k}_{k=1}^2} + \p{1-\theta}\sum_{\ell=1}^2\alpha_\ell\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}} \,,
\end{align*}

where the boundary are given at the indices $j_k=0$ and $j_k= n_k-1$ where $n_k$ are the number of points i $k$ direction. Now I will transform this into a linear algebra problem $\textbf{A}_{i+1}\textbf{v}_{i+1} = \textbf{b}_i$, where elements of $\textbf{v}_{i+1}$ are

\begin{align}
v_{(i+1)j} = v_{(i+1)(j_1 n_2 + j_2)} = u_{(i+1)\kr{j_k}_{k=1}^2} \,.
\label{eq_18}
\end{align}

The fixed boundary values with $u_0=1$ at the presynaptic $x_1=0$ and $w_1 \leq x_2 \leq w_2$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_1 = \mathbb{N}_{m_1}^{m_2}\,.
\end{align*}

We have non-fixed values at the boundary on the side of the synaptic vesicles at the presynaptic $x_1=0$, $0 \leq x_2 < w_1$ and $w_2 < x_2 < w$, and are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_2 = j\in\mathbb{N}_0^{n_2-1} \setminus \mathbb{B}_1 \,.
\end{align*}

The boundary values at the postsynaptic $x_1=d$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_3 = \mathbb{N}_{n_1 n_2 - n_2}^{n_1 n_2 - 1} \,.
\end{align*}

The left side boundary $x_2=0$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_4 = \kr{\ell n_2}_{\ell=1}^{n_1-2} \,.
\end{align*}

The right side boundary $x_2=w$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_5 = \kr{\ell n_2 - 1}_{\ell=2}^{n_1-1}\,.
\end{align*}

And the boundary as a whole are given by the indices $j$ in the set

\begin{align*}
\mathbb{B} = \bigcup_{i=1}^5 \mathbb{B}_i \,.
\end{align*}

The inner points are given by the indices $j$ in the set

\begin{align*}
\mathbb{I} = \mathbb{N}_{0}^{n_1 n_2 -1} \setminus \mathbb{B}\,.
\end{align*}

We can now write the matrix elements of $\textbf{A}_{(i+1)}$

\begin{align}
a_{(i+1)jk} = a_{jk} = \begin{cases} 
1 &:\forall j\in\mathbb{B}\setminus\mathbb{B}_2  : j=k \,\,\,\,\quad\qquad\qquad \text{(fixed boundary)}\\
1+ 2\theta \alpha_2 &: \forall j\in\mathbb{B}_2: j=k \,\,\quad\qquad\qquad\qquad (\text{side of vesicles}) \\ 
1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell &: \forall j\in\mathbb{I}: j=k \qquad\qquad\qquad\qquad (\text{the inner points }u_{(i+1)j_1j_2}) \\ 
-\theta \alpha_2 &: \forall j\in\mathbb{I}\cup\mathbb{B}_2: k\in\kr{j-1,j+1}\quad \text{(diffusion in $x_2$ direction)} \\ 
- \theta \alpha_1 &: \forall j\in\mathbb{I} : k \in \kr{j-n_2,j+n_2} \,\,\qquad\text{(diffusion in $x_1$ direction)} \\
 0 & : \text{otherwise,} \end{cases}
\label{eq_19}
\end{align}

and the elements of the vector $\textbf{b}_i$ are

\begin{align}
b_{ij} = b_{i(j_1 n_2 + j_2)} = \begin{cases}
1 &: j\in\mathbb{B}_1 \\
\p{1-2\p{1-\theta}\alpha_2} u_{i0j_2} + \p{1-\theta}\alpha_2\p{u_{i0(j_2+1)} + u_{i0(j_2-1)}} &: j\in\mathbb{B}_2 \\
\p{1-2\p{1-\theta}\sum_{\ell = 1}^2 \alpha_\ell} u_{i\kr{j_k}_{k=1}^2} + \p{1-\theta}\sum_{\ell=1}^2\alpha_\ell\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}} &: j\in\mathbb{I} \\
0 &: \text{otherwise.}
\end{cases}
\label{eq_20}
\end{align}

Unfortunately  is the linear algebra problem $\textbf{A}\textbf{v}_{i+1} = \textbf{b}_i$ is computational expensive to solve for the matrix in \eqref{eq_19}. To lower the computational time I therefore prepare the matrix $\textbf{A}$ for Jacobi's iterative method by splitting it into a diagonal matrix $\textbf{D}$ and a remainder matrix $\textbf{R}$;

\begin{align*}
\textbf{A} = \textbf{D} + \textbf{R} \,.
\end{align*}

From \eqref{eq_19} we see that the elements of the diagonal matrix are given by

\begin{align}
d_{jk} = \begin{cases} 
1 &:\forall j\in\mathbb{B}\setminus\mathbb{B}_2  : j=k \,\,\,\,\quad\qquad\qquad \text{(fixed boundary)}\\
1+ 2\theta \alpha_2 &: \forall j\in\mathbb{B}_2: j=k \,\,\quad\qquad\qquad\qquad (\text{side of vesicles}) \\ 
1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell &: \forall j\in\mathbb{I}: j=k \qquad\qquad\qquad\qquad (\text{the inner points }u_{(i+1)j_1j_2}) \\ 
 0 & : \text{otherwise,} \end{cases}
\label{eq_21}
\end{align}

and the elements of the remainder matrix are given by

\begin{align}
r_{jk} = \begin{cases} 
-\theta \alpha_2 &: \forall j\in\mathbb{I}\cup\mathbb{B}_2: k\in\kr{j-1,j+1}\quad \text{(diffusion in $x_2$ direction)} \\ 
- \theta \alpha_1 &: \forall j\in\mathbb{I} : k \in \kr{j-n_2,j+n_2} \,\,\qquad\text{(diffusion in $x_1$ direction)} \\
 0 & : \text{otherwise.} \end{cases}
\label{eq_22}
\end{align}

We can now introduce Jacobi's iterative method

\begin{align*}
\textbf{v}_{i+1}^{(\ell)} = \begin{cases}
\textbf{v}_i &: \ell=0 \\
\textbf{D}^{-1}\p{\textbf{b}_i-\textbf{R}\textbf{v}_{i+1}^{(\ell-1)}} &: \ell\in\mathbb{N}_1
\end{cases}
\end{align*}

where $\ell$ is the number of iterations and $\ell=0$ are the starting point. Rewriting it on element form yields

\begin{align*}
v_{(i+1)j}^{(\ell)} = \begin{cases} v_j &: \ell = 0 \\ \frac{1}{d_{jj}}\p{b_j - \sum_{k\neq j} r_{jk} v_{(i+1)k}^{(\ell-1)}} &: j\in\mathbb{N}_1 \,.\end{cases}
\end{align*}

Using \eqref{eq_18} and \eqref{eq_22} we have the iterative solution to the heat equation in \eqref{eq_6} with the boundaries in \eqref{eq_2}

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^2}^{(\ell)} = \begin{cases}
u_{i\kr{j_k}_{k=1}^2} &: \ell = 0 \\
1 &: j_1=0 \wedge j_2\in\mathbb{N}_{m_1}^{m_2} \\
c_0 + c_1\p{u_{(i+1)0(j_2+1)}^{(\ell-1)}+u_{(i+1)0(j_2-1)}^{(\ell-1)}} &: j_1=0 \wedge j_2\in\mathbb{N}_0^{n_2-1} \setminus\mathbb{N}_{m_1}^{m_2} \\
c_2 + \sum_{o=1}^2 c_{o+2}\p{u_{(i+1)\kr{j_k+\delta_{ko}}_{k=1}^2}^{(\ell-1)}+u_{(i+1)\kr{j_k-\delta_{ko}}_{k=1}^2}^{(\ell-1)}} &: j_1\in \mathbb{N}_{1}^{n_1-2} \wedge j_2\in\mathbb{N}_1^{n_2-2} 
\end{cases} 
\label{eq_23}
\end{align}

\begin{align}
c_0 &= c_5 u_{i0j_2} + c_6\p{u_{i0(j_2+1)} + u_{i0(j_2-1)}}
\label{eq_24}\\
c_1 &= \frac{\theta \alpha_2}{1+2\theta\alpha_2}
\label{eq_25}\\
c_2 &= c_7 u_{i\kr{j_k}_{k=1}^2} + \sum_{\ell=1}^2 c_{\ell+7} \p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}}
\label{eq_26}\\
c_3 &= \frac{\theta \alpha_1}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_27}\\
c_4 &= \frac{\theta \alpha_2}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_28}\\
c_5 &= \frac{1-2\p{1-\theta}\alpha_2}{1+2\theta \alpha_2} 
\label{eq_29}\\
c_6 &= \frac{\p{1-\theta}\alpha_2}{1+2\theta \alpha_2} 
\label{eq_30}\\
c_7 &= \frac{1-2\p{1-\theta}\sum_{\ell = 1}^2 \alpha_\ell}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_31}\\
c_8 &= \frac{\p{1-\theta}\alpha_1}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_32}\\
c_9 &= \frac{\p{1-\theta}\alpha_2}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_33}
\end{align}

The Jacobi iterative method converges when the matrix $\textbf{A}$ is diagonally dominant 

\begin{align*}
\abs{a_{ii}} > \sum_{j\neq i} \abs{a_{ij}}\,,
\end{align*}

which is the case for the matrix spanned out by \eqref{eq_19} because $1+2\theta \alpha_2 > 2\theta \alpha_2$ and $1+2\theta\sum_{\ell=1}^2 \alpha_{\ell} > 2\theta\sum_{\ell=1}^2 \alpha_\ell$.

\section{Markov chain}

The Markov chain is given by

\begin{align}
w_{(i+1)\kr{j_k}_{k=1}^n}^+ = \sum_{\kr{\ell_k}_{k=1}^n} W_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} w_{i\kr{\ell_k}_{k=1}^n} \,,
\label{eq_34}
\end{align}

where $w_{i\kr{j_k}_{k=1}^n}$ is the probability distribution function \anf{PDF} at time step $i$, and $W_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n}$  is the transition probability from state $\kr{\ell_k}_{k=1}^n$ to $\kr{j_k}_{k=1}^n$. And the reverse Markov chain is given by

\begin{align}
w_{(i+1)\kr{j_k}_{k=1}^n}^- = \sum_{\kr{\ell_k}_{k=1}^n} W_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} w_{i\kr{j_k}_{k=1}^n} \,,
\label{eq_35}
\end{align}

Diffusion can be modelled by Markov chain where the normalized concentration at time step $i+1$ should be given by the difference of transition to and from the state $\kr{j_k}_{k=1}^n$ plus the amount that stays in the state; 

\begin{align*}
u_{(i+1)\p{j_k}_{k=1}^n} = W_{\kr{j_k}_k\kr{j_k}_{k=1}^n} w_{i\kr{j_k}_{k=1}^n} +  w_{(i+1)\kr{j_k}_{k=1}^n}^+ - w_{(i+1)\kr{j_k}_{k=1}^n}^-
\end{align*} 

We assume that one can't stay in the same state $W_{\kr{j_k}_{k=1}^n\kr{j_k}_{k=1}^n} = 0$, hence all of the concentration should transition to other states at any given time step $i+1$, which means that the normalized concentration are equal to the reverse Markov chain in the next time step

\begin{align*}
u_{i\kr{j_k}_{k=1}^n} =  w_{(i+1)\kr{j_k}_{k=1}^n}^- \,.
\end{align*}

This assumption yields

\begin{align*}
u_{(i+1)\p{j_k}_{k=1}^n} = w_{(i+1)\kr{j_k}_{k=1}^n}^+ - u_{i\kr{j_k}_{k=1}^n} \,.
\end{align*}

Further we assume that the probability $p$ are equal to transition to neighbouring states, and zero to transition beyond the neighbouring states

\begin{align*}
\sum_{\ell=1}^n \p{W_{\kr{j_k}_{k=1}^n\kr{j_k + \delta_{k\ell}}_{k=1}^n} + W_{\kr{j_k}_{k=1}^n}\kr{j_k - \delta_{k\ell}}_{k=1}^n} = 2n p = 1 \,.
\end{align*}

Then the normalized concentration is then given by

\begin{align*}
u_{(i+1)\p{j_k}_{k=1}^n} = \frac{1}{2n}\sum_{\ell=1}^n\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^n} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^n}} - u_{i\kr{j_k}_{k=1}^n} \,,
\end{align*}

which can be rewritten to

\begin{align}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} = \frac{\Delta x^2}{2n\Delta t}\sum_{\ell=1}^n\frac{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^n}}{\Delta x^2} \,,
\label{eq_36}
\end{align}

which is the discretization of the heat equation in \eqref{eq_1} with diffusion constant

\begin{align}
\Delta x = \sqrt{2nD\Delta t}\,. 
\label{eq_37}
\end{align}


\section{Attachments}

The source files developed are


\section{Resources}

\begin{enumerate}
\item{\href{http://qt-project.org/downloads}{QT Creator 5.3.1 with C11}}
\item{\href{https://www.eclipse.org/downloads/}{Eclipse Standard/SDK  - Version: Luna Release (4.4.0) with PyDev for Python}}
\item{\href{http://www.ubuntu.com/download/desktop}{Ubuntu 14.04.1 LTS}}
\item{\href{http://shop.lenovo.com/no/en/laptops/thinkpad/w-series/w540/#tab-reseller}{ThinkPad W540 P/N: 20BG0042MN with 32 GB RAM}}
\end{enumerate}

\begin{thebibliography}{1}
\bibitem{project4}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/projects/project-5-deadline-december-1/project5_diffusion.pdf}{\emph{FYS4150 - Project 5}} - \emph{Diffusion in two dimensions}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{lecture}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/Lecture\%20Notes/lecture2014.pdf}{\emph{Computational Physics - Lecture Notes Fall 2014}}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{Farnell}
Farnell and Gibson, \href{https://vpn2.uio.no/+CSCO+00756767633A2F2F6A6A6A2E667076726170727176657270672E70627A++/science/article/pii/S0021999105001087#}
{\emph{Monte Carlo simulation of diffusion in a spatially
nonhomogeneous medium:}} A biased random walk
on an asymmetrical lattice, Journal of Computational Physics, volume 208, pages 253-265, 2005
\bibitem{Diffusion}\href{http://en.wikipedia.org/wiki/Diffusion\_equation}{http://en.wikipedia.org/wiki/Diffusion\_equation}
\bibitem{Heat}\href{http://en.wikipedia.org/wiki/Heat\_equation}{http://en.wikipedia.org/wiki/Heat\_equation}
\bibitem{Dirichlet}\href{http://en.wikipedia.org/wiki/Dirichlet\_boundary\_condition}{http://en.wikipedia.org/wiki/Dirichlet\_boundary\_condition}
\bibitem{Poisson_eq}\href{http://en.wikipedia.org/wiki/Poisson\%27s\_equation}{http://en.wikipedia.org/wiki/Poisson\%27s\_equation}
\bibitem{Uniqueness}\href{http://en.wikipedia.org/wiki/Uniqueness\_theorem\_for\_Poisson\%27s\_equation}{http://en.wikipedia.org/wiki/Uniqueness\_theorem\_for\_Poisson\%27s\_equation}
\bibitem{Ansatz}\href{http://en.wikipedia.org/wiki/Ansatz}{http://en.wikipedia.org/wiki/Ansatz}
\bibitem{Kronecker}\href{http://en.wikipedia.org/wiki/Kronecker\_delta}{http://en.wikipedia.org/wiki/Kronecker\_delta}
\bibitem{Jacobi}\href{http://en.wikipedia.org/wiki/Jacobi\_method}{http://en.wikipedia.org/wiki/Jacobi\_method}
\bibitem{Markov}\href{http://en.wikipedia.org/wiki/Markov\_chain}{http://en.wikipedia.org/wiki/Markov\_chain}
\end{thebibliography}

\end{flushleft}
\end{document}
