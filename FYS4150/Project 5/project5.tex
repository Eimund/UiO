\documentclass[11pt,english,a4paper]{article}
\include{mitt_oppsett}
\usepackage{fullpage}
\setcounter{MaxMatrixCols}{20}

\renewcommand\title{FYS4150 - Computational Physics - Project 5}

\renewcommand\author{Eimund Smestad  - Candidate number: 68}
%\newcommand\adress{}
\renewcommand\date{\today}
\newcommand\email{\href{mailto:eimundsm@fys.uio.no}{eimundsm@fys.uio.no}}

%\lstset{language=[Visual]C++,caption={Descriptive Caption Text},label=DescriptiveLabel}
\lstset{language=c++}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{black}\bfseries}
\lstset{commentstyle=\itshape\color{black}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\lstset{tabsize=2}

\begin{document}
\maketitle
\begin{flushleft}

\begin{abstract}

\end{abstract}

\section{Diffusion of neurotransmitters}

I will study diffusion as a transport process for neurotransmitters  across synaptic cleft separating the cell membrane of two neurons, for more detail see \cite{project4}. The diffusion equation is the partial differential equation 

\begin{align*}
\frac{\partial u\p{\textbf{x},t}}{\partial t} = \nabla \cdot\p{D\p{\textbf{x},t}\boldsymbol{\nabla} u\p{\textbf{x},t}}\,,
\end{align*}

where $u$ is the concentration of particular neurotransmitters at location $\textbf{x}$ and time $t$ with the diffusion coefficient $D$. In this study I consider the diffusion coefficient as constant, which simplify the diffusion equation to the heat equation

\begin{align*}
\frac{\partial u\p{\textbf{x},t}}{\partial t} = D\nabla^2 u\p{\textbf{x},t}\,.
\end{align*}

I will look at the concentration of neurotransmitter $u$ in two dimensions with $x_1$ parallel with the direction between the presynaptic to the postsynaptic across the synaptic cleft, and $x_2$ is parallel with both presynaptic to the postsynaptic. Hence we have the differential equation

\begin{align}
\frac{\partial u\p{\kr{x_i}_{i=1}^2,t}}{\partial t} = D\sum_{j=1}^2\frac{\partial^2 u\p{\kr{x_i}_{i=1}^2,t}}{\partial {x_j}^2}\,,
\label{eq_1}
\end{align}

where $\kr{x_i}_{i=1}^2 = \p{x_1, x_2} = \textbf{x}$. The boundary and initial condition that I'm going to study is 

\begin{align}
&\exists \kr{d, w} \subseteq \mathbb{R}_{0+}\exists\kr{w_i}_{i=1}^2 \subseteq \mathbb{R}_{0+}^{w-}\left(\forall t \in\mathbb{R}_{0}:\forall x_2 \in \mathbb{R}_{w_1}^{w_2} : u\p{0,x_2,t} = u_0  \right.
\nonumber\\
& \quad \wedge \forall  t \in\mathbb{R} \left(\forall x_2\in\mathbb{R}_{0+}^{w-} : u\p{d, x_2, t} = 0 
\wedge \forall x_1\in\mathbb{R}_{0}^d : \left(u\p{x_1,0, t} = 0 \wedge u\p{x_1,w, t} = 0\right) \right)
\nonumber\\
& \quad \left.\wedge
\forall x_1\in\mathbb{R}_{0+}^{d-}\forall x_2\in\mathbb{R}_{0+}^{w-} :u\p{\kr{x_i}_{i=1}^2,0}=0 \wedge \forall x_2 \in \mathbb{R}_0^w \setminus \mathbb{R}_{w_1}^{w_2} : u\p{0,x_2,0} = 0 \right)
\label{eq_2}
\end{align}

where $d$ is the distance between the presynaptic and the postsynaptic, and $w$ is the width of the presynaptic and postsynaptic. Note that the notation $\forall x\in\mathbb{R}_{a+}^{b-} \Leftrightarrow a < x < b$, where as $\forall x\in\mathbb{R}_{a}^{b} \Leftrightarrow a \leq x \leq b$. Note also that these boundary conditions implies that the neurotransmitters are transmitted from presynaptic at $x_1=0$ and $w_1 \leq x_2\leq w_2$ with constant concentration $u_0$; the neurotransmitters are  immediately absorbed at the postsynaptic $x_1=d$; there are no neurotransmitters at boundary width $x_2=0$ and $x_2 = w$ of the synaptic cleft; and we have the initial condition at $t=0$ where there are no neurotransmitters between the pre- and postsynaptic as well on the side of the synaptic vesicles $x_1=0$, $0\leq x_2 < w_1$ and $w_2 < x_2 \leq w$. \linebreak

\figur{0.8}{thompsonB2000-p39.eps}{Left: Schematic drawing of the process of vesicle release from the axon terminal and release of transmitter molecules into the synaptic cleft. (From Thompson: \anf{The Brain}, Worth Publ., 2000). Right: Molecular structure of the two important neurotransmitters glutamate and GABA.}{fig1} 

To solve the differential equation \eqref{eq_1} with the boundary and initial condition \eqref{eq_2} we make an ansatz that the solution is unique, which is the case for a deterministic system. We recognize the heat equation as part of the class of partial differential equations spanned by the Poisson's equation for each time instance. The Uniqueness theorem for the Poisson's equation $\nabla^2 u = f$ \cite{Uniqueness} says that the Poisson's equation has a unique solution with the Dirichlet boundary condition, where Dirichlet boundary condition is here defined as a boundary that specifies the values the solution must have at the boundary. Not to be confused with Dirichlet boundary condition with zero at the boundary, which is just a special case and I will refer to it as 0th Dirichlet boundary condition. \linebreak

Unfortunately the boundary condition in \eqref{eq_2} is not a Dirichlet boundary, since the boundary is not specified on the side of the synaptic vesicles $x_1=0$, $0\leq x_2 < w_1$ and $w_2 < x_2 \leq w$, and closer investigation will show that the boundary condition in \eqref{eq_2} does not provide a unique solution. So we need to add further condition to make the solution unique, and I make an assumption that the total concentration $u$ in an infinitesimal area has uniform concentration per length $u_1$ and $u_2$ in each direction $x_1$ and $x_2$ accordingly. Hence $u=u_1 u_2$ at every point and therefore we can write

\begin{align}
\forall x_1 \in \mathbb{R}_0^d \forall x_2 \in \mathbb{R}_0^w \forall t \in \mathbb{R}_{0}: u\p{\kr{x_i}_{i=1}^2,t} = u_1\p{x_1,t} u_2\p{x_2,t} \,.
\label{eq_3}
\end{align}

Putting this into the heat equation \eqref{eq_1} we get

\begin{align*}
u_2\p{x_2,t}\frac{\partial u_1\p{x_1,t}}{\partial t} + u_1\p{x_1,t}\frac{\partial u_2\p{x_2,t}}{\partial t} = D\p{u_2\p{x_2,t}\frac{\partial^2 u_1\p{x_1,t}}{\partial {x_1}^2} + u_1\p{x_1,t}\frac{\partial^2 u_2\p{x_2,t}}{\partial {x_2}^2}} \,,
\end{align*}

which can be written as two heat equations

\begin{align}
\forall i \in \mathbb{N}_1^2 : \frac{\partial u_i\p{x_i,t}}{\partial t}  = D \frac{\partial^2 u_i\p{x_i,t}}{\partial {x_i}^2} \,.
\label{eq_4}
\end{align}

I make another ansatz that the the boundary at $u\p{0,x_2,t}$ determined by $u_2\p{x_2,t}$ alone, and by satisfying initial condition $u\p{0,x_2,0}$ for $u_2$

\begin{align}
\forall t \in \mathbb{R}_0 \p{: u_2\p{0,t} = u_2\p{w,t} = 0 \wedge \forall x_2\in\mathbb{R}_{w_1}^{w_2}: u_2\p{x_2,t} = u_0} \wedge \forall x_2 \in\mathbb{R}_{0}^{w} \setminus \mathbb{R}_{w_1}^{w_2}: u_2\p{x_2,0} = 0
\label{eq_5}
\end{align}

we have now determined the values on all the boundaries have therefore Dirichlet boundary condition, and therefore a unique solution of $u$. We found the analytical solution to the heat equation in \eqref{eq_4} for $i=2$ with similar boundary and initial condition in project 4 \cite{project4}, and I will therefore not show the derivation here, but just state the solutions

\begin{align}
u_2\p{x_2,t} = u_0 \begin{cases} \frac{x_2}{w_1} - \sum_{n=1} \frac{2}{n \pi} \sin\p{n\pi\p{1-\frac{x_2}{w_1}}}\exp\p{-D\p{\frac{n\pi}{w_1}}^2 t} &: x_2\in\mathbb{R}_0^{w_1-}\\ 1 &: x_2 \in\mathbb{R}_{w_1}^{w_2} \\  \frac{w-x_2}{w-w_2} - \sum_{n=1} \frac{2}{n \pi} \sin\p{n\pi\frac{x_2-w_2}{w-w_2}}\exp\p{-D\p{\frac{n\pi}{w-w_2}}^2 t} & : x_2\in\mathbb{R}_{w_2+}^w \,.\end{cases}
\label{eq_6}
\end{align}

Since I have established that the boundary at $u\p{0,x_2,t}$ determined by $u_2\p{x_2,t}$ alone, means that $u_1$ has the following boundary and initial condition

\begin{align}
\forall t \in \mathbb{R}_0\p{: u_1\p{0,t} = 1 \wedge u_1\p{d,t} = 0} \wedge  \forall x_1 \in \mathbb{R}_{0+}^{d-} : u_1\p{x_1,0}=0 \,.  
\label{eq_7}
\end{align}

And using the analytical solution from project 4 to heat equation \eqref{eq_4} for $i=1$ with boundary and initial condition in \eqref{eq_7}, we have 

\begin{align}
u_1\p{x_1,t} = 1-\frac{x_1}{d} - \sum_{n=1} \frac{2}{n \pi} \sin\p{n\pi\frac{x_1}{d}}\exp\p{-D\p{\frac{n\pi}{d}}^2 t} \,.
\label{eq_8}
\end{align}

To summarize the solution to the concentration $u$ is given by \eqref{eq_3} with \eqref{eq_6} and \eqref{eq_8}. 


\section{Numerical methods}

\subsection{The $\theta$-rule}

The Taylor expansion is given by

\begin{align}
u\p{x} = \sum_{n=0} \frac{\T{u}{(n)}\p{x_0}}{n!}\p{x-x_0}^n
\label{eq_9}
\end{align}

where $\T{u}{(n)} = \frac{\mathrm{d}^n u}{\mathrm{d}t^n}$ and $x_0$ is a initial value where we step from to $x$ . If we now use the first order approximation

\begin{align*}
u\p{x} \approx u\p{x_0} + \T{u}{(1)}\p{x_0}\p{x-x_0} \,.
\end{align*}

The first order differential equation $\T{u}{(1)}\p{x} = f\p{x}$ is determined when we have the initial condition $u\p{x_0}$, however $\T{u}{(1)}\p{x_0}$ is not an initial condition, and it depends on how we calculate it numerically from the initial condition. Now note that $\T{u}{(1)}\p{x_0}$ is the same for different values of $x$ in the approximation above and lets say that we calculate it as given from the approximation above;

\begin{align}
\T{u}{(1)}\p{x_0} \approx \frac{u\p{x}-u\p{x_0}}{x-x_0}\,.
\label{eq_10}
\end{align}

So now use this in another point $x_{\theta} = \theta x + \p{1-\theta}x_0$ which we also approximate to the first order, and if we use the expression above for $\T{u}{(1)}\p{x_0}$ we get

\begin{align}
u\p{x_\theta} &\approx u\p{x_0} + \T{u}{(1)}\p{x_0}\p{x_{\theta}-x_0} = u\p{x_0} + \theta\,\T{u}{(1)}\p{x_0}\p{x-x_0} 
\nonumber\\
&\approx u\p{x_0} + \frac{u\p{x}-u\p{x_0}}{x-x_0}\theta\p{x-x_0} = \theta u\p{x} + \p{1-\theta}u\p{x_0} \,,
\label{eq_11}
\end{align}

this is known as the $\theta$-rule. The $\theta$-rule can be used to approximate the solution of the following first order differential equation

\begin{align}
\T{u}{(1)}\p{x} = f\p{u\p{x}} \,,
\label{eq_12}
\end{align}

where we use \eqref{eq_10} to approximate the expression $\T{u}{(1)}\p{x}$ and given an even better or worse approximation to the solution $u\p{x}$ by approximating $f\p{x}\approx f\p{x_{\theta}}$;

\begin{align*}
\frac{u\p{x}-u\p{x_0}}{x-x_0} \approx f\p{u\p{x_{\theta}}} = f\p{\theta u\p{x} + \p{1-\theta}u\p{x_0}}\,,
\end{align*}

which discretize to

\begin{align}
\frac{u_{i+1}-u_i}{x_{i+1}-x_i} = f\p{\theta u_{i+1} + \p{1-\theta}u_i} \qquad \text{where } i\in\mathbb{N}_0 \text{ and $u_0$ is an initial contidion.} 
\label{eq_13}
\end{align}

We can find the the next step in the numerical solution to \eqref{eq_12} by solving this difference equation with regard to $u_{i+1}$. Note the above discretization is known as Forward Euler scheme (Explicit) when $\theta = 0$, Backward Euler scheme (Implicit) when $\theta = 1$ and Crank-Nicolson scheme when $\theta = \frac{1}{2}$.

\subsection{Second order derivative}

We approximated the first order derivative in \eqref{eq_10}, but we need to approximate the second order derivative to be able to solve the diffusion in \eqref{eq_1}. I start by expanding the Taylor series in \eqref{eq_9} around the point $x_0\pm\Delta x$ accordingly;

\begin{align}
u\p{x_0\pm\Delta x} = \sum_{n=0} \frac{\T{u}{(n)}\p{x_0}}{n!}\p{\pm\Delta x}^n\,.
\label{eq_14}
\end{align}

Now adding these two expansions

\begin{align*}
u\p{x_0+\Delta x} + u\p{x_0-\Delta x} = 2\sum_{n=0} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2n} = 2 u\p{x_0} + \T{u}{(2)}\p{x_0}\Delta x^2 + 2\sum_{n=2} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2n}\,,
\end{align*}

and solve it with

\begin{align*}
\T{u}{(2)}\p{x_0} &= \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2} - 2\sum_{n=2} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2(n-1)}
\\ &=  \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2} + \mathcal{O}\p{\Delta x^2}\,,
\end{align*}

So the second order derivative can be approximated with

\begin{align}
\T{u}{(2)}\p{x_0} \approx   \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2}
\label{eq_15}
\end{align}

with the local truncation error $\mathcal{O}\p{\Delta x^2}$.

\subsection{The heat equation} \label{heat_equation}

We want to discretize the dimensionless heat equation from \eqref{eq_1}, where we use $D=1$, $u_0=1$ and $d=1$, 

\begin{align*}
\frac{\partial u\p{\kr{x_i}_{i=1}^2,t}}{\partial t} = \sum_{\ell=1}^2\frac{\partial^2 u\p{\kr{x_i}_{i=1}^2,t}}{\partial {x_\ell}^2}\,,
\end{align*}

to numerically solve diffusion of neurotransmitters. First we do the $\theta$-rule discretization in \eqref{eq_13} 

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^2}-u_{i\kr{j_k}_{k=1}^2}}{\Delta t} 
&= \sum_{\ell = 1}^2\frac{\partial^2 u_{(i+\theta)\kr{j_k}_{k=1}^2}}{\partial {x_\ell}^2} 
= \sum_{\ell=1}^2\frac{\partial^2 \p{\theta u_{(i+1)\kr{j_k}_{k=1}^2}+\p{1-\theta}u_{i\kr{j_k}_{k=1}^2}}}{\partial {x_\ell}^2} 
\\
&= \sum_{\ell=1}^2\p{\theta \frac{\partial u_{(i+1)\kr{j_k}_{k=1}^2}}{\partial {x_\ell}^2} + \p{1-\theta}\frac{\partial u_{i\kr{j_k}_{k=1}^2}}{\partial {x_\ell}^2}}
\end{align*}

where index $i$ is stepping of $t$ and $j_k$ are stepping of $x_k$. Note also that the following notation expand accordingly $u_{i\kr{j_k}_{k=1}^2} = u_{ij_1 j_2}$, which becomes a more elegant notation for larger $n$ in $u_{i\kr{j_k}_{k=1}^n} = u_{ij_1j_2\ldots j_n}$. Now we implement the discretization of the second order in \eqref{eq_15}

\begin{align}
\frac{u_{(i+1)\kr{j_k}_{k=1}^2}-u_{i\kr{j_k}_{k=1}^2}}{\Delta t} =& \sum_{\ell=1}^2 \left(\frac{\theta}{\Delta {x_\ell}^2}\p{u_{(i+1)\kr{j_k+\delta_{k\ell}}_{k=1}^2} - 2 u_{(i+1)\kr{j_k}_{k=1}^2} + u_{(i+1)\kr{j_k-\delta_{k\ell}}_{k=1}^2}} \right.
\nonumber\\
&\quad \left. + \frac{1-\theta}{\Delta {x_\ell}^2}\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} - 2u_{i\kr{j_k}_{k=1}^2} +u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}}\right) \,,
\label{eq_16}
\end{align}

where $\delta_{k\ell}$ is the Kronecker delta, and we now clearly see the elegance of the notation $u_{i\kr{j_k}_{k=1}^n}$. \linebreak

The dimensionless initial condition from \eqref{eq_4} gives us

\begin{align*}
u_{0\kr{j_k}_{k=1}^2} = \begin{cases} 1 & : j_1 = 0 \text{ and } x_{2 j_2} \in \mathbb{R}_{w_1}^{w_2} \\ 0 & :\text{elsewhere,} \end{cases} 
\end{align*}

where $x_{2 j_2} = x_{2 0}  + \frac{j_2}{\Delta x_2}$. For the explicit scheme $\theta = 0$ we get

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^2} = u_{i\kr{j_k}_{k=1}^2} + \sum_{\ell=1}^2 \alpha_\ell\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} - 2u_{i\kr{j_k}_{k=1}^2} +u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}}
\label{eq_17}
\end{align}

where

\begin{align*}
\alpha_\ell = \frac{\Delta t}{\Delta {x_\ell}^2} \qquad \text{and} \qquad n_\ell = \frac{1}{\Delta x_\ell} \,,
\end{align*}

where $n_\ell$ is number of grid points in $\ell$ direction. 

\subsubsection{Stability and convergence} \label{sec_explicit_stab_conv}

According to Lax equivalence theorem a consistent finite difference method for well-posed  (a solution exists, unique and continuous) linear initial value problem, the method is convergent if and only if stable. Which means that we only can show either stability and convergence to show both. \linebreak

There is a theorem that states that a matrix $\textbf{A}$ converges $\lim_{i\to\infty} \textbf{A}^i = 0$ if and only if the spectral radius $\rho\p{\textbf{A}} < 1$. The spectral radius is defined as

\begin{align*}
\rho\p{\textbf{A}} = \max_m \abs{\lambda_m} \,,
\end{align*}

where $\lambda_m$ are the eigenvalues of $\textbf{A}$. The explicit scheme in \eqref{eq_17}  can be written as $\textbf{u}_{i+1} = \textbf{A}\textbf{u}_i = \textbf{A}^{i+1}\textbf{u}_0$, and the this matrix can be written as

\begin{align*}
\textbf{A} = \textbf{I} - \textbf{B}
\end{align*}

where $\textbf{I}$ is the identity matrix. This implies that the eigenvalues $\mu_m$ of $\textbf{B}$ is related to the eigenvalues $\lambda_m$ of $\textbf{A}$ as follows

\begin{align}
\lambda_m = 1 - \mu_m\,.
\label{eq_18}
\end{align}

The eigenvalues $\mu_m$ can be found from the eigenequation \eqref{eq_17}

\begin{align}
\sum_{\ell=1}^2\alpha_\ell\p{-u_{i\kr{j_k + \delta_{k\ell}}_{k=1}^2} + 2 u_{i\kr{j_k}_{k=1}^2}-u_{i\kr{j_k - \delta_{k\ell}}_{k=1}^2}} = \mu_m u_{i\kr{j_k}_{k=1}^2} \,.
\label{eq_19}
\end{align}

Using the expression for$\alpha_\ell = \frac{\Delta t}{\Delta {x_\ell}^2}$ we have the discretization of the following equation

\begin{align*}
u\p{\kr{x_i}_{i=1}^2,t} = -{\omega_{m}}^2 \sum_{\ell=1}^2 \frac{\partial^2 u\p{\kr{x_i}_{i=1}^2,t}}{\partial {x_\ell}^2}\,,
\end{align*}

which has the solution 

\begin{align*}
u\p{\kr{x_i}_{i=1}^2, t} = \sum_{\ell=1}^2 A_\ell\p{t} \prod_{\ell=1}^2 \sin\p{\omega_{m} x_\ell + \varphi_\ell}
\end{align*}

where 

\begin{align*}
\omega_{m} = \sqrt{\frac{\Delta t}{\mu_m}}\,.
\end{align*}

Now using this solution into the eigenequation \eqref{eq_19} we get

\begin{align*}
&\mu_m \sum_{\ell=1}^2\sin\p{\omega_{m} x_{\ell j_\ell} + \varphi_\ell}
\\
&\quad =
\sum_{\ell=1}^2 \alpha_\ell\p{-\sin\p{\omega_{m} \p{x_{\ell j_\ell}+\Delta x_\ell} + \varphi_\ell} + 2\sin\p{\omega_{m} x_{\ell j_\ell} + \varphi_\ell} - \sin\p{\omega_{m} \p{x_{\ell j_\ell}-\Delta x_\ell} + \varphi_\ell}}
\end{align*} 

which can be simplified with the trigonometrical relation $\sin\p{u+v} = \sin u \cos v + \cos u \sin v$
 
\begin{align}
\mu_m\sum_{\ell=1}^2\sin\p{\omega_m x_\ell + \varphi_\ell} = 2\sum_{\ell=1}^2 \alpha_\ell\p{1-\cos\p{\omega_{m} \Delta x_\ell}}\sin\p{\omega_m x_\ell + \varphi_\ell} \,,
\label{eq_20}
\end{align}

note that $\mu_m\neq 0$ because of $\omega_{m} \propto \frac{1}{\sqrt{\mu_m}}$. For this equation to be an eigeneqution we need to satisfy

\begin{align*}
\alpha_1 \p{1-\cos\p{\omega_m \Delta x_1}} = \alpha_2\p{1-\cos\p{\omega_m \Delta x_2}}\,.
\end{align*}

If we have square lattice $\alpha_1 = \alpha_2$ the above relation is automatically satisfied, if not we need to solve the equation with regard to $\omega_m$, and from that find the eigenvalue $\mu_m$ from the relation $\omega_m = \sqrt{\frac{\Delta t}{\mu_m}}$. Since the approach with $\alpha_1 \neq \alpha_2$ is difficult to solve analytically I will show the further steps with square lattice $\alpha = \alpha_1 = \alpha_2$, and in the case of $\alpha_1 \neq \alpha_2$ one can approximate to a square lattice problem with the smallest step size of $\Delta x_\ell$. If you forfill the convergence criteria for the a square lattice with the smallest step size, then you are guaranteed to satisfy the convergence criteria in the non-square lattice case as well. \linebreak

For the square lattice with $\alpha$ we see from \eqref{eq_20} that the eigenvalue $\mu_m$ must be within

\begin{align*}
0 \leq \mu_m \leq 2\alpha \,.
\end{align*}


Inserting these eigenvalues into \eqref{eq_18} and using the convergence condition $\rho\p{\textbf{A}} < 1$ yields the following inequality

\begin{align*}
-1 < 1 - 2\alpha < 1
\end{align*}

which result in the constraint for convergences

\begin{align*}
\alpha < \frac{1}{2} \,,
\end{align*}

where the $\alpha$ is always positive. So when we want to solve the heat equation explicitly we satisfy the convergence criteria if we chose a time step accordingly

\begin{align}
\Delta t < \min_\ell \frac{1}{{n_\ell}^2} \,.
\label{eq_21}
\end{align}

\subsection{Jacobi's iterative method}

When we have $\theta\neq 0$ in \eqref{eq_16} we have implicit schemes, and I rewrite \eqref{eq_16} with unknowns on the left side and knowns on the right side;

\begin{align*}
\forall k\in\mathbb{N}_1^2 \forall j_k \in \mathbb{N}_1^{n_k-2} : &\p{1+2\theta\sum_{\ell=1}^2 \alpha_\ell} u_{(i+1)\kr{j_k}_{k=1}^2} - \theta\sum_{\ell = 1}^2\alpha_\ell \p{u_{(i+1)\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{(i+1)\kr{j_k-\delta_{k\ell}}_{k=1}^2}} 
\\
&\quad = \p{1-2\p{1-\theta}\sum_{\ell = 1}^2 \alpha_\ell} u_{i\kr{j_k}_{k=1}^2} + \p{1-\theta}\sum_{\ell=1}^2\alpha_\ell\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}} \,,
\end{align*}

where the boundary are given at the indices $j_k=0$ and $j_k= n_k-1$ where $n_k$ are the number of points i $k$ direction. Now I will transform this into a linear algebra problem $\textbf{A}_{i+1}\textbf{v}_{i+1} = \textbf{b}_i$, where elements of $\textbf{v}_{i+1}$ are

\begin{align}
v_{(i+1)j} = v_{(i+1)(j_1 n_2 + j_2)} = u_{(i+1)\kr{j_k}_{k=1}^2} \,.
\label{eq_22}
\end{align}

The fixed boundary values with $u_0=1$ at the presynaptic $x_1=0$ and $w_1 \leq x_2 \leq w_2$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_1 = \mathbb{N}_{m_1}^{m_2}\,.
\end{align*}

We have non-fixed values at the boundary on the side of the synaptic vesicles at the presynaptic $x_1=0$, $0 \leq x_2 < w_1$ and $w_2 < x_2 < w$, and are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_2 = j\in\mathbb{N}_0^{n_2-1} \setminus \mathbb{B}_1 \,.
\end{align*}

The boundary values at the postsynaptic $x_1=d$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_3 = \mathbb{N}_{n_1 n_2 - n_2}^{n_1 n_2 - 1} \,.
\end{align*}

The left side boundary $x_2=0$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_4 = \kr{\ell n_2}_{\ell=1}^{n_1-2} \,.
\end{align*}

The right side boundary $x_2=w$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_5 = \kr{\ell n_2 - 1}_{\ell=2}^{n_1-1}\,.
\end{align*}

And the boundary as a whole are given by the indices $j$ in the set

\begin{align*}
\mathbb{B} = \bigcup_{i=1}^5 \mathbb{B}_i \,.
\end{align*}

The inner points are given by the indices $j$ in the set

\begin{align*}
\mathbb{I} = \mathbb{N}_{0}^{n_1 n_2 -1} \setminus \mathbb{B}\,.
\end{align*}

We can now write the matrix elements of $\textbf{A}_{(i+1)}$

\begin{align}
a_{(i+1)jk} = a_{jk} = \begin{cases} 
1 &:\forall j\in\mathbb{B}\setminus\mathbb{B}_2  : j=k \,\,\,\,\quad\qquad\qquad \text{(fixed boundary)}\\
1+ 2\theta \alpha_2 &: \forall j\in\mathbb{B}_2: j=k \,\,\quad\qquad\qquad\qquad (\text{side of vesicles}) \\ 
1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell &: \forall j\in\mathbb{I}: j=k \qquad\qquad\qquad\qquad (\text{the inner points }u_{(i+1)j_1j_2}) \\ 
-\theta \alpha_2 &: \forall j\in\mathbb{I}\cup\mathbb{B}_2: k\in\kr{j-1,j+1}\quad \text{(diffusion in $x_2$ direction)} \\ 
- \theta \alpha_1 &: \forall j\in\mathbb{I} : k \in \kr{j-n_2,j+n_2} \,\,\qquad\text{(diffusion in $x_1$ direction)} \\
 0 & : \text{otherwise,} \end{cases}
\label{eq_23}
\end{align}

and the elements of the vector $\textbf{b}_i$ are

\begin{align}
b_{ij} = b_{i(j_1 n_2 + j_2)} = \begin{cases}
1 &: j\in\mathbb{B}_1 \\
\p{1-2\p{1-\theta}\alpha_2} u_{i0j_2} + \p{1-\theta}\alpha_2\p{u_{i0(j_2+1)} + u_{i0(j_2-1)}} &: j\in\mathbb{B}_2 \\
\p{1-2\p{1-\theta}\sum_{\ell = 1}^2 \alpha_\ell} u_{i\kr{j_k}_{k=1}^2} + \p{1-\theta}\sum_{\ell=1}^2\alpha_\ell\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}} &: j\in\mathbb{I} \\
0 &: \text{otherwise.}
\end{cases}
\label{eq_24}
\end{align}

Unfortunately  is the linear algebra problem $\textbf{A}\textbf{v}_{i+1} = \textbf{b}_i$ is computational expensive to solve for the matrix in \eqref{eq_23}. To lower the computational time I therefore prepare the matrix $\textbf{A}$ for Jacobi's iterative method by splitting it into a diagonal matrix $\textbf{D}$ and a remainder matrix $\textbf{R}$;

\begin{align*}
\textbf{A} = \textbf{D} + \textbf{R} \,.
\end{align*}

From \eqref{eq_23} we see that the elements of the diagonal matrix are given by

\begin{align}
d_{jk} = \begin{cases} 
1 &:\forall j\in\mathbb{B}\setminus\mathbb{B}_2  : j=k \,\,\,\,\quad\qquad\qquad \text{(fixed boundary)}\\
1+ 2\theta \alpha_2 &: \forall j\in\mathbb{B}_2: j=k \,\,\quad\qquad\qquad\qquad (\text{side of vesicles}) \\ 
1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell &: \forall j\in\mathbb{I}: j=k \qquad\qquad\qquad\qquad (\text{the inner points }u_{(i+1)j_1j_2}) \\ 
 0 & : \text{otherwise,} \end{cases}
\label{eq_25}
\end{align}

and the elements of the remainder matrix are given by

\begin{align}
r_{jk} = \begin{cases} 
-\theta \alpha_2 &: \forall j\in\mathbb{I}\cup\mathbb{B}_2: k\in\kr{j-1,j+1}\quad \text{(diffusion in $x_2$ direction)} \\ 
- \theta \alpha_1 &: \forall j\in\mathbb{I} : k \in \kr{j-n_2,j+n_2} \,\,\qquad\text{(diffusion in $x_1$ direction)} \\
 0 & : \text{otherwise.} \end{cases}
\label{eq_26}
\end{align}

We can now introduce Jacobi's iterative method

\begin{align*}
\textbf{v}_{i+1}^{(\ell)} = \begin{cases}
\textbf{v}_i &: \ell=0 \\
\textbf{D}^{-1}\p{\textbf{b}_i-\textbf{R}\textbf{v}_{i+1}^{(\ell-1)}} &: \ell\in\mathbb{N}_1
\end{cases}
\end{align*}

where $\ell$ is the number of iterations and $\ell=0$ are the starting point. Rewriting it on element form yields

\begin{align*}
v_{(i+1)j}^{(\ell)} = \begin{cases} v_j &: \ell = 0 \\ \frac{1}{d_{jj}}\p{b_j - \sum_{k\neq j} r_{jk} v_{(i+1)k}^{(\ell-1)}} &: j\in\mathbb{N}_1 \,.\end{cases}
\end{align*}

Using \eqref{eq_22} and \eqref{eq_26} we have the iterative solution to the heat equation in \eqref{eq_6} with the boundaries in \eqref{eq_2}

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^2}^{(\ell)} = \begin{cases}
u_{i\kr{j_k}_{k=1}^2} &: \ell = 0 \\
1 &: j_1=0 \wedge j_2\in\mathbb{N}_{m_1}^{m_2} \\
c_0 + c_1\p{u_{(i+1)0(j_2+1)}^{(\ell-1)}+u_{(i+1)0(j_2-1)}^{(\ell-1)}} &: j_1=0 \wedge j_2\in\mathbb{N}_0^{n_2-1} \setminus\mathbb{N}_{m_1}^{m_2} \\
c_2 + \sum_{o=1}^2 c_{o+2}\p{u_{(i+1)\kr{j_k+\delta_{ko}}_{k=1}^2}^{(\ell-1)}+u_{(i+1)\kr{j_k-\delta_{ko}}_{k=1}^2}^{(\ell-1)}} &: j_1\in \mathbb{N}_{1}^{n_1-2} \wedge j_2\in\mathbb{N}_1^{n_2-2} 
\end{cases} 
\label{eq_27}
\end{align}

\begin{align}
c_0 &= c_5 u_{i0j_2} + c_6\p{u_{i0(j_2+1)} + u_{i0(j_2-1)}}
\label{eq_28}\\
c_1 &= \frac{\theta \alpha_2}{1+2\theta\alpha_2}
\label{eq_29}\\
c_2 &= c_7 u_{i\kr{j_k}_{k=1}^2} + \sum_{\ell=1}^2 c_{\ell+7} \p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}}
\label{eq_30}\\
c_3 &= \frac{\theta \alpha_1}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_31}\\
c_4 &= \frac{\theta \alpha_2}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_32}\\
c_5 &= \frac{1-2\p{1-\theta}\alpha_2}{1+2\theta \alpha_2} 
\label{eq_33}\\
c_6 &= \frac{\p{1-\theta}\alpha_2}{1+2\theta \alpha_2} 
\label{eq_34}\\
c_7 &= \frac{1-2\p{1-\theta}\sum_{\ell = 1}^2 \alpha_\ell}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_35}\\
c_8 &= \frac{\p{1-\theta}\alpha_1}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_36}\\
c_9 &= \frac{\p{1-\theta}\alpha_2}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_37}
\end{align}

\subsubsection{Stability and convergence}

The same procedure as discussed for convergence and stability for the explicit scheme applies for the Jacobi method as well, but now with the spectral radius $\rho\p{\textbf{D}^{-1} \textbf{R}} < 1$, because the Jacobi method are given by 

\begin{align*}
\textbf{v}_{i+1}^{\p{\ell}} = \textbf{D}^{-1}\p{\textbf{b}_i - \textbf{R}\textbf{v}_{i+1}^{\p{\ell-1}}} = \textbf{D}^{-1}\sum_{j=0}^{\ell-1} \p{-\textbf{D}^{-1}\textbf{R}}^j \textbf{b}_i + \p{-\textbf{D}^{-1}\textbf{R}}^\ell\textbf{v}_{i+1}^{(0)} \,.
\end{align*}

Similar steps as shown in section \ref{sec_explicit_stab_conv} it can be shown that the always converges for our diffusion problem. More generally it can be shown that the Jacobi iterative method converges when the matrix $\textbf{A} = \textbf{D}+ \textbf{R}$ is diagonally dominant 

\begin{align*}
\abs{a_{ii}} > \sum_{j\neq i} \abs{a_{ij}}\,,
\end{align*}

which is the case for the matrix spanned out by \eqref{eq_23} because $1+2\theta \alpha_2 > 2\theta \alpha_2$ and $1+2\theta\sum_{\ell=1}^2 \alpha_{\ell} > 2\theta\sum_{\ell=1}^2 \alpha_\ell$.

\subsection{Markov chains}

I assume that diffusion is a memoryless physical process that approximates to a stochastic process, hence diffusion is assumed to satisfy the Markov property and therefore is a Markov process where the transition between states are described by Markov chains 

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n}^+ = \sum_{\kr{\ell_k}_{k=1}^n} W_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} u_{i\kr{\ell_k}_{k=1}^n} \,,
\label{eq_38}
\end{align}

where $u_{i\kr{j_k}_{k=1}^n}$ is the probability distribution function \anf{PDF} at time step $i$, and $W_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n}$  is the transition probability from state $\kr{\ell_k}_{k=1}^n$ to $\kr{j_k}_{k=1}^n$. And the reverse Markov chain is given by

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n}^- = \sum_{\kr{\ell_k}_{k=1}^n} W_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} u_{i\kr{j_k}_{k=1}^n} \,,
\label{eq_39}
\end{align}

Using the assumption that diffusion can be modelled as Markov process then we can use a Master equation to describe the normalized concentration at time step $i+1$ by the difference of transition to and from the state $\kr{j_k}_{k=1}^n$ plus the amount that was in the state; 

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n} = u_{i\kr{j_k}_{k=1}^n} +  u_{(i+1)\kr{j_k}_{k=1}^n}^+ - u_{(i+1)\kr{j_k}_{k=1}^n}^-
\label{eq_40}
\end{align} 

Note that the reverse Markov chain is all the transitions that a state $\kr{j_k}_{k=1}$ takes, including transition to itself. Therefore the reverse Markov chain in time step $i+1$ describes all the transitions that the state does at time step $i$;

\begin{align*}
u_{i\kr{j_k}_{k=1}^n} =  u_{(i+1)\kr{j_k}_{k=1}^n}^- \,,
\end{align*}


which yields

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n} = u_{(i+1)\kr{j_k}_{k=1}^n}^+ \,.
\label{eq_41}
\end{align}

Further, diffusion is assumed to be approximated as stochastic move in distance and direction, where distance and direction is stochastically independent. Which means that transition probability can be written as

\begin{align}
W_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2} = T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2} A_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2} \,,
\label{eq_42}
\end{align}

where $T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2}$ is the probability of moving the distance between the states $\kr{j_k}_{k=1}^n$ and $\kr{\ell_k}_{k=1}^2$, and $ A_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2}$ is the probability of moving in the direction between the states $\kr{j_k}_{k=1}^n$ and $\kr{\ell_k}_{k=1}^2$. \linebreak

Assume that we move a distance $\ell$ from state $\kr{j_k}_{k=1}^n$, which means that the states $\kr{j_k+\delta_{km}\ell}$ are the possible destinations, which puts the following constraint on the direction probability $A$;

\begin{align}
\sum_{m=1}^n \p{A_{\kr{j_k+\delta_{km}\ell}_{k=1}^n\kr{j_k}_{k=1}^n}+A_{\kr{j_k-\delta_{km}\ell}_{k=1}^n\kr{j_k}_{k=1}^n}} = 1 \,.
\label{eq_43}
\end{align}

Now assuming isotropic diffusion, which means that

\begin{align*}
\forall m,o\in \mathbb{N}_1^n : A_{\kr{j_k\pm\delta_{km} \ell}_{k=1}^n\kr{j_k}_{k=1}^n} &= A_{\kr{j_k\pm\delta_{ko} \ell}_{k=1}^n\kr{j_k}_{k=1}^n} \\
\forall m,o\in \mathbb{N}_1^n : T_{\kr{j_k\pm\delta_{km} \ell}_{k=1}^n\kr{j_k}_{k=1}^n} &= T_{\kr{j_k\pm\delta_{ko} \ell}_{k=1}^n\kr{j_k}_{k=1}^n}
\end{align*}

and the constraint on the direction probability then yields

\begin{align}
A = A_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} = \frac{1}{2n}\,,
\label{eq_44}
\end{align}

and the distance probability $T$ is only dependent on the distance $\ell$

\begin{align*}
p_\ell = T_{\kr{j_k\pm\delta_{km} \ell}_{k=1}^n\kr{j_k}_{k=1}^n}\,,
\end{align*}

which has the constraint

\begin{align*}
\sum_{\ell} p_\ell = 1\,.
\end{align*}

Then the normalized concentration \eqref{eq_41} is now given by

\begin{align*}
u_{(i+1)\kr{j_k}_{k=1}^n} = \frac{1}{2n}\sum_\ell p_\ell\sum_{m=1}^n \p{u_{i\kr{j_k+\delta_{km}\ell}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}\ell}_{k=1}^n}} \,,
\end{align*}

which can be rewritten to

\begin{align}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} = \sum_{\ell}\frac{p_\ell \p{\ell \Delta x}^2}{2n\Delta t}\sum_{m=1}^n\frac{u_{i\kr{j_k+\delta_{km}\ell}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}\ell}_{k=1}^n}}{\p{\ell\Delta x}^2} \,.
\label{eq_45}
\end{align}

When we have the Kronecker delta distribution $p_\ell = \delta_{1\ell}$ this becomes the discretization of the heat equation in \eqref{eq_1} 

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} = \frac{\Delta x^2}{2n\Delta t}\sum_{m=1}^n\frac{u_{i\kr{j_k+\delta_{km}}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}}_{k=1}^n}}{\Delta x^2} \,,
\end{align*}

with the relation to the diffusion constant $D$ as follows

\begin{align}
\Delta x = \sqrt{2nD\Delta t}\,. 
\label{eq_46}
\end{align}

For a general distribution $p_\ell$ we see from \eqref{eq_45} that the step length now is given by

\begin{align}
\Delta x_\ell = \ell \Delta x = \ell\sqrt{2nD\Delta t}\,.
\label{eq_47}
\end{align} 

If we use a random generator to make a proposition to move at each time step, we get a deterministic running time with approximation in the result, which is known as Monte Carlo algorithm. 

\subsubsection{Metropolis-Hastings algorithm}

The steps above with Markov chains are designed to give a approximated time evolution. However if we are only interested in the steady state solution, we can derive a faster algorithm to reach it, which is the Metropolis-Hastings algorithm. To derive the Metropolis-Hastings algorithm we start by looking back at the Master equation in \eqref{eq_40} for equilibrium, where we realize that transition to and from the state $\kr{j_k}_{k=1}^n$ must be equal

\begin{align*}
\lim_{i\to \infty} \p{u_{(i+1)\kr{j_k}_{k=1}^n}^+ - u_{(i+1)\kr{j_k}_{k=1}^n}^-} = 0 \,.
\end{align*}

Using the equality of transition to and from the state $\kr{j_k}_{k=1}^n$ at equilibrium,  the Markov chains in \eqref{eq_38} and \eqref{eq_39}, with proposed transition distribution $T$ which is independent of the accepted $A$, must satisfy the following equation

\begin{align*}
\sum_{\kr{\ell_k}_{k=1}^n}\p{T_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n}A_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n} u_{i\kr{\ell_k}_{k=1}^n} - T_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n}A_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n} u_{i\kr{j_k}_{k=1}^n}} = 0 \,,
\end{align*}


which is satisfied by the relation

\begin{align*}
\frac{A_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n}}{A_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n}} = \frac{T_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n}u_{i\kr{j_k}_{k=1}^n}}{T_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n}u_{i\kr{\ell_k}_{k=1}^n}} \,,
\end{align*}

where $T$ is a distribution that we propose and $A$ is a probability to accept the proposition. We reach equilibrium faster when the transition from states are as large as possible, which means $A_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n}$ should be as close to 1 as possible and still satisfy that $A_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n} \leq 1$, which is the Metropolis-Hastings algorithm;

\begin{align}
A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} = \min\p{1,\frac{T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n}u_{i\kr{\ell_k}_{k=1}^n}}{T_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n}u_{i\kr{j_k}_{k=1}^n}}}\,,
\label{eq_48}
\end{align}

where the two states change accordingly to a Master equation when we transition from $\kr{j_k}_{k=1}^2$ to $\kr{\ell_k}_{k=1}^2$ 

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n} &=  T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} u_{i\kr{\ell_k}_{k=1}^n} + \p{1-T_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n}} u_{i\kr{j_k}_{k=1}^n}
\label{eq_49}\\
u_{(i+1)\kr{\ell_k}_{k=1}^n} &= T_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} u_{i\kr{j_k}_{k=1}^n} + \p{1-T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n}} u_{i\kr{\ell_k}_{k=1}^n} \,.
\label{eq_50}
\end{align} 

\subsubsection{Wall boundary}

The Markov chain give insight into how to treat a boundary that is a wall. A wall is a boundary that does not allow transitions beyond it. If we look at direction probability $A$ in \eqref{eq_43} and assume that our wall sets $A_{\kr{j_k-\delta_{kp}}_{k=1}^n\kr{j_k}_{k=1}^n}=0$ we get

\begin{align*}
A_{\kr{j_k+\delta_{kp}\ell}_{k=1}^n\kr{j_k}_{k=1}^n} + \sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n \p{A_{\kr{j_k+\delta_{km}\ell}_{k=1}^n\kr{j_k}_{k=1}^n}+A_{\kr{j_k-\delta_{km}\ell}_{k=1}^n\kr{j_k}_{k=1}^n}} = 1 \,,
\end{align*}

and imposing isotropy the direction probability at the wall becomes

\begin{align*}
A = A_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} = \frac{1}{2n-1} \,.
\end{align*}

Now our Master equation becomes

\begin{align*}
u_{(i+1)\kr{j_k}_{k=1}^n} = \frac{1}{2n-1}\sum_\ell p_\ell \p{u_{i\kr{j_k+\delta_{kp}\ell}_{k=1}^n}+\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n \p{u_{i\kr{j_k+\delta_{km}\ell}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}\ell}_{k=1}^n}}}
\end{align*}

which can be rewritten to 

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} =& \sum_{\ell}\frac{p_\ell \p{\ell \Delta x}^2}{\p{2n-1}\Delta t}\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n\frac{u_{i\kr{j_k+\delta_{km}\ell}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}\ell}_{k=1}^n}}{\p{\ell\Delta x}^2} 
\\
&\quad +\sum_{\ell}\frac{p_\ell\ell \Delta x}{\p{2n-1}\Delta t} \frac{u_{i\kr{j_k+\delta_{kp}\ell}_{k=1}^n} - u_{i\kr{j_k}_{k=1}^n}}{\ell\Delta x} \,.
\end{align*}

When we have the Kronecker delta distribution $p_\ell = \delta_{1\ell}$ this becomes the following discretization

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} =& \frac{\Delta x^2}{\p{2n-1}\Delta t}\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n\frac{u_{i\kr{j_k+\delta_{km}}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}}_{k=1}^n}}{\Delta x^2} 
\\&\quad + \frac{\Delta x}{\p{2n-1}\Delta t} \frac{u_{i\kr{j_k+\delta_{kp}\ell}_{k=1}^n} - u_{i\kr{j_k}_{k=1}^n}}{\Delta x} \,.
\end{align*}

Relating to the diffusion constant $D$ gives

\begin{align}
\Delta x = \sqrt{\p{2n-1}D\Delta t} \,,
\label{eq_51}
\end{align}

and in general

\begin{align}
\Delta x_\ell = \ell \Delta x = \ell\sqrt{\p{2n-1}D\Delta t}
\label{eq_52}
\end{align}

which results in the discretization

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} =& D\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n\frac{u_{i\kr{j_k+\delta_{km}}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}}_{k=1}^n}}{\Delta x^2} 
\\&\quad + \frac{D}{\Delta x} \frac{u_{i\kr{j_k+\delta_{kp}}_{k=1}^n} - u_{i\kr{j_k}_{k=1}^n}}{\Delta x} \,.
\end{align*}

To avoid that the above equation diverges for smaller an smaller step sizes, we realize that the we must satisfy the following the partial differential equation at the wall boundary

\begin{align}
\frac{\partial u\p{\kr{x_i}_{i=1}^n, t}}{\partial t} = D\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n \frac{\partial^2 u\p{\kr{x_i}_{i=1}^n, t}}{\partial {x_m}^2} \qquad \text{and} \qquad \frac{\partial u\p{\kr{x_i}_{i=1}^n, t}}{\partial x_p} = 0 \,,
\label{eq_53}
\end{align}

and we have proven the ansatz that I made in the analytical derivation where the boundary $u\p{0,x_2,t}$ is determined by $u_2\p{x_2,t}$ which yielded the solution on the boundary as in \eqref{eq_6}.

\section{Implementation}

\section{Results}

Order of relative error of $\varv$ to $u$ is calculated by

\begin{align*}
\epsilon = \log_{10}\abs{\frac{\varv - u}{u}} \,.
\end{align*}

\figur{1}{MonteCarlo1D_t0.01.eps}{Monte Carlo simulations for 1D diffusion problem, plotted in 100 lattice. Uniform step is of size $\Delta x$ \eqref{eq_46}, and Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle. Uniform distribution requires smaller time step than the Gaussian distribution to give a good approximation. The problem with uniform distribution is that it leaves grid points empty which should not be empty when the plot grid is smaller than the step size of the simulation. This is not a problem for Gaussian distribution.}{fig1}

\begin{tabell}{|r|c|c|c|}{\small}{$N$ & MCU 1e-3 & MCU 1e-5 & MCG 1e-4 \\}{\input{build-project5-Desktop-Debug/MonteCarlo_1D_time_t0.01.dat}}{Calculation time for the Monte Carlo 1D in seconds for $t=0.01$. MCU = Monte Carlo Uniform, MCG = Monte Carlo Gaussian.}{tab1}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{$N$ & MCU 1e-3 & MCU 1e-5 & MCG 1e-4 \\}{\input{build-project5-Desktop-Debug/MonteCarlo_1D_error_t0.01.dat}}{Order of relative error for the Monte Carlo 1D at $t=0.01$. MCU = Monte Carlo Uniform, MCG = Monte Carlo Gaussian. Relative error of calculated and exact value less than $10^{-2}$ are excluded.}{tab2}
\end{tabell}

\figur{1}{MonteCarlo1D_t1.eps}{Monte Carlo simulations and Metropolis simulation for 1D diffusion problem at equilibrium, plotted in 100 lattice. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle. 1 Metropolis cycle is a loop forward and backward on all the grid points.}{fig2}

\begin{tabell}{|r|cIr|c|}{\small}{$N$ & MCG 1e-3 & $N$ & Metropolis \\}{\input{build-project5-Desktop-Debug/MonteCarlo_1D_time_t1.dat}}{Calculation time for 1D diffusion in seconds for $t=1$. MCG = Monte Carlo Gaussian.}{tab3}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{$N$ & MCG 1e-3 & $N$ & Metropolis \\}{\input{build-project5-Desktop-Debug/MonteCarlo_1D_error_t1.dat}}{Order of relative error for 1D diffusion at $t=1$. MCG = Monte Carlo Gaussian. Relative error of calculated and exact value less than $10^{-1}$ are excluded.}{tab4}
\end{tabell}

\figur{1}{Exact_2D_t0.01_w(0.1,9.9).eps}{Exact solution at $t=0.01$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Which corresponds well with 1D dimensional case in \reffig{fig1}.}{fig3}

\figur{1}{Explicit_2D_t0.01_w(0.1,9.9)_alpha0.49.eps}{Explicit solution in 100x100 lattice at $t=0.01$ with $\alpha = 0.49$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$.}{fig4}

\figur{1}{Explicit_2D_t0.01_w(0.1,9.9)_alpha0.51.eps}{Explicit solution in 100x100 lattice at $t=0.01$ with $\alpha = 0.51$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Instability in the solution because of the convergence criteria is not meet.}{fig5}

\figur{1}{Jacobi_2D_t0.01_w(0.1,9.9)_theta1_alpha100.eps}{Jacobi solution in 100x100 lattice at $t=0.01$ with $\theta=1$ and $\alpha = 100$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$.}{fig6}

\figur{1}{MonteCarlo_2D_t0.01_w(0.1,9.9)_N1.eps}{1 Monte Carlo Gaussian experiment in 100x100 lattice at $t=0.01$ with time step $10^{-4}$ for 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig7}

\figur{1}{MonteCarlo_2D_t0.01_w(0.1,9.9)_N100.eps}{100 Monte Carlo Gaussian experiments in 100x100 lattice at $t=0.01$ with time step $10^{-4}$ for 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig8}

\figur{1}{MonteCarlo_2D_t0.01_w(0.1,9.9)_N10000.eps}{10000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=0.01$ with time step $10^{-4}$ for 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig9}

\figur{1}{Exact_2D_t1_w(0.1,9.9).eps}{Exact solution at $t=1$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Which corresponds well with 1D dimensional case in \reffig{fig2}.}{fig10}

\figur{1}{Explicit_2D_t1_w(0.1,9.9)_alpha0.49.eps}{Explicit solution in 100x100 lattice at $t=1$ with $\alpha = 0.49$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$.}{fig11}

\figur{1}{Jacobi_2D_t1_w(0.1,9.9)_theta1_alpha10000.eps}{Jacobi solution in 100x100 lattice at $t=1$ with $\theta=1$ and $\alpha = 10000$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$.}{fig12}

\figur{1}{MonteCarlo_2D_t1_w(0.1,9.9)_N10000.eps}{10000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig13}

\figur{1}{Exact_2D_t0.1_w(7,8).eps}{Exact solution at $t=0.1$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig14}
\newpage

\figur{1}{Explicit_2D_t0.1_w(7,8)_alpha0.49.eps}{Explicit solution in 100x100 lattice at $t=0.1$ with $\alpha = 0.49$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig15}

\figur{1}{Jacobi_2D_t0.1_w(7,8)_theta1_alpha1000.eps}{Jacobi solution in 100x100 lattice at $t=0.1$ with $\theta=1$ and $\alpha = 1000$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig12}

\figur{1}{MonteCarlo_2D_t0.1_w(7,8)_N10000.eps}{10000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=0.1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig17}

\figur{1}{Exact_2D_t1_w(7,8).eps}{Exact solution at $t=1$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig18}

\figur{1}{Explicit_2D_t1_w(7,8)_alpha0.49.eps}{Explicit solution in 100x100 lattice at $t=1$ with $\alpha = 0.49$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig19}

\figur{1}{Jacobi_2D_t1_w(7,8)_theta1_alpha10000.eps}{Jacobi solution in 100x100 lattice at $t=1$ with $\theta=1$ and $\alpha = 10000$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig20}

\figur{1}{MonteCarlo_2D_t1_w(7,8)_N100.eps}{100 Monte Carlo Gaussian experiments in 100x100 lattice at $t=1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig21}

\figur{1}{MonteCarlo_2D_t1_w(7,8)_N10000.eps}{10000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig22}

\figur{1}{MonteCarlo_2D_t1_w(7,8)_N1000000.eps}{1000000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig23}

\newpage

\begin{tabell}{|r|c|c|c|}{\small}{Case & Explicit & Jacobi & Monte Carlo 10000 \\}{\input{build-project5-Desktop-Debug/2D_time.dat}}{Calculation time for 2D diffusion in seconds.}{tab5}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{Case & Explicit & Jacobi & Monte Carlo 10000 \\}{\input{build-project5-Desktop-Debug/2D_error.dat}}{Order of relative error for 2D diffusion Relative error of calculated and exact value less than $10^{-1}$ are excluded.}{tab6}
\end{tabell}

\section{Attachments}

The source files developed are


\section{Resources}

\begin{enumerate}
\item{\href{http://qt-project.org/downloads}{QT Creator 5.3.1 with C11}}
\item{\href{https://www.eclipse.org/downloads/}{Eclipse Standard/SDK  - Version: Luna Release (4.4.0) with PyDev for Python}}
\item{\href{http://www.ubuntu.com/download/desktop}{Ubuntu 14.04.1 LTS}}
\item{\href{http://shop.lenovo.com/no/en/laptops/thinkpad/w-series/w540/#tab-reseller}{ThinkPad W540 P/N: 20BG0042MN with 32 GB RAM}}
\end{enumerate}

\begin{thebibliography}{1}
\bibitem{project5}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/projects/project-5-deadline-december-1/project5_diffusion.pdf}{\emph{FYS4150 - Project 5}} - \emph{Diffusion in two dimensions}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{lecture}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/Lecture\%20Notes/lecture2014.pdf}{\emph{Computational Physics - Lecture Notes Fall 2014}}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{project4} \href{mailto:eimundsm@fys.uio.no}{Eimund Smestad}, \href{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%204}{\emph{FYS4150 - Computational Physics - Project 4}}, \href{http://www.uio.no}{University of Oslo}, 2014
\bibitem{Farnell}
Farnell and Gibson, \href{https://vpn2.uio.no/+CSCO+00756767633A2F2F6A6A6A2E667076726170727176657270672E70627A++/science/article/pii/S0021999105001087#}
{\emph{Monte Carlo simulation of diffusion in a spatially
nonhomogeneous medium:}} \emph{A biased random walk
on an asymmetrical lattice}, Journal of Computational Physics Vol. 208 p. 253-265, 2005
\bibitem{Diffusion}\href{http://en.wikipedia.org/wiki/Diffusion\_equation}{http://en.wikipedia.org/wiki/Diffusion\_equation}
\bibitem{Heat}\href{http://en.wikipedia.org/wiki/Heat\_equation}{http://en.wikipedia.org/wiki/Heat\_equation}
\bibitem{Dirichlet}\href{http://en.wikipedia.org/wiki/Dirichlet\_boundary\_condition}{http://en.wikipedia.org/wiki/Dirichlet\_boundary\_condition}
\bibitem{Poisson_eq}\href{http://en.wikipedia.org/wiki/Poisson\%27s\_equation}{http://en.wikipedia.org/wiki/Poisson\%27s\_equation}
\bibitem{Uniqueness}\href{http://en.wikipedia.org/wiki/Uniqueness\_theorem\_for\_Poisson\%27s\_equation}{http://en.wikipedia.org/wiki/Uniqueness\_theorem\_for\_Poisson\%27s\_equation}
\bibitem{Ansatz}\href{http://en.wikipedia.org/wiki/Ansatz}{http://en.wikipedia.org/wiki/Ansatz}
\bibitem{Kronecker}\href{http://en.wikipedia.org/wiki/Kronecker\_delta}{http://en.wikipedia.org/wiki/Kronecker\_delta}
\bibitem{Jacobi}\href{http://en.wikipedia.org/wiki/Jacobi\_method}{http://en.wikipedia.org/wiki/Jacobi\_method}
\bibitem{StochasticProcess}\href{http://en.wikipedia.org/wiki/Stochastic\_process}{http://en.wikipedia.org/wiki/Stochastic\_process}
\bibitem{Stochastic}\href{http://en.wikipedia.org/wiki/Independence\_\%28probability\_theory\%29}{http://en.wikipedia.org/wiki/Independence\_\%28probability\_theory\%29}
\bibitem{MarkovProperty}\href{http://en.wikipedia.org/wiki/Markov\_property}{http://en.wikipedia.org/wiki/Markov\_property}
\bibitem{MarkovProcess}\href{http://en.wikipedia.org/wiki/Markov\_process}{http://en.wikipedia.org/wiki/Markov\_process}
\bibitem{MarkovChain}\href{http://en.wikipedia.org/wiki/Markov\_chain}{http://en.wikipedia.org/wiki/Markov\_chain}
\bibitem{MasterEquation}\href{http://en.wikipedia.org/wiki/Master\_equation}{http://en.wikipedia.org/wiki/Master\_equation}
\bibitem{Metropolis}\href{http://en.wikipedia.org/wiki/Metropolis\%E2\%80\%93Hastings\_algorithm}{http://en.wikipedia.org/wiki/Metropolis\%E2\%80\%93Hastings\_algorithm}
\bibitem{MonteCarlo}\href{http://en.wikipedia.org/wiki/Monte\_Carlo\_algorithm}{http://en.wikipedia.org/wiki/Monte\_Carlo\_algorithm}
\bibitem{Isotropic}\href{http://en.wikipedia.org/wiki/Isotropy}{http://en.wikipedia.org/wiki/Isotropy}
\bibitem{Lax}\href{http://en.wikipedia.org/wiki/Lax\_equivalence\_theorem}{http://en.wikipedia.org/wiki/Lax\_equivalence\_theorem}
\bibitem{WellPosed}\href{http://en.wikipedia.org/wiki/Well-posed\_problem}{http://en.wikipedia.org/wiki/Well-posed\_problem}
\bibitem{Spectral_radius}\href{http://en.wikipedia.org/wiki/Spectral\_radius}{http://en.wikipedia.org/wiki/Spectral\_radius}
\end{thebibliography}

\end{flushleft}
\end{document}
