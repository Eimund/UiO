\documentclass[11pt,english,a4paper]{article}
\include{mitt_oppsett}
\usepackage{fullpage}
\setcounter{MaxMatrixCols}{20}

\renewcommand\title{FYS4150 - Computational Physics - Project 5}

\renewcommand\author{Eimund Smestad  - Candidate number: 68}
%\newcommand\adress{}
\renewcommand\date{\today}
\newcommand\email{\href{mailto:eimundsm@fys.uio.no}{eimundsm@fys.uio.no}}

%\lstset{language=[Visual]C++,caption={Descriptive Caption Text},label=DescriptiveLabel}
\lstset{language=c++}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{black}\bfseries}
\lstset{commentstyle=\itshape\color{black}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\lstset{tabsize=2}

\begin{document}
\maketitle
\begin{flushleft}

\begin{abstract}
This project is a continuation of the study of diffusion of neurotransmitters across a synaptic cleft in project 4, but where I have extended to diffusion in 2 dimensions. An analytical solution are derived for the partial differential equation for 2D diffusion problem with wall boundaries. Different numerical methods for solving the 2D diffusion problems are studied to verify the methods and the analytical solution. These methods are explicit scheme, Jacobi iterative method, Monte Carlo and Metropolis algorithm. The different numerical methods are compared in computation time, stability and accuracy.
\end{abstract}

\section{Diffusion of neurotransmitters}

I will study diffusion as a transport process for neurotransmitters  across synaptic cleft separating the cell membrane of two neurons, for more detail see \cite{project4}. The diffusion equation is the partial differential equation 

\begin{align*}
\frac{\partial u\p{\textbf{x},t}}{\partial t} = \nabla \cdot\p{D\p{\textbf{x},t}\boldsymbol{\nabla} u\p{\textbf{x},t}}\,,
\end{align*}

where $u$ is the concentration of particular neurotransmitters at location $\textbf{x}$ and time $t$ with the diffusion coefficient $D$. In this study I consider the diffusion coefficient as constant, which simplify the diffusion equation to the heat equation

\begin{align*}
\frac{\partial u\p{\textbf{x},t}}{\partial t} = D\nabla^2 u\p{\textbf{x},t}\,.
\end{align*}

I will look at the concentration of neurotransmitter $u$ in two dimensions with $x_1$ parallel with the direction between the presynaptic to the postsynaptic across the synaptic cleft, and $x_2$ is parallel with both presynaptic to the postsynaptic. Hence we have the differential equation

\begin{align}
\frac{\partial u\p{\kr{x_i}_{i=1}^2,t}}{\partial t} = D\sum_{j=1}^2\frac{\partial^2 u\p{\kr{x_i}_{i=1}^2,t}}{\partial {x_j}^2}\,,
\label{eq_1}
\end{align}

where $\kr{x_i}_{i=1}^2 = \p{x_1, x_2} = \textbf{x}$. The boundary and initial condition that I'm going to study is 

\begin{align}
&\exists \kr{d, w} \subseteq \mathbb{R}_{0+}\exists\kr{w_i}_{i=1}^2 \subseteq \mathbb{R}_{0+}^{w-}\left(\forall t \in\mathbb{R}_{0}:\forall x_2 \in \mathbb{R}_{w_1}^{w_2} : u\p{0,x_2,t} = u_0  \right.
\nonumber\\
& \quad \wedge \forall  t \in\mathbb{R} \left(\forall x_2\in\mathbb{R}_{0+}^{w-} : u\p{d, x_2, t} = 0 
\wedge \forall x_1\in\mathbb{R}_{0}^d : \left(u\p{x_1,0, t} = 0 \wedge u\p{x_1,w, t} = 0\right) \right)
\nonumber\\
& \quad \left.\wedge
\forall x_1\in\mathbb{R}_{0+}^{d-}\forall x_2\in\mathbb{R}_{0+}^{w-} :u\p{\kr{x_i}_{i=1}^2,0}=0 \wedge \forall x_2 \in \mathbb{R}_0^w \setminus \mathbb{R}_{w_1}^{w_2} : u\p{0,x_2,0} = 0 \right)
\label{eq_2}
\end{align}

where $d$ is the distance between the presynaptic and the postsynaptic, and $w$ is the width of the presynaptic and postsynaptic. Note that the notation $\forall x\in\mathbb{R}_{a+}^{b-} \Leftrightarrow a < x < b$, where as $\forall x\in\mathbb{R}_{a}^{b} \Leftrightarrow a \leq x \leq b$. Note also that these boundary conditions implies that the neurotransmitters are transmitted from presynaptic at $x_1=0$ and $w_1 \leq x_2\leq w_2$ with constant concentration $u_0$; the neurotransmitters are  immediately absorbed at the postsynaptic $x_1=d$; there are no neurotransmitters at boundary width $x_2=0$ and $x_2 = w$ of the synaptic cleft; and we have the initial condition at $t=0$ where there are no neurotransmitters between the pre- and postsynaptic as well on the side of the synaptic vesicles $x_1=0$, $0\leq x_2 < w_1$ and $w_2 < x_2 \leq w$. \linebreak

\figur{0.8}{thompsonB2000-p39.eps}{Left: Schematic drawing of the process of vesicle release from the axon terminal and release of transmitter molecules into the synaptic cleft. (From Thompson: \anf{The Brain}, Worth Publ., 2000). Right: Molecular structure of the two important neurotransmitters glutamate and GABA.}{fig1} 

To solve the differential equation \eqref{eq_1} with the boundary and initial condition \eqref{eq_2} we make an ansatz that the solution is unique, which is the case for a deterministic system. We recognize the heat equation as part of the class of partial differential equations spanned by the Poisson's equation for each time instance. The Uniqueness theorem for the Poisson's equation $\nabla^2 u = f$ \cite{Uniqueness} says that the Poisson's equation has a unique solution with the Dirichlet boundary condition, where Dirichlet boundary condition is here defined as a boundary that specifies the values the solution must have at the boundary. Not to be confused with Dirichlet boundary condition with zero at the boundary, which is just a special case and I will refer to it as 0th Dirichlet boundary condition. \linebreak

Unfortunately the boundary condition in \eqref{eq_2} is not a Dirichlet boundary, since the boundary is not specified on the side of the synaptic vesicles $x_1=0$, $0\leq x_2 < w_1$ and $w_2 < x_2 \leq w$, and closer investigation will show that the boundary condition in \eqref{eq_2} does not provide a unique solution. So we need to add further condition to make the solution unique, and I make an assumption that the total concentration $u$ in an infinitesimal area has uniform concentration per length $u_1$ and $u_2$ in each direction $x_1$ and $x_2$ accordingly. Hence $u=u_1 u_2$ at every point and therefore we can write

\begin{align}
\forall x_1 \in \mathbb{R}_0^d \forall x_2 \in \mathbb{R}_0^w \forall t \in \mathbb{R}_{0}: u\p{\kr{x_i}_{i=1}^2,t} = u_1\p{x_1,t} u_2\p{x_2,t} \,.
\label{eq_3}
\end{align}

Putting this into the heat equation \eqref{eq_1} we get

\begin{align*}
u_2\p{x_2,t}\frac{\partial u_1\p{x_1,t}}{\partial t} + u_1\p{x_1,t}\frac{\partial u_2\p{x_2,t}}{\partial t} = D\p{u_2\p{x_2,t}\frac{\partial^2 u_1\p{x_1,t}}{\partial {x_1}^2} + u_1\p{x_1,t}\frac{\partial^2 u_2\p{x_2,t}}{\partial {x_2}^2}} \,,
\end{align*}

which can be written as two heat equations

\begin{align}
\forall i \in \mathbb{N}_1^2 : \frac{\partial u_i\p{x_i,t}}{\partial t}  = D \frac{\partial^2 u_i\p{x_i,t}}{\partial {x_i}^2} \,.
\label{eq_4}
\end{align}

I make another ansatz that the the boundary at $u\p{0,x_2,t}$ determined by $u_2\p{x_2,t}$ alone, and by satisfying initial condition $u\p{0,x_2,0}$ for $u_2$

\begin{align}
\forall t \in \mathbb{R}_0 \p{: u_2\p{0,t} = u_2\p{w,t} = 0 \wedge \forall x_2\in\mathbb{R}_{w_1}^{w_2}: u_2\p{x_2,t} = u_0} \wedge \forall x_2 \in\mathbb{R}_{0}^{w} \setminus \mathbb{R}_{w_1}^{w_2}: u_2\p{x_2,0} = 0
\label{eq_5}
\end{align}

we have now determined the values on all the boundaries have therefore Dirichlet boundary condition, and therefore a unique solution of $u$. We found the analytical solution to the heat equation in \eqref{eq_4} for $i=2$ with similar boundary and initial condition in project 4 \cite{project4}, and I will therefore not show the derivation here, but just state the solutions

\begin{align}
u_2\p{x_2,t} = u_0 \begin{cases} \frac{x_2}{w_1} - \sum_{n=1} \frac{2}{n \pi} \sin\p{n\pi\p{1-\frac{x_2}{w_1}}}\exp\p{-D\p{\frac{n\pi}{w_1}}^2 t} &: x_2\in\mathbb{R}_0^{w_1-}\\ 1 &: x_2 \in\mathbb{R}_{w_1}^{w_2} \\  \frac{w-x_2}{w-w_2} - \sum_{n=1} \frac{2}{n \pi} \sin\p{n\pi\frac{x_2-w_2}{w-w_2}}\exp\p{-D\p{\frac{n\pi}{w-w_2}}^2 t} & : x_2\in\mathbb{R}_{w_2+}^w \,.\end{cases}
\label{eq_6}
\end{align}

Since I have established that the boundary at $u\p{0,x_2,t}$ determined by $u_2\p{x_2,t}$ alone, means that $u_1$ has the following boundary and initial condition

\begin{align}
\forall t \in \mathbb{R}_0\p{: u_1\p{0,t} = 1 \wedge u_1\p{d,t} = 0} \wedge  \forall x_1 \in \mathbb{R}_{0+}^{d-} : u_1\p{x_1,0}=0 \,.  
\label{eq_7}
\end{align}

And using the analytical solution from project 4 to heat equation \eqref{eq_4} for $i=1$ with boundary and initial condition in \eqref{eq_7}, we have 

\begin{align}
u_1\p{x_1,t} = 1-\frac{x_1}{d} - \sum_{n=1} \frac{2}{n \pi} \sin\p{n\pi\frac{x_1}{d}}\exp\p{-D\p{\frac{n\pi}{d}}^2 t} \,.
\label{eq_8}
\end{align}

To summarize the solution to the concentration $u$ is given by \eqref{eq_3} with \eqref{eq_6} and \eqref{eq_8}. 


\section{Numerical methods}

\subsection{The $\theta$-rule}

The Taylor expansion is given by

\begin{align}
u\p{x} = \sum_{n=0} \frac{\T{u}{(n)}\p{x_0}}{n!}\p{x-x_0}^n
\label{eq_9}
\end{align}

where $\T{u}{(n)} = \frac{\mathrm{d}^n u}{\mathrm{d}t^n}$ and $x_0$ is a initial value where we step from to $x$ . If we now use the first order approximation

\begin{align*}
u\p{x} \approx u\p{x_0} + \T{u}{(1)}\p{x_0}\p{x-x_0} \,.
\end{align*}

The first order differential equation $\T{u}{(1)}\p{x} = f\p{x}$ is determined when we have the initial condition $u\p{x_0}$, however $\T{u}{(1)}\p{x_0}$ is not an initial condition, and it depends on how we calculate it numerically from the initial condition. Now note that $\T{u}{(1)}\p{x_0}$ is the same for different values of $x$ in the approximation above and lets say that we calculate it as given from the approximation above;

\begin{align}
\T{u}{(1)}\p{x_0} \approx \frac{u\p{x}-u\p{x_0}}{x-x_0}\,.
\label{eq_10}
\end{align}

So now use this in another point $x_{\theta} = \theta x + \p{1-\theta}x_0$ which we also approximate to the first order, and if we use the expression above for $\T{u}{(1)}\p{x_0}$ we get

\begin{align}
u\p{x_\theta} &\approx u\p{x_0} + \T{u}{(1)}\p{x_0}\p{x_{\theta}-x_0} = u\p{x_0} + \theta\,\T{u}{(1)}\p{x_0}\p{x-x_0} 
\nonumber\\
&\approx u\p{x_0} + \frac{u\p{x}-u\p{x_0}}{x-x_0}\theta\p{x-x_0} = \theta u\p{x} + \p{1-\theta}u\p{x_0} \,,
\label{eq_11}
\end{align}

this is known as the $\theta$-rule. The $\theta$-rule can be used to approximate the solution of the following first order differential equation

\begin{align}
\T{u}{(1)}\p{x} = f\p{u\p{x}} \,,
\label{eq_12}
\end{align}

where we use \eqref{eq_10} to approximate the expression $\T{u}{(1)}\p{x}$ and given an even better or worse approximation to the solution $u\p{x}$ by approximating $f\p{x}\approx f\p{x_{\theta}}$;

\begin{align*}
\frac{u\p{x}-u\p{x_0}}{x-x_0} \approx f\p{u\p{x_{\theta}}} = f\p{\theta u\p{x} + \p{1-\theta}u\p{x_0}}\,,
\end{align*}

which discretize to

\begin{align}
\frac{u_{i+1}-u_i}{x_{i+1}-x_i} = f\p{\theta u_{i+1} + \p{1-\theta}u_i} \qquad \text{where } i\in\mathbb{N}_0 \text{ and $u_0$ is an initial contidion.} 
\label{eq_13}
\end{align}

We can find the the next step in the numerical solution to \eqref{eq_12} by solving this difference equation with regard to $u_{i+1}$. Note the above discretization is known as Forward Euler scheme (Explicit) when $\theta = 0$, Backward Euler scheme (Implicit) when $\theta = 1$ and Crank-Nicolson scheme when $\theta = \frac{1}{2}$.

\subsection{Second order derivative}

We approximated the first order derivative in \eqref{eq_10}, but we need to approximate the second order derivative to be able to solve the diffusion in \eqref{eq_1}. I start by expanding the Taylor series in \eqref{eq_9} around the point $x_0\pm\Delta x$ accordingly;

\begin{align}
u\p{x_0\pm\Delta x} = \sum_{n=0} \frac{\T{u}{(n)}\p{x_0}}{n!}\p{\pm\Delta x}^n\,.
\label{eq_14}
\end{align}

Now adding these two expansions

\begin{align*}
u\p{x_0+\Delta x} + u\p{x_0-\Delta x} = 2\sum_{n=0} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2n} = 2 u\p{x_0} + \T{u}{(2)}\p{x_0}\Delta x^2 + 2\sum_{n=2} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2n}\,,
\end{align*}

and solve it with

\begin{align*}
\T{u}{(2)}\p{x_0} &= \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2} - 2\sum_{n=2} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2(n-1)}
\\ &=  \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2} + \mathcal{O}\p{\Delta x^2}\,,
\end{align*}

So the second order derivative can be approximated with

\begin{align}
\T{u}{(2)}\p{x_0} \approx   \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2}
\label{eq_15}
\end{align}

with the local truncation error $\mathcal{O}\p{\Delta x^2}$.

\subsection{The heat equation} \label{heat_equation}

We want to discretize the dimensionless heat equation from \eqref{eq_1}, where we use $D=1$, $u_0=1$ and $d=1$, 

\begin{align*}
\frac{\partial u\p{\kr{x_i}_{i=1}^2,t}}{\partial t} = \sum_{\ell=1}^2\frac{\partial^2 u\p{\kr{x_i}_{i=1}^2,t}}{\partial {x_\ell}^2}\,,
\end{align*}

to numerically solve diffusion of neurotransmitters. First we do the $\theta$-rule discretization in \eqref{eq_13} 

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^2}-u_{i\kr{j_k}_{k=1}^2}}{\Delta t} 
&= \sum_{\ell = 1}^2\frac{\partial^2 u_{(i+\theta)\kr{j_k}_{k=1}^2}}{\partial {x_\ell}^2} 
= \sum_{\ell=1}^2\frac{\partial^2 \p{\theta u_{(i+1)\kr{j_k}_{k=1}^2}+\p{1-\theta}u_{i\kr{j_k}_{k=1}^2}}}{\partial {x_\ell}^2} 
\\
&= \sum_{\ell=1}^2\p{\theta \frac{\partial u_{(i+1)\kr{j_k}_{k=1}^2}}{\partial {x_\ell}^2} + \p{1-\theta}\frac{\partial u_{i\kr{j_k}_{k=1}^2}}{\partial {x_\ell}^2}}
\end{align*}

where index $i$ is stepping of $t$ and $j_k$ are stepping of $x_k$. Note also that the following notation expand accordingly $u_{i\kr{j_k}_{k=1}^2} = u_{ij_1 j_2}$, which becomes a more elegant notation for larger $n$ in $u_{i\kr{j_k}_{k=1}^n} = u_{ij_1j_2\ldots j_n}$. Now we implement the discretization of the second order in \eqref{eq_15}

\begin{align}
\frac{u_{(i+1)\kr{j_k}_{k=1}^2}-u_{i\kr{j_k}_{k=1}^2}}{\Delta t} =& \sum_{\ell=1}^2 \left(\frac{\theta}{\Delta {x_\ell}^2}\p{u_{(i+1)\kr{j_k+\delta_{k\ell}}_{k=1}^2} - 2 u_{(i+1)\kr{j_k}_{k=1}^2} + u_{(i+1)\kr{j_k-\delta_{k\ell}}_{k=1}^2}} \right.
\nonumber\\
&\quad \left. + \frac{1-\theta}{\Delta {x_\ell}^2}\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} - 2u_{i\kr{j_k}_{k=1}^2} +u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}}\right) \,,
\label{eq_16}
\end{align}

where $\delta_{k\ell}$ is the Kronecker delta, and we now clearly see the elegance of the notation $u_{i\kr{j_k}_{k=1}^n}$. \linebreak

The dimensionless initial condition from \eqref{eq_4} gives us

\begin{align*}
u_{0\kr{j_k}_{k=1}^2} = \begin{cases} 1 & : j_1 = 0 \text{ and } x_{2 j_2} \in \mathbb{R}_{w_1}^{w_2} \\ 0 & :\text{elsewhere,} \end{cases} 
\end{align*}

where $x_{2 j_2} = x_{2 0}  + \frac{j_2}{\Delta x_2}$. For the explicit scheme $\theta = 0$ we get

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^2} = u_{i\kr{j_k}_{k=1}^2} + \sum_{\ell=1}^2 \alpha_\ell\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} - 2u_{i\kr{j_k}_{k=1}^2} +u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}}
\label{eq_17}
\end{align}

where

\begin{align*}
\alpha_\ell = \frac{\Delta t}{\Delta {x_\ell}^2} \qquad \text{and} \qquad n_\ell = \frac{1}{\Delta x_\ell} \,,
\end{align*}

where $n_\ell$ is number of grid points in $\ell$ direction. 

\subsubsection{Stability and convergence} \label{sec_explicit_stab_conv}

According to Lax equivalence theorem a consistent finite difference method for well-posed  (a solution exists, unique and continuous) linear initial value problem, the method is convergent if and only if stable. Which means that we only can show either stability and convergence to show both. \linebreak

There is a theorem that states that a matrix $\textbf{A}$ converges $\lim_{i\to\infty} \textbf{A}^i = 0$ if and only if the spectral radius $\rho\p{\textbf{A}} < 1$. The spectral radius is defined as

\begin{align*}
\rho\p{\textbf{A}} = \max_m \abs{\lambda_m} \,,
\end{align*}

where $\lambda_m$ are the eigenvalues of $\textbf{A}$. The explicit scheme in \eqref{eq_17}  can be written as $\textbf{u}_{i+1} = \textbf{A}\textbf{u}_i = \textbf{A}^{i+1}\textbf{u}_0$, and the this matrix can be written as

\begin{align*}
\textbf{A} = \textbf{I} - \textbf{B}
\end{align*}

where $\textbf{I}$ is the identity matrix. This implies that the eigenvalues $\mu_m$ of $\textbf{B}$ is related to the eigenvalues $\lambda_m$ of $\textbf{A}$ as follows

\begin{align}
\lambda_m = 1 - \mu_m\,.
\label{eq_18}
\end{align}

The eigenvalues $\mu_m$ can be found from the eigenequation \eqref{eq_17}

\begin{align}
\sum_{\ell=1}^2\alpha_\ell\p{-u_{i\kr{j_k + \delta_{k\ell}}_{k=1}^2} + 2 u_{i\kr{j_k}_{k=1}^2}-u_{i\kr{j_k - \delta_{k\ell}}_{k=1}^2}} = \mu_m u_{i\kr{j_k}_{k=1}^2} \,.
\label{eq_19}
\end{align}

Using the expression for$\alpha_\ell = \frac{\Delta t}{\Delta {x_\ell}^2}$ we have the discretization of the following equation

\begin{align*}
u\p{\kr{x_i}_{i=1}^2,t} = -{\omega_{m}}^2 \sum_{\ell=1}^2 \frac{\partial^2 u\p{\kr{x_i}_{i=1}^2,t}}{\partial {x_\ell}^2}\,,
\end{align*}

which has the solution 

\begin{align*}
u\p{\kr{x_i}_{i=1}^2, t} = \sum_{\ell=1}^2 A_\ell\p{t} \prod_{\ell=1}^2 \sin\p{\omega_{m} x_\ell + \varphi_\ell}
\end{align*}

where 

\begin{align*}
\omega_{m} = \sqrt{\frac{\Delta t}{\mu_m}}\,.
\end{align*}

Now using this solution into the eigenequation \eqref{eq_19} we get

\begin{align*}
&\mu_m \sum_{\ell=1}^2\sin\p{\omega_{m} x_{\ell j_\ell} + \varphi_\ell}
\\
&\quad =
\sum_{\ell=1}^2 \alpha_\ell\p{-\sin\p{\omega_{m} \p{x_{\ell j_\ell}+\Delta x_\ell} + \varphi_\ell} + 2\sin\p{\omega_{m} x_{\ell j_\ell} + \varphi_\ell} - \sin\p{\omega_{m} \p{x_{\ell j_\ell}-\Delta x_\ell} + \varphi_\ell}}
\end{align*} 

which can be simplified with the trigonometrical relation $\sin\p{u+v} = \sin u \cos v + \cos u \sin v$
 
\begin{align}
\mu_m\sum_{\ell=1}^2\sin\p{\omega_m x_\ell + \varphi_\ell} = 2\sum_{\ell=1}^2 \alpha_\ell\p{1-\cos\p{\omega_{m} \Delta x_\ell}}\sin\p{\omega_m x_\ell + \varphi_\ell} \,,
\label{eq_20}
\end{align}

note that $\mu_m\neq 0$ because of $\omega_{m} \propto \frac{1}{\sqrt{\mu_m}}$. For this equation to be an eigeneqution we need to satisfy

\begin{align*}
\alpha_1 \p{1-\cos\p{\omega_m \Delta x_1}} = \alpha_2\p{1-\cos\p{\omega_m \Delta x_2}}\,.
\end{align*}

If we have square lattice $\alpha_1 = \alpha_2$ the above relation is automatically satisfied, if not we need to solve the equation with regard to $\omega_m$, and from that find the eigenvalue $\mu_m$ from the relation $\omega_m = \sqrt{\frac{\Delta t}{\mu_m}}$. Since the approach with $\alpha_1 \neq \alpha_2$ is difficult to solve analytically I will show the further steps with square lattice $\alpha = \alpha_1 = \alpha_2$, and in the case of $\alpha_1 \neq \alpha_2$ one can approximate to a square lattice problem with the smallest step size of $\Delta x_\ell$. If you forfill the convergence criteria for the a square lattice with the smallest step size, then you are guaranteed to satisfy the convergence criteria in the non-square lattice case as well. \linebreak

For the square lattice with $\alpha$ we see from \eqref{eq_20} that the eigenvalue $\mu_m$ must be within

\begin{align*}
0 \leq \mu_m \leq 2\alpha \,.
\end{align*}


Inserting these eigenvalues into \eqref{eq_18} and using the convergence condition $\rho\p{\textbf{A}} < 1$ yields the following inequality

\begin{align*}
-1 < 1 - 2\alpha < 1
\end{align*}

which result in the constraint for convergences

\begin{align*}
\alpha < \frac{1}{2} \,,
\end{align*}

where the $\alpha$ is always positive. So when we want to solve the heat equation explicitly we satisfy the convergence criteria if we chose a time step accordingly

\begin{align}
\Delta t < \min_\ell \frac{1}{{n_\ell}^2} \,.
\label{eq_21}
\end{align}

\subsection{Jacobi's iterative method}

When we have $\theta\neq 0$ in \eqref{eq_16} we have implicit schemes, and I rewrite \eqref{eq_16} with unknowns on the left side and knowns on the right side;

\begin{align*}
\forall k\in\mathbb{N}_1^2 \forall j_k \in \mathbb{N}_1^{n_k-2} : &\p{1+2\theta\sum_{\ell=1}^2 \alpha_\ell} u_{(i+1)\kr{j_k}_{k=1}^2} - \theta\sum_{\ell = 1}^2\alpha_\ell \p{u_{(i+1)\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{(i+1)\kr{j_k-\delta_{k\ell}}_{k=1}^2}} 
\\
&\quad = \p{1-2\p{1-\theta}\sum_{\ell = 1}^2 \alpha_\ell} u_{i\kr{j_k}_{k=1}^2} + \p{1-\theta}\sum_{\ell=1}^2\alpha_\ell\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}} \,,
\end{align*}

where the boundary are given at the indices $j_k=0$ and $j_k= n_k-1$ where $n_k$ are the number of points i $k$ direction. Now I will transform this into a linear algebra problem $\textbf{A}_{i+1}\textbf{v}_{i+1} = \textbf{b}_i$, where elements of $\textbf{v}_{i+1}$ are

\begin{align}
v_{(i+1)j} = v_{(i+1)(j_1 n_2 + j_2)} = u_{(i+1)\kr{j_k}_{k=1}^2} \,.
\label{eq_22}
\end{align}

The fixed boundary values with $u_0=1$ at the presynaptic $x_1=0$ and $w_1 \leq x_2 \leq w_2$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_1 = \mathbb{N}_{m_1}^{m_2}\,.
\end{align*}

We have non-fixed values at the boundary on the side of the synaptic vesicles at the presynaptic $x_1=0$, $0 \leq x_2 < w_1$ and $w_2 < x_2 < w$, and are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_2 = j\in\mathbb{N}_0^{n_2-1} \setminus \mathbb{B}_1 \,.
\end{align*}

The boundary values at the postsynaptic $x_1=d$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_3 = \mathbb{N}_{n_1 n_2 - n_2}^{n_1 n_2 - 1} \,.
\end{align*}

The left side boundary $x_2=0$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_4 = \kr{\ell n_2}_{\ell=1}^{n_1-2} \,.
\end{align*}

The right side boundary $x_2=w$ are given by the indices $j$ in the set

\begin{align*}
\mathbb{B}_5 = \kr{\ell n_2 - 1}_{\ell=2}^{n_1-1}\,.
\end{align*}

And the boundary as a whole are given by the indices $j$ in the set

\begin{align*}
\mathbb{B} = \bigcup_{i=1}^5 \mathbb{B}_i \,.
\end{align*}

The inner points are given by the indices $j$ in the set

\begin{align*}
\mathbb{I} = \mathbb{N}_{0}^{n_1 n_2 -1} \setminus \mathbb{B}\,.
\end{align*}

We can now write the matrix elements of $\textbf{A}_{(i+1)}$

\begin{align}
a_{(i+1)jk} = a_{jk} = \begin{cases} 
1 &:\forall j\in\mathbb{B}\setminus\mathbb{B}_2  : j=k \,\,\,\,\quad\qquad\qquad \text{(fixed boundary)}\\
1+ 2\theta \alpha_2 &: \forall j\in\mathbb{B}_2: j=k \,\,\quad\qquad\qquad\qquad (\text{side of vesicles}) \\ 
1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell &: \forall j\in\mathbb{I}: j=k \qquad\qquad\qquad\qquad (\text{the inner points }u_{(i+1)j_1j_2}) \\ 
-\theta \alpha_2 &: \forall j\in\mathbb{I}\cup\mathbb{B}_2: k\in\kr{j-1,j+1}\quad \text{(diffusion in $x_2$ direction)} \\ 
- \theta \alpha_1 &: \forall j\in\mathbb{I} : k \in \kr{j-n_2,j+n_2} \,\,\qquad\text{(diffusion in $x_1$ direction)} \\
 0 & : \text{otherwise,} \end{cases}
\label{eq_23}
\end{align}

and the elements of the vector $\textbf{b}_i$ are

\begin{align}
b_{ij} = b_{i(j_1 n_2 + j_2)} = \begin{cases}
1 &: j\in\mathbb{B}_1 \\
\p{1-2\p{1-\theta}\alpha_2} u_{i0j_2} + \p{1-\theta}\alpha_2\p{u_{i0(j_2+1)} + u_{i0(j_2-1)}} &: j\in\mathbb{B}_2 \\
\p{1-2\p{1-\theta}\sum_{\ell = 1}^2 \alpha_\ell} u_{i\kr{j_k}_{k=1}^2} + \p{1-\theta}\sum_{\ell=1}^2\alpha_\ell\p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}} &: j\in\mathbb{I} \\
0 &: \text{otherwise.}
\end{cases}
\label{eq_24}
\end{align}

Unfortunately  is the linear algebra problem $\textbf{A}\textbf{v}_{i+1} = \textbf{b}_i$ is computational expensive to solve for the matrix in \eqref{eq_23}. To lower the computational time I therefore prepare the matrix $\textbf{A}$ for Jacobi's iterative method by splitting it into a diagonal matrix $\textbf{D}$ and a remainder matrix $\textbf{R}$;

\begin{align*}
\textbf{A} = \textbf{D} + \textbf{R} \,.
\end{align*}

From \eqref{eq_23} we see that the elements of the diagonal matrix are given by

\begin{align}
d_{jk} = \begin{cases} 
1 &:\forall j\in\mathbb{B}\setminus\mathbb{B}_2  : j=k \,\,\,\,\quad\qquad\qquad \text{(fixed boundary)}\\
1+ 2\theta \alpha_2 &: \forall j\in\mathbb{B}_2: j=k \,\,\quad\qquad\qquad\qquad (\text{side of vesicles}) \\ 
1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell &: \forall j\in\mathbb{I}: j=k \qquad\qquad\qquad\qquad (\text{the inner points }u_{(i+1)j_1j_2}) \\ 
 0 & : \text{otherwise,} \end{cases}
\label{eq_25}
\end{align}

and the elements of the remainder matrix are given by

\begin{align}
r_{jk} = \begin{cases} 
-\theta \alpha_2 &: \forall j\in\mathbb{I}\cup\mathbb{B}_2: k\in\kr{j-1,j+1}\quad \text{(diffusion in $x_2$ direction)} \\ 
- \theta \alpha_1 &: \forall j\in\mathbb{I} : k \in \kr{j-n_2,j+n_2} \,\,\qquad\text{(diffusion in $x_1$ direction)} \\
 0 & : \text{otherwise.} \end{cases}
\label{eq_26}
\end{align}

We can now introduce Jacobi's iterative method

\begin{align*}
\textbf{v}_{i+1}^{(\ell)} = \begin{cases}
\textbf{v}_i &: \ell=0 \\
\textbf{D}^{-1}\p{\textbf{b}_i-\textbf{R}\textbf{v}_{i+1}^{(\ell-1)}} &: \ell\in\mathbb{N}_1
\end{cases}
\end{align*}

where $\ell$ is the number of iterations and $\ell=0$ are the starting point. Rewriting it on element form yields

\begin{align*}
v_{(i+1)j}^{(\ell)} = \begin{cases} v_j &: \ell = 0 \\ \frac{1}{d_{jj}}\p{b_j - \sum_{k\neq j} r_{jk} v_{(i+1)k}^{(\ell-1)}} &: j\in\mathbb{N}_1 \,.\end{cases}
\end{align*}

Using \eqref{eq_22} and \eqref{eq_26} we have the iterative solution to the heat equation in \eqref{eq_6} with the boundaries in \eqref{eq_2}

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^2}^{(\ell)} = \begin{cases}
u_{i\kr{j_k}_{k=1}^2} &: \ell = 0 \\
1 &: j_1=0 \wedge j_2\in\mathbb{N}_{m_1}^{m_2} \\
c_0 + c_1\p{u_{(i+1)0(j_2+1)}^{(\ell-1)}+u_{(i+1)0(j_2-1)}^{(\ell-1)}} &: j_1=0 \wedge j_2\in\mathbb{N}_0^{n_2-1} \setminus\mathbb{N}_{m_1}^{m_2} \\
c_2 + \sum_{o=1}^2 c_{o+2}\p{u_{(i+1)\kr{j_k+\delta_{ko}}_{k=1}^2}^{(\ell-1)}+u_{(i+1)\kr{j_k-\delta_{ko}}_{k=1}^2}^{(\ell-1)}} &: j_1\in \mathbb{N}_{1}^{n_1-2} \wedge j_2\in\mathbb{N}_1^{n_2-2} 
\end{cases} 
\label{eq_27}
\end{align}

\begin{align}
c_0 &= c_5 u_{i0j_2} + c_6\p{u_{i0(j_2+1)} + u_{i0(j_2-1)}}
\label{eq_28}\\
c_1 &= \frac{\theta \alpha_2}{1+2\theta\alpha_2}
\label{eq_29}\\
c_2 &= c_7 u_{i\kr{j_k}_{k=1}^2} + \sum_{\ell=1}^2 c_{\ell+7} \p{u_{i\kr{j_k+\delta_{k\ell}}_{k=1}^2} + u_{i\kr{j_k-\delta_{k\ell}}_{k=1}^2}}
\label{eq_30}\\
c_3 &= \frac{\theta \alpha_1}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_31}\\
c_4 &= \frac{\theta \alpha_2}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_32}\\
c_5 &= \frac{1-2\p{1-\theta}\alpha_2}{1+2\theta \alpha_2} 
\label{eq_33}\\
c_6 &= \frac{\p{1-\theta}\alpha_2}{1+2\theta \alpha_2} 
\label{eq_34}\\
c_7 &= \frac{1-2\p{1-\theta}\sum_{\ell = 1}^2 \alpha_\ell}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_35}\\
c_8 &= \frac{\p{1-\theta}\alpha_1}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_36}\\
c_9 &= \frac{\p{1-\theta}\alpha_2}{1+ 2\theta \sum_{\ell = 1}^2 \alpha_\ell}
\label{eq_37}
\end{align}

\subsubsection{Stability and convergence}

The same procedure as discussed for convergence and stability for the explicit scheme applies for the Jacobi method as well, but now with the spectral radius $\rho\p{\textbf{D}^{-1} \textbf{R}} < 1$, because the Jacobi method are given by 

\begin{align*}
\textbf{v}_{i+1}^{\p{\ell}} = \textbf{D}^{-1}\p{\textbf{b}_i - \textbf{R}\textbf{v}_{i+1}^{\p{\ell-1}}} = \textbf{D}^{-1}\sum_{j=0}^{\ell-1} \p{-\textbf{D}^{-1}\textbf{R}}^j \textbf{b}_i + \p{-\textbf{D}^{-1}\textbf{R}}^\ell\textbf{v}_{i+1}^{(0)} \,.
\end{align*}

Similar steps as shown in section \ref{sec_explicit_stab_conv} it can be shown that the always converges for our diffusion problem. More generally it can be shown that the Jacobi iterative method converges when the matrix $\textbf{A} = \textbf{D}+ \textbf{R}$ is diagonally dominant 

\begin{align*}
\abs{a_{ii}} > \sum_{j\neq i} \abs{a_{ij}}\,,
\end{align*}

which is the case for the matrix spanned out by \eqref{eq_23} because $1+2\theta \alpha_2 > 2\theta \alpha_2$ and $1+2\theta\sum_{\ell=1}^2 \alpha_{\ell} > 2\theta\sum_{\ell=1}^2 \alpha_\ell$.

\subsection{Markov chains}

I assume that diffusion is a memoryless physical process that approximates to a stochastic process, hence diffusion is assumed to satisfy the Markov property and therefore is a Markov process where the transition between states are described by Markov chains 

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n}^+ = \sum_{\kr{\ell_k}_{k=1}^n} W_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} u_{i\kr{\ell_k}_{k=1}^n} \,,
\label{eq_38}
\end{align}

where $u_{i\kr{j_k}_{k=1}^n}$ is the probability distribution function \anf{PDF} at time step $i$, and $W_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n}$  is the transition probability from state $\kr{\ell_k}_{k=1}^n$ to $\kr{j_k}_{k=1}^n$. And the reverse Markov chain is given by

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n}^- = \sum_{\kr{\ell_k}_{k=1}^n} W_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} u_{i\kr{j_k}_{k=1}^n} \,,
\label{eq_39}
\end{align}

Using the assumption that diffusion can be modelled as Markov process then we can use a Master equation to describe the normalized concentration at time step $i+1$ by the difference of transition to and from the state $\kr{j_k}_{k=1}^n$ plus the amount that was in the state; 

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n} = u_{i\kr{j_k}_{k=1}^n} +  u_{(i+1)\kr{j_k}_{k=1}^n}^+ - u_{(i+1)\kr{j_k}_{k=1}^n}^-
\label{eq_40}
\end{align} 

Note that the reverse Markov chain is all the transitions that a state $\kr{j_k}_{k=1}$ takes, including transition to itself. Therefore the reverse Markov chain in time step $i+1$ describes all the transitions that the state does at time step $i$;

\begin{align*}
u_{i\kr{j_k}_{k=1}^n} =  u_{(i+1)\kr{j_k}_{k=1}^n}^- \,,
\end{align*}


which yields

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n} = u_{(i+1)\kr{j_k}_{k=1}^n}^+ \,.
\label{eq_41}
\end{align}

Further, diffusion is assumed to be approximated as stochastic move in distance and direction, where distance and direction is stochastically independent. Which means that transition probability can be written as

\begin{align}
W_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2} = T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2} A_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2} \,,
\label{eq_42}
\end{align}

where $T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2}$ is the probability of moving the distance between the states $\kr{j_k}_{k=1}^n$ and $\kr{\ell_k}_{k=1}^2$, and $ A_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^2}$ is the probability of moving in the direction between the states $\kr{j_k}_{k=1}^n$ and $\kr{\ell_k}_{k=1}^2$. \linebreak

Assume that we move a distance $\ell$ from state $\kr{j_k}_{k=1}^n$, which means that the states $\kr{j_k+\delta_{km}\ell}$ are the possible destinations, which puts the following constraint on the direction probability $A$;

\begin{align}
\sum_{m=1}^n \p{A_{\kr{j_k+\delta_{km}\ell}_{k=1}^n\kr{j_k}_{k=1}^n}+A_{\kr{j_k-\delta_{km}\ell}_{k=1}^n\kr{j_k}_{k=1}^n}} = 1 \,.
\label{eq_43}
\end{align}

Now assuming isotropic diffusion, which means that

\begin{align*}
\forall m,o\in \mathbb{N}_1^n : A_{\kr{j_k\pm\delta_{km} \ell}_{k=1}^n\kr{j_k}_{k=1}^n} &= A_{\kr{j_k\pm\delta_{ko} \ell}_{k=1}^n\kr{j_k}_{k=1}^n} \\
\forall m,o\in \mathbb{N}_1^n : T_{\kr{j_k\pm\delta_{km} \ell}_{k=1}^n\kr{j_k}_{k=1}^n} &= T_{\kr{j_k\pm\delta_{ko} \ell}_{k=1}^n\kr{j_k}_{k=1}^n}
\end{align*}

and the constraint on the direction probability then yields

\begin{align}
A = A_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} = \frac{1}{2n}\,,
\label{eq_44}
\end{align}

and the distance probability $T$ is only dependent on the distance $\ell$

\begin{align*}
p_\ell = T_{\kr{j_k\pm\delta_{km} \ell}_{k=1}^n\kr{j_k}_{k=1}^n}\,,
\end{align*}

which has the constraint

\begin{align*}
\sum_{\ell} p_\ell = 1\,.
\end{align*}

Then the normalized concentration \eqref{eq_41} is now given by

\begin{align*}
u_{(i+1)\kr{j_k}_{k=1}^n} = \frac{1}{2n}\sum_\ell p_\ell\sum_{m=1}^n \p{u_{i\kr{j_k+\delta_{km}\ell}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}\ell}_{k=1}^n}} \,,
\end{align*}

which can be rewritten to

\begin{align}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} = \sum_{\ell}\frac{p_\ell \p{\ell \Delta x}^2}{2n\Delta t}\sum_{m=1}^n\frac{u_{i\kr{j_k+\delta_{km}\ell}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}\ell}_{k=1}^n}}{\p{\ell\Delta x}^2} \,.
\label{eq_45}
\end{align}

When we have the Kronecker delta distribution $p_\ell = \delta_{1\ell}$ this becomes the discretization of the heat equation in \eqref{eq_1} 

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} = \frac{\Delta x^2}{2n\Delta t}\sum_{m=1}^n\frac{u_{i\kr{j_k+\delta_{km}}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}}_{k=1}^n}}{\Delta x^2} \,,
\end{align*}

with the relation to the diffusion constant $D$ as follows

\begin{align}
\Delta x = \sqrt{2nD\Delta t}\,. 
\label{eq_46}
\end{align}

For a general distribution $p_\ell$ we see from \eqref{eq_45} that the step length now is given by

\begin{align}
\Delta x_\ell = \ell \Delta x = \ell\sqrt{2nD\Delta t}\,.
\label{eq_47}
\end{align} 

If we use a random generator to make a proposition to move at each time step, we get a deterministic running time with approximation in the result, which is known as Monte Carlo algorithm. 

\subsubsection{Metropolis-Hastings algorithm}

The steps above with Markov chains are designed to give a approximated time evolution. However if we are only interested in the steady state solution, we can derive a faster algorithm to reach it, which is the Metropolis-Hastings algorithm. To derive the Metropolis-Hastings algorithm we start by looking back at the Master equation in \eqref{eq_40} for equilibrium, where we realize that transition to and from the state $\kr{j_k}_{k=1}^n$ must be equal

\begin{align*}
\lim_{i\to \infty} \p{u_{(i+1)\kr{j_k}_{k=1}^n}^+ - u_{(i+1)\kr{j_k}_{k=1}^n}^-} = 0 \,.
\end{align*}

Using the equality of transition to and from the state $\kr{j_k}_{k=1}^n$ at equilibrium,  the Markov chains in \eqref{eq_38} and \eqref{eq_39}, with proposed transition distribution $T$ which is independent of the accepted $A$, must satisfy the following equation

\begin{align*}
\sum_{\kr{\ell_k}_{k=1}^n}\p{T_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n}A_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n} u_{i\kr{\ell_k}_{k=1}^n} - T_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n}A_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n} u_{i\kr{j_k}_{k=1}^n}} = 0 \,,
\end{align*}


which is satisfied by the relation

\begin{align*}
\frac{A_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n}}{A_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n}} = \frac{T_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n}u_{i\kr{j_k}_{k=1}^n}}{T_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n}u_{i\kr{\ell_k}_{k=1}^n}} \,,
\end{align*}

where $T$ is a distribution that we propose and $A$ is a probability to accept the proposition. We reach equilibrium faster when the transition from states are as large as possible, which means $A_{\kr{j_k}_{k=1}^n \kr{\ell_k}_{k=1}^n}$ should be as close to 1 as possible and still satisfy that $A_{\kr{\ell_k}_{k=1}^n \kr{j_k}_{k=1}^n} \leq 1$, which is the Metropolis-Hastings algorithm;

\begin{align}
A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} = \min\p{1,\frac{T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n}u_{i\kr{\ell_k}_{k=1}^n}}{T_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n}u_{i\kr{j_k}_{k=1}^n}}}\,,
\label{eq_48}
\end{align}

where the two states change accordingly to a Master equation when we transition from $\kr{j_k}_{k=1}^2$ to $\kr{\ell_k}_{k=1}^2$ 

\begin{align}
u_{(i+1)\kr{j_k}_{k=1}^n} &=  T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} u_{i\kr{\ell_k}_{k=1}^n} + \p{1-T_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n}} u_{i\kr{j_k}_{k=1}^n}
\label{eq_49}\\
u_{(i+1)\kr{\ell_k}_{k=1}^n} &= T_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n} u_{i\kr{j_k}_{k=1}^n} + \p{1-T_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} A_{\kr{\ell_k}_{k=1}^n\kr{j_k}_{k=1}^n}} u_{i\kr{\ell_k}_{k=1}^n} \,.
\label{eq_50}
\end{align} 

\subsubsection{Wall boundary}

The Markov chain give insight into how to treat a boundary that is a wall. A wall is a boundary that does not allow transitions beyond it. If we look at direction probability $A$ in \eqref{eq_43} and assume that our wall sets $A_{\kr{j_k-\delta_{kp}}_{k=1}^n\kr{j_k}_{k=1}^n}=0$ we get

\begin{align*}
A_{\kr{j_k+\delta_{kp}\ell}_{k=1}^n\kr{j_k}_{k=1}^n} + \sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n \p{A_{\kr{j_k+\delta_{km}\ell}_{k=1}^n\kr{j_k}_{k=1}^n}+A_{\kr{j_k-\delta_{km}\ell}_{k=1}^n\kr{j_k}_{k=1}^n}} = 1 \,,
\end{align*}

and imposing isotropy the direction probability at the wall becomes

\begin{align*}
A = A_{\kr{j_k}_{k=1}^n\kr{\ell_k}_{k=1}^n} = \frac{1}{2n-1} \,.
\end{align*}

Now our Master equation becomes

\begin{align*}
u_{(i+1)\kr{j_k}_{k=1}^n} = \frac{1}{2n-1}\sum_\ell p_\ell \p{u_{i\kr{j_k+\delta_{kp}\ell}_{k=1}^n}+\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n \p{u_{i\kr{j_k+\delta_{km}\ell}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}\ell}_{k=1}^n}}}
\end{align*}

which can be rewritten to 

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} =& \sum_{\ell}\frac{p_\ell \p{\ell \Delta x}^2}{\p{2n-1}\Delta t}\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n\frac{u_{i\kr{j_k+\delta_{km}\ell}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}\ell}_{k=1}^n}}{\p{\ell\Delta x}^2} 
\\
&\quad +\sum_{\ell}\frac{p_\ell\ell \Delta x}{\p{2n-1}\Delta t} \frac{u_{i\kr{j_k+\delta_{kp}\ell}_{k=1}^n} - u_{i\kr{j_k}_{k=1}^n}}{\ell\Delta x} \,.
\end{align*}

When we have the Kronecker delta distribution $p_\ell = \delta_{1\ell}$ this becomes the following discretization

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} =& \frac{\Delta x^2}{\p{2n-1}\Delta t}\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n\frac{u_{i\kr{j_k+\delta_{km}}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}}_{k=1}^n}}{\Delta x^2} 
\\&\quad + \frac{\Delta x}{\p{2n-1}\Delta t} \frac{u_{i\kr{j_k+\delta_{kp}\ell}_{k=1}^n} - u_{i\kr{j_k}_{k=1}^n}}{\Delta x} \,.
\end{align*}

Relating to the diffusion constant $D$ gives

\begin{align}
\Delta x = \sqrt{\p{2n-1}D\Delta t} \,,
\label{eq_51}
\end{align}

and in general

\begin{align}
\Delta x_\ell = \ell \Delta x = \ell\sqrt{\p{2n-1}D\Delta t}
\label{eq_52}
\end{align}

which results in the discretization

\begin{align*}
\frac{u_{(i+1)\kr{j_k}_{k=1}^n}-u_{i\kr{j_k}_{k=1}^n}}{\Delta t} =& D\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n\frac{u_{i\kr{j_k+\delta_{km}}_{k=1}^n} - 2 u_{i\kr{j_k}_{k=1}^n} + u_{i\kr{j_k-\delta_{km}}_{k=1}^n}}{\Delta x^2} 
\\&\quad + \frac{D}{\Delta x} \frac{u_{i\kr{j_k+\delta_{kp}}_{k=1}^n} - u_{i\kr{j_k}_{k=1}^n}}{\Delta x} \,.
\end{align*}

To avoid that the above equation diverges for smaller an smaller step sizes, we realize that the we must satisfy the following the partial differential equation at the wall boundary

\begin{align}
\frac{\partial u\p{\kr{x_i}_{i=1}^n, t}}{\partial t} = D\sum_{\scriptsize\begin{matrix}m=1\\ m\neq p\end{matrix}}^n \frac{\partial^2 u\p{\kr{x_i}_{i=1}^n, t}}{\partial {x_m}^2} \qquad \text{and} \qquad \frac{\partial u\p{\kr{x_i}_{i=1}^n, t}}{\partial x_p} = 0 \,,
\label{eq_53}
\end{align}

and we have proven the ansatz that I made in the analytical derivation where the boundary $u\p{0,x_2,t}$ is determined by $u_2\p{x_2,t}$ which yielded the solution on the boundary as in \eqref{eq_6}.

\section{Implementation}

\subsection{Chains}

An important part of the Monte Carlo simulation of diffusion is memory management of the particles that are added and removed continuously in the simulation. Regular arrays are inefficient to use as storage in this case, which would require continuously allocation and deallocation of the complete array of particles for every time we add or remove a particle. Therefore I made a \texttt{struct Chain} where it's objects points to two neighbour objects in such a way that it form a chain that is similar to an array. Each \texttt{Chain} object contain an template object of data, so it's functionality is like an array. However the advantage is that you can break the chain at any point you want and remove or add elements and recombine the chain again without having to reallocate the memory of the other elements in the chain. This is achieved by the functions \texttt{Chain::Add} and \texttt{Chain::Remove}.

\begin{lstlisting}[title={\texttt{template<class T> struct Chain}}]
template<class T> struct Chain {
	unsigned int N;
	T e;
	Chain<T>* owner;
	Chain<T>* prev;
	Chain<T>* next;
	
	Chain() {
		N = 0;
		owner = this;
		prev = this;
		next = this;
	}
	
	~Chain() {
		if(N) {
			Chain<T>* p = this;
			for(int i = 1; i < N; i++)
				p = p->next;
			for(int i = 0; i < N; i++) {
				delete p->next;
				p = p->prev;
			}
		}
	}
	
	void Add(T element) {				// Add next
		if(next == this) {
			next = new Chain<T>;
			next->next = next;
		}
		else {
			next->prev = new Chain<T>;
			next->prev->next = next;
			next = next->prev;
		}
		owner->N++;
		next->owner = owner;
		next->prev = this;
		next->e = element;
	}
	
	void Remove() {             // Remove next
		if(next->next == next) {
			delete next;
			next = this;
		} else {
			next = next->next;
			delete next->prev;
			next->prev = this;
		}
		owner->N--;
	}
};
\end{lstlisting}

\subsection{Space}

Since this project makes extensive use of 2D arrays, it was important to make simple use of multidimensional arrays, since there is a lot of hazel to manage multidimensional arrays. For generality I made \texttt{struct Space} to make it easy to allocate and deallocate memory of an arbitrary dimensional array, where the dimension is set by the template argument \texttt{D}. To enable this feature I needed a generic way to represent an arbitrary dimensional pointer, which I did with a very simple \texttt{struct Pointer} that inherit itself till the correct dimensionality of the pointer is reached. This is done by adding an astric for each parent, and the dimensionality of the pointer is given by a template argument \texttt{D}.

\begin{lstlisting}[title={\texttt{struct Pointer}}]
template<typename T, unsigned int D> struct Pointer : Pointer<T*,D-1> {
};

template<typename T> struct Pointer<T,0> {
	typedef T Type;
};
\end{lstlisting} 

It's not only memory management that is useful operation to do on a multidimensional array. Because the \texttt{struct Space} enables a easy way to traverse all the elements in a multidimensional array by recursively call same member function, it was natural to equip \texttt{Space} with different functionality to operate on an multidimensional array, for instance mathematical operation as adding two multidimensional arrays through \texttt{Space<T,D>::Add}, or normalize all the elements through \texttt{Space<T,D>::Normalize}. I also made a easy to use function for writing an arbitrary dimensional array to file \texttt{Space<T,D>::ToFile}, and I also made functions in Python in \href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%205/plot.py}{plot.py} to read this files and rebuild it to a multidimensional array ready to plot. \linebreak

A very important functionality in the Monte Carlo simulations in this project was to map the position of a particle into a multidimensional array, therefore I made a function \texttt{Space<T,D>::Map} that takes a chain of particles maps into a space that is spanned out by a multidimensional array. \linebreak

Because of the extensive and important use of \texttt{Space} in my work in this project, I will display it's code here:

\begin{lstlisting}[title={\texttt{struct Space}}]
template<typename T, unsigned int D> struct Space {

	static void Add(typename Pointer<T,D>::Type s1, typename Pointer<T,D>::Type s2, unsigned int n[D]) {
		for(int i = 0; i < n[0]; i++)
			Space<T,D-1>::Add(s1[i], s2[i], &n[1]);
	}
	
	static typename Pointer<T,D>::Type Allocate(unsigned int n[D]) {
		typename Pointer<T,D>::Type array = new typename Pointer<T,D-1>::Type[n[0]];
		for(int i = 0; i < n[0]; i++)
			array[i] = Space<T,D-1>::Allocate(&n[1]);
		return array;
	}
	
	static void ArrayFile(ofstream& file, typename Pointer<T,D>::Type array, unsigned int n[D]) {
		for(int i = 0; i < n[0]; i++) {
			if(D == 1 && i)
				file << '\t';
			else if(D > 1 && i)
				file << endl;
			Space<T,D-1>::ArrayFile(file, array[i], &n[1]);
		}
	}
	
	template<typename V> static typename Pointer<T,D>::Type Cast(typename Pointer<V,D>::Type space, unsigned int n[D]) {
		typename Pointer<T,D>::Type array = new typename Pointer<T,D-1>::Type[n[0]];
		for(int i = 0; i < n[0]; i++)
			array[i] = Space<T,D-1>::Cast(space[i], &n[1]);
		return array;
	}
	
	static void Deallocate(typename Pointer<T,D>::Type array, unsigned int n[D]) {
		for(int i = 0; i < n[0]; i++)
			Space<T,D-1>::Deallocate(array[i], &n[1]);
		delete [] array;
	}
	
	static void DeRange(T** range) {
		for(int i = 0; i < D; i++)
			delete [] range[i];
		delete [] range;
	}
	
	template<typename V> static typename Pointer<V,D>::Type Map(Chain<Vector<T,D>>& chain, T* range[D], unsigned int n[D]) {
		auto space = Space<V,D>::Allocate(n);
		auto p = chain.owner;
		for(unsigned int i = 0; i < chain.N; i++) {
			p = p->next;
			Space<T,D>::Mapping<V>(space, p->e.e, range, n);
		}
		return space;
	}
	
	template<typename V> static void Map(typename Pointer<V,D>::Type& space, Chain<Vector<T,D>>& chain, T* range[D], unsigned int n[D]) {
		auto p = chain.owner;
		for(unsigned int i = 0; i < chain.N; i++) {
			p = p->next;
			Space<T,D>::Mapping<V>(space, p->e.e, range, n);
		}
	}
	
	template<typename V> static void Mapping(typename Pointer<V,D>::Type space, T element[D], T* range[D], unsigned int n[D]) {
		unsigned int m = n[0]-1;
		if(element[0] >= range[0][0] || element[0] <= range[0][m]) {
			for(unsigned int i = 0; i < m; i++) {
				if(element[0] < (range[0][i]+range[0][i+1])/2) {
					Space<T,D-1>::template Mapping<V>(space[i], &element[1], &range[1], &n[1]);
					return;
				}
			}
			Space<T,D-1>::template Mapping<V>(space[m], &element[1], &range[1], &n[1]);
		}
	}
	
	static T Max(typename Pointer<T,D>::Type space, unsigned int n[D]) {
		T val, max = 0;
		for(unsigned int i = 0; i < n[0]; i++) {
			val = Space<T,D-1>::Max(space[i], &n[1]);
			if(val > max)
				max = val;
			}
		return max;
	}
	
	static void Normalize(typename Pointer<T,D>::Type space, unsigned int n[D], T val) {
		for(int i = 0; i < n[0]; i++)
			Space<T,D-1>::Normalize(space[i], &n[1], val);
	}
	
	template<typename V> static typename Pointer<T,D>::Type Normalize(typename Pointer<V,D>::Type space, unsigned int n[D], V val) {
		typename Pointer<T,D>::Type array = new typename Pointer<T,D-1>::Type[n[0]];
		for(int i = 0; i < n[0]; i++)
			array[i] = Space<T,D-1>::template Normalize<V>(space[i], &n[1], val);
		return array;
	}
	
	static T** Range(T lower[D], T upper[D], unsigned int n[D]) {
		T** array = new T*[D];
		for(int i = 0; i < D; i++) {
			T step = (upper[i]-lower[i])/(n[i]-1);
			array[i] = new T[n[i]];
			array[i][0] = lower[i];
			for(int j = 1; j < n[i]; j++)
				array[i][j] = array[i][j-1] + step;
			}
		return array;
	}
	
	static void RangeFile(ofstream& file, T* range[D], unsigned int n[D]) {
		for(int i = 0; i < D; i++) {
			if(i)
				file << endl;
			Space<T,1>::ArrayFile(file, range[i], &n[i]);
		}
	}
	
	static void ToFile(ofstream& file, T* range[D], typename Pointer<T,D>::Type array, unsigned int n[D]) {
		file << D << endl;
		RangeFile(file, range, n);
		file << endl;
		ArrayFile(file, array, n);
	}
};

template<typename T> struct Space<T,0> {

	static void Add(T& s1, T& s2, unsigned int[0]) {
		s1 += s2;
	}
	
	static T Allocate(unsigned int[0]) {
		return T(0);
	}
	
	static void ArrayFile(ofstream& file, T array, unsigned int[0]) {
		file << array;
	}
	
	template<typename V> static T Cast(V& space, unsigned int[0]) {
		return static_cast<T>(space);
	}
	
	static void Deallocate(T, unsigned int[0]) {
	}

	template<typename V> static void Mapping(typename Pointer<V,0>::Type& space, T[0], T*[0], unsigned int[0]) {
		space++;
	}
	
	static T Max(T space, unsigned int[0]) {
		return space;
	}
	
	static T Normalize(T& space, unsigned int[0], T val) {
		space /= val;
	}
	
	template<typename V> static T Normalize(V space, unsigned int[0], V val) {
		return static_cast<T>(space) / static_cast<T>(val);
	}
};
\end{lstlisting}

\subsection{Experiment}

The Monte Carlo simulation is a stochastic method and is therefore of interest to run them in many experiments to get a better result. In fact the Monte Carlo simulation I did in this project I used only 1 seeding particle for each experiment, and the result from one 1 experiment is unusable, but with many experiments one gets good result.  \linebreak

A experiment in general can have arbitrary number of input parameters, therefore I made a variadic template function that takes a arbitrary number of input parameters that are sent to a function pointer for the experiment that we want to run many times. Each experiment should return a chain that is mapped and accumulated in a multidimensional array. And when the experiments are done the multidimensional array are normalized with the largest value in the array.

\begin{lstlisting}[title={\texttt{Experiment}}]
template<typename V, typename T, unsigned int D, typename C, typename... P> typename Pointer<T,D>::Type Experiment(V N, T* x[D], unsigned int n[D], Delegate<C,Chain<Vector<T,D>>,P...> experiment, P... arg) {

	auto space = Space<V,D>::Allocate(n);
	
	for(int i = 0; i < N; i++) {
		Chain<Vector<T,D>> states = experiment(arg...);
		Space<T,D>::template Map<V>(space, states, x, n);
	}
	
	auto space2 = Space<T,D>::template Normalize<V>(space, n, Space<V,D>::Max(space, n));
	Space<V,D>::Deallocate(space, n);
	return space2;
}
\end{lstlisting}  

\subsection{Monte Carlo}

I made a Monte Carlo method that solves 1D and 2D diffusion problem which has only 1 seeding particle at the source. Which means that when this seeding particle moves, a new particle seeding particle is added at the source to take the place of the old seeding particle.

\begin{lstlisting}[title={\texttt{Diffusion1D\_MonteCarlo}}]
template<typename C, typename T> Chain<Vector<T,1>> Diffusion1D_MonteCarlo(T t, T dt, T d, Delegate<C,T> step) {

	T val;
	unsigned int n = t / dt;
	T dx = sqrt(2*dt);
	uniform_real_distribution<T> pdf(0.0,1.0);	// Distribution for accepting a move
	default_random_engine rng;									// Set RNG seeding value
	rng.seed(chrono::high_resolution_clock::now().time_since_epoch().count());

	Chain<Vector<T,1>> particles, *p;
	Vector<T,1> vec = {0};
	particles.Add(vec);													// Seeding particle

	for(int i = 0; i < n; i++) {								// Loop of timesteps
		p = &particles;
		for(int j = 0; j < particles.N; j++) {		// Loop of particles
			val = pdf(rng);													// Random number
			p = p->next;														// Next particle
			
			if(val <= 0.5) {												// Sampling rule
			
				val = p->e.e[0] - dx * step();				// Calculate backward move
				if(val > 0)														// Valid move
					p->e.e[0] = val;
				
			} else {
			
				if(p->e.e[0] == 0) {									// Particle at the source
					p->prev->Add(p->e);									// Add new particle at prev
					j++;
				}
				p->e.e[0] += dx * step();							// Forward move particle
				if(p->e.e[0] >= d) {									// Remove particle
					p = p->prev;												// into the postsynaptic
					p->Remove();
					j--;
				}
			}
		}
	}
	
	return particles;
}
\end{lstlisting}  

\begin{lstlisting}[title={\texttt{Diffusion2D\_MonteCarlo}}]
template<typename C, typename T> Chain<Vector<T,2>> Diffusion2D_MonteCarlo(T t, T dt, T d[2], T w[2], Delegate<C,T> step) {

	T val;
	unsigned int n = t / dt;
	T dx = sqrt(4*dt);
	T dx0 = sqrt(3*dt);
	uniform_real_distribution<T> pdf(0.0,1.0);	// Distribution for accepting a move
	default_random_engine rng;									// Set RNG seeding value
	rng.seed(chrono::high_resolution_clock::now().time_since_epoch().count());

	Chain<Vector<T,2>> particles, *p;
	T dw = w[1]-w[0];
	Vector<T,2> vec = {0,0};
	vec.e[1] = w[0] + pdf(rng)*dw;							// Initial position
	particles.Add(vec);													// of seeding particles
	
	for(int i = 0; i < n; i++) {								// Loop of timesteps
		p = &particles;
		for(int j = 0; j < particles.N; j++) {		// Loop of particles
			p = p->next;														// Next particle
			val = pdf(rng);													// Random number
			
			if(p->e.e[0] == 0) {										// Wall boundary
			
				if(val <= (T)1/3) {										// x direction
			
					if(p->e.e[1] >= w[0] && p->e.e[1] <= w[1]) {
						vec.e[1] = w[0] + pdf(rng)*dw;
						p->prev->Add(vec);								// Add new particle
						j++;
					}
					p->e.e[0] += dx0 * step();					// Forward move particle
					if(p->e.e[0] >= d[0]) {							// Remove particle
						p = p->prev;											// into the postsynaptic
						p->Remove();
						j--;
					}
				
				} else if(val <= (T)2/3) {						// y direction
			
					val = p->e.e[1] - dx0 * step();
					if(val < w[0] || val > w[1]) {
						p->e.e[1] = val;									// Backward move
						if(p->e.e[1] <= 0) {							// Remove particle
							p = p->prev;										// outside cleft
							p->Remove();
							j--;
						}
					}
				
				} else {
			
					val = p->e.e[1] + dx0 * step();         
					if(val < w[0] || val > w[1]) {
						p->e.e[1] = val;									// Backward move
						if(p->e.e[1] >= d[1]) {						// Remove particle
							p = p->prev;										// outside cleft
							p->Remove();
							j--;
						}
					}
				}
			
			} else {																// Free particle
				
				if(val <= 0.25) {											// x direction

					val = p->e.e[0] - dx * step();			// Backward move
					if(val > 0)													// Valid move
						p->e.e[0] = val;
				
				} else if(val <= 0.5) {
					
					p->e.e[0] += dx * step();						// Forward move
					if(p->e.e[0] >= d[0]) {							// Remove particle
						p = p->prev;											// into the postsynaptic
						p->Remove();
						j--;
					}
				
				} else if(val <= 0.75) {							// y direction
					
					p->e.e[1] -= dx * step();						// Backward move
					if(p->e.e[1] <= 0) {								// Remove particle
						p = p->prev;											// outside cleft
						p->Remove();
						j--;
					}
				
				} else {
					
					p->e.e[1] += dx * step();						// Backward move
					if(p->e.e[1] >= d[1]) {							// Remove particle
						p = p->prev;											// outside cleft
						p->Remove();
						j--;
					}
				}
			}
		}
	}
	
	return particles;
}
\end{lstlisting}

\subsection{Metropolis}

I made a Metropolis algorithm that finds the equilibrium state of 1D diffusion problem.

\begin{lstlisting}[title={\texttt{Metropolis}}]
template<typename T> void Metropolis(T T1, T& w1, T T2, T& w2) {
	T a1 = T1 * w1;
	T a2 = T2 * w2;
	T A;
	
	if(a1 <= a2)				// Metropolis algorithm
		A = 1;
	else
	A = a2/a1;
	
	w1 += A*a2 - A*a1;	// Master equation
	w2 += A*a1 - A*a2;
}
\end{lstlisting}

The implementation of the Metropolis algorithm that I used was a forward and backward loop over the lattice elements which is done $N$ times.

\begin{lstlisting}[title={\texttt{Diffusion1D\_Metropolis}}]
template<typename T> T* Diffusion1D_Metropolis(unsigned long N, unsigned int n) {
	
	T val;
	auto s = Space<T,1>::Allocate(&n);
	s[0] = 1;
	
	unsigned int j = 0;
	for(unsigned long i = 0; i < N; i++) {

		for(j = 1; j < n; j++)
			Metropolis(0.5, s[j-1], 0.5, s[j]);		// Forward move
		s[0] = 1;
		s[n-1] = 0;
		
		for(j = n-1; j; j--)
			Metropolis(0.5, s[j], 0.5, s[j-1]);		// Backward move
		s[0] = 1;
		s[n-1] = 0;
	}
	return s;
}
\end{lstlisting}

\subsection{Explicit 2D solver}

I made a explicit solver of the 2D diffusion as described in \eqref{eq_17}.

\begin{lstlisting}[title={\texttt{Diffusion2D\_Explicit}}]
template<typename T> T** Diffusion2D_Explicit(T alpha, T t, T d[2], T w[2], unsigned int n[2]) {

	Vector<unsigned int,2> m = Diffusion2D_Source(d[1], w, n[1]);
	T** u = Diffusion2D_Initialize<T>(m, n);
	T** v = Diffusion2D_Initialize<T>(m, n);
	T** z;
	Vector<T,2> dx2 = Diffusion2D_deltaX2(d, n);
	T dt = Diffusion2D_deltaT<T>(alpha, dx2);				// Calculate time step
	Vector<T,2> a = Diffusion2D_alpha<T>(dt, dx2);	// Alpha values
	unsigned int nt = t/dt;
	
	n[0]--;
	n[1]--;
	for(int i = 0, j, k; i < nt; i++) {							// Number of time steps
		
		for(k = 1; k < m.e[0]; k++) {									// Left wall boundary	
			v[0][k] = u[0][k];
			v[0][k] += a.e[1]*(u[0][k+1] - 2.0*u[0][k] + u[0][k-1]);
		}
		for(k = m.e[1]+1; k < n[1]; k++) {						// Right wall boundary					
			v[0][k] = u[0][k];
			v[0][k] += a.e[1]*(u[0][k+1] - 2.0*u[0][k] + u[0][k-1]);
		}
		
		for(j = 1; j < n[0]; j++) {										// Inner points
			for(k = 1; k < n[1]; k++) {
				v[j][k] = u[j][k];
				v[j][k] += a.e[0]*(u[j+1][k] - 2.0*u[j][k] + u[j-1][k]);
				v[j][k] += a.e[1]*(u[j][k+1] - 2.0*u[j][k] + u[j][k-1]);
			}
		}
		z = u;												// Prepare for next time step
		u = v;
		v = z;
	}
	n[0]++;
	n[1]++;
	Space<T,2>::Deallocate(v, n);
	
	return u;
}
\end{lstlisting}

\subsection{Jacobi 2D solver}

This is the theta implicit scheme for solving a 2D diffusion problem.

\begin{lstlisting}[title={\texttt{Diffusion2D\_Jacobi}}]
template<typename T> T** Diffusion2D_Jacobi(T theta, T alpha, T t,T d[2], T w[2], unsigned int n[2]) {
	
	Vector<unsigned int,2> m = Diffusion2D_Source(d[1], w, n[1]);
	T** u = Diffusion2D_Initialize<T>(m, n);
	T** v = Diffusion2D_Initialize<T>(m, n);
	auto c = Space<T,2>::Allocate(n);
	T** z;
	Vector<T,2> dx2 = Diffusion2D_deltaX2(d, n);
	T dt = Diffusion2D_deltaT<T>(alpha, dx2);				// Calculate time step
	Vector<T,2> a = Diffusion2D_alpha<T>(dt, dx2);	// Alpha values
	unsigned int nt = t/dt;
	
	n[0]--;
	n[1]--;
	
	T diff;
	T c0 = theta * a.e[1];									// Precalculate constants 
	T c1 = 1 + 2 * c0;
	T c2 = theta * a.e[0];
	T c3 = 1 + 2 * (c0 + c2);
	T c4 = a.e[1] - c0;
	T c5 = 1 - 2 * c4;
	T c6 = a.e[0] - c2;
	T c7 = (c5 - 2 * c6) / c3;
	T c8 = c6 / c3;
	T c9 = c4 / c3;
	c5 = c5 / c1;
	c6 = c4 / c1;
	c1 = c0 / c1;
	c4 = c0 / c3;
	c3 = c2 / c3;
	
	for(int i = 0, j, k; i < nt; i++) {			// Number of time steps
		
		for(k = 1; k < m.e[0]; k++)						// Calculate coefficients
			c[0][k] = c5 * u[0][k] + c6 * (u[0][k+1] + u[0][k-1]);
		for(k = m.e[1]+1; k < n[1]; k++)
			c[0][k] = c5 * u[0][k] + c6 * (u[0][k+1] + u[0][k-1]);
		for(j = 1; j < n[0]; j++) {
			for(k = 1; k < n[1]; k++)
				c[j][k] = c7 * u[j][k] + c8 * (u[j+1][k] + u[j-1][k]) + c9 * (u[j][k+1] + u[j][k-1]);
		}
		
		do {																	// Iterate for each time step
			diff = 0;
			
			for(k = 1; k < m.e[0]; k++) {				// Left wall boundary
				v[0][k] = c[0][k] + c1 * (u[0][k+1] + u[0][k-1]);
				diff += abs(v[0][k] - u[0][k]);
			}
			for(k = m.e[1]+1; k < n[1]; k++) {	// Right wall boundary
				v[0][k] = c[0][k] + c1 * (u[0][k+1] + u[0][k-1]);
				diff += abs(v[0][k] - u[0][k]);
			}
			
			for(j = 1; j < n[0]; j++) {					// Inner points
				for(k = 1; k < n[1]; k++) {
					v[j][k] = c[j][k] + c3 * (u[j+1][k] + u[j-1][k]) + c4 * (u[j][k+1] + u[j][k-1]);
					diff += abs(v[j][k] - u[j][k]);
				}
			}
			
			diff /= n[0] * n[1];
			
			z = u;															// Prepare for next iteration
			u = v;
			v = z;
		
		} while(diff > 0.00001);							// Converged?
	}
	
	n[0]++;
	n[1]++;
	Space<T,2>::Deallocate(v, n);
	Space<T,2>::Deallocate(c, n);
	
	return u;
}
\end{lstlisting}

\section{Results}

Order of relative error of $\varv$ to $u$ is calculated by

\begin{align*}
\epsilon = \log_{10}\abs{\frac{\varv - u}{u}} \,.
\end{align*}

\figur{1}{MonteCarlo1D_t0.01.eps}{Monte Carlo simulations for 1D diffusion problem, plotted in 100 lattice. Uniform step is of size $\Delta x$ \eqref{eq_46}, and Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle. Uniform distribution requires smaller time step than the Gaussian distribution to give a good approximation. The problem with uniform distribution is that it leaves grid points empty which should not be empty when the plot grid is smaller than the step size of the simulation. This is not a problem for Gaussian distribution.}{fig1}

\begin{tabell}{|r|c|c|c|}{\small}{$N$ & MCU 1e-3 & MCU 1e-5 & MCG 1e-4 \\}{\input{build-project5-Desktop-Debug/MonteCarlo_1D_time_t0.01.dat}}{Calculation time for the Monte Carlo 1D in seconds for $t=0.01$. MCU = Monte Carlo Uniform, MCG = Monte Carlo Gaussian.}{tab1}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{$N$ & MCU 1e-3 & MCU 1e-5 & MCG 1e-4 \\}{\input{build-project5-Desktop-Debug/MonteCarlo_1D_error_t0.01.dat}}{Order of relative error for the Monte Carlo 1D at $t=0.01$. MCU = Monte Carlo Uniform, MCG = Monte Carlo Gaussian. Relative error of calculated and exact value less than $10^{-2}$ are excluded.}{tab2}
\end{tabell}

\figur{1}{MonteCarlo1D_t1.eps}{Monte Carlo simulations and Metropolis simulation for 1D diffusion problem at equilibrium, plotted in 100 lattice. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle. 1 Metropolis cycle is a loop forward and backward on all the grid points.}{fig2}

\begin{tabell}{|r|cIr|c|}{\small}{$N$ & MCG 1e-3 & $N$ & Metropolis \\}{\input{build-project5-Desktop-Debug/MonteCarlo_1D_time_t1.dat}}{Calculation time for 1D diffusion in seconds for $t=1$. MCG = Monte Carlo Gaussian.}{tab3}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{$N$ & MCG 1e-3 & $N$ & Metropolis \\}{\input{build-project5-Desktop-Debug/MonteCarlo_1D_error_t1.dat}}{Order of relative error for 1D diffusion at $t=1$. MCG = Monte Carlo Gaussian. Relative error of calculated and exact value less than $10^{-1}$ are excluded.}{tab4}
\end{tabell}

\figur{1}{Exact_2D_t0.01_w(0.1,9.9).eps}{Exact solution at $t=0.01$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Which corresponds well with 1D dimensional case in \reffig{fig1}.}{fig3}

\figur{1}{Explicit_2D_t0.01_w(0.1,9.9)_alpha0.49.eps}{Explicit solution in 100x100 lattice at $t=0.01$ with $\alpha = 0.49$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$.}{fig4}

\figur{1}{Explicit_2D_t0.01_w(0.1,9.9)_alpha0.51.eps}{Explicit solution in 100x100 lattice at $t=0.01$ with $\alpha = 0.51$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Instability in the solution because of the convergence criteria is not meet.}{fig5}

\figur{1}{Jacobi_2D_t0.01_w(0.1,9.9)_theta1_alpha100.eps}{Jacobi solution in 100x100 lattice at $t=0.01$ with $\theta=1$ and $\alpha = 100$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$.}{fig6}

\figur{1}{MonteCarlo_2D_t0.01_w(0.1,9.9)_N1.eps}{1 Monte Carlo Gaussian experiment in 100x100 lattice at $t=0.01$ with time step $10^{-4}$ for 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig7}

\figur{1}{MonteCarlo_2D_t0.01_w(0.1,9.9)_N100.eps}{100 Monte Carlo Gaussian experiments in 100x100 lattice at $t=0.01$ with time step $10^{-4}$ for 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig8}

\figur{1}{MonteCarlo_2D_t0.01_w(0.1,9.9)_N10000.eps}{10000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=0.01$ with time step $10^{-4}$ for 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig9}

\figur{1}{Exact_2D_t1_w(0.1,9.9).eps}{Exact solution at $t=1$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Which corresponds well with 1D dimensional case in \reffig{fig2}.}{fig10}

\figur{1}{Explicit_2D_t1_w(0.1,9.9)_alpha0.49.eps}{Explicit solution in 100x100 lattice at $t=1$ with $\alpha = 0.49$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$.}{fig11}

\figur{1}{Jacobi_2D_t1_w(0.1,9.9)_theta1_alpha10000.eps}{Jacobi solution in 100x100 lattice at $t=1$ with $\theta=1$ and $\alpha = 10000$ of 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$.}{fig12}

\figur{1}{MonteCarlo_2D_t1_w(0.1,9.9)_N10000.eps}{10000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $0.1 \leq x_2 \leq 9.9$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig13}

\figur{1}{Exact_2D_t0.1_w(7,8).eps}{Exact solution at $t=0.1$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig14}
\newpage

\figur{1}{Explicit_2D_t0.1_w(7,8)_alpha0.49.eps}{Explicit solution in 100x100 lattice at $t=0.1$ with $\alpha = 0.49$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig15}

\figur{1}{Jacobi_2D_t0.1_w(7,8)_theta1_alpha1000.eps}{Jacobi solution in 100x100 lattice at $t=0.1$ with $\theta=1$ and $\alpha = 1000$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig12}

\figur{1}{MonteCarlo_2D_t0.1_w(7,8)_N10000.eps}{10000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=0.1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig17}

\figur{1}{Exact_2D_t1_w(7,8).eps}{Exact solution at $t=1$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig18}

\figur{1}{Explicit_2D_t1_w(7,8)_alpha0.49.eps}{Explicit solution in 100x100 lattice at $t=1$ with $\alpha = 0.49$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig19}

\figur{1}{Jacobi_2D_t1_w(7,8)_theta1_alpha10000.eps}{Jacobi solution in 100x100 lattice at $t=1$ with $\theta=1$ and $\alpha = 10000$ of 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$.}{fig20}

\figur{1}{MonteCarlo_2D_t1_w(7,8)_N100.eps}{100 Monte Carlo Gaussian experiments in 100x100 lattice at $t=1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig21}

\figur{1}{MonteCarlo_2D_t1_w(7,8)_N10000.eps}{10000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig22}

\figur{1}{MonteCarlo_2D_t1_w(7,8)_N1000000.eps}{1000000 Monte Carlo Gaussian experiments in 100x100 lattice at $t=1$ with time step $10^{-3}$ for 2D diffusion with source boundary of neurotransmitters $7 \leq x_2 \leq 8$. Gaussian step is of size $\Delta x_\ell$  \eqref{eq_47} where $\ell = -\log x$ and $x$ is a uniform random number between 0 and 1, this correspond to mean value 0 and $1 / \sqrt{2}$ standard divination in the Gaussian distribution. 1 Monte Carlo experiment has only 1 seeding particle.}{fig23}

\newpage

\begin{tabell}{|r|c|c|c|}{\small}{Case & Explicit & Jacobi & Monte Carlo 10000 \\}{\input{build-project5-Desktop-Debug/2D_time.dat}}{Calculation time for 2D diffusion in seconds.}{tab5}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{Case & Explicit & Jacobi & Monte Carlo 10000 \\}{\input{build-project5-Desktop-Debug/2D_error.dat}}{Order of relative error for 2D diffusion Relative error of calculated and exact value less than $10^{-1}$ are excluded.}{tab6}
\end{tabell}

\section{Conclusion}

A 2D diffusion has more complexity to consider when solving it than a 1D, especially when it comes to the boundary conditions. But by studying the problem with different methods builds confidence that the considerations made are reasonable. I solved the 2 dimensional heat equation with wall boundary analytically and I verified the assumptions that I made was reasonable with alternative derivation with Markov chains. The advantage of Markov chains is that it's easier to set up the physical model without setting up a mathematical model to solve it. \linebreak

The results show good agreement with each other, however there are difference in when the different methods are most applicable. For the 1D Monte Carlo simulation we clearly see that it's advantages to have a Gaussian step length rather than uniform. This is because the uniform step length have the problem with gaps in plots due to the lattice resolution, which is amended by the Gaussian distribution on the step length which enables the particles to more smoothly distribute out in space and fill the gaps in the plot. Because of the stochastic nature of the Monte Carlo methods results in more noise in the results than other schemes for solving partial differential equation. And we see from project 4 that the Monte Carlo simulations is slower and less accurate for solving a 1D case. The advantage of the Monte Carlo method is that you can do few experiments and still get an good idea of the shape of the result, even though the result has a lot of noise. This means that one can get an good idea of the solution with less computation time. Another advantage is that Monte Carlo methods may be easier to model a physical process, and can help in finding a analytical solution to the problem. And also seems that Monte Carlo methods requires less resources to increase dimensionality of the problem than lattice solvers. This is probably due to that particles does not have to align up in space like a lattice, and can therefore use fewer extra computation points to get a good enough resolution. \linebreak

The best numerical method to solve the 2D diffusion problem is by far the Jacobi iterative method. This is because Jacobi method is numerical stable for all time step sizes, which makes it's much faster than the explicit scheme for longer end times of the results, since the explicit scheme is dependent of small enough time step to converge. 
\newpage

\section{Attachments}

The files produced in working with this project can be found at  \href{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%205}{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%205}\linebreak

The source files developed are

\begin{enumerate}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%205/project5/Array.h}{Array.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%205/project5/Delegate.h}{Delegate.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%205/project5/Experiment.h}{Experiment.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%205/project5/Type.h}{Type.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%205/project5/project5.cpp}{project5.cpp}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%205/plot.py}{plot.py}}
\end{enumerate}


\section{Resources}

\begin{enumerate}
\item{\href{http://qt-project.org/downloads}{QT Creator 5.3.1 with C11}}
\item{\href{https://www.eclipse.org/downloads/}{Eclipse Standard/SDK  - Version: Luna Release (4.4.0) with PyDev for Python}}
\item{\href{http://www.ubuntu.com/download/desktop}{Ubuntu 14.04.1 LTS}}
\item{\href{http://shop.lenovo.com/no/en/laptops/thinkpad/w-series/w540/#tab-reseller}{ThinkPad W540 P/N: 20BG0042MN with 32 GB RAM}}
\end{enumerate}

\begin{thebibliography}{1}
\bibitem{project5}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/projects/project-5-deadline-december-1/project5_diffusion.pdf}{\emph{FYS4150 - Project 5}} - \emph{Diffusion in two dimensions}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{lecture}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/Lecture\%20Notes/lecture2014.pdf}{\emph{Computational Physics - Lecture Notes Fall 2014}}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{project4} \href{mailto:eimundsm@fys.uio.no}{Eimund Smestad}, \href{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%204}{\emph{FYS4150 - Computational Physics - Project 4}}, \href{http://www.uio.no}{University of Oslo}, 2014
\bibitem{Farnell}
Farnell and Gibson, \href{https://vpn2.uio.no/+CSCO+00756767633A2F2F6A6A6A2E667076726170727176657270672E70627A++/science/article/pii/S0021999105001087#}
{\emph{Monte Carlo simulation of diffusion in a spatially
nonhomogeneous medium:}} \emph{A biased random walk
on an asymmetrical lattice}, Journal of Computational Physics Vol. 208 p. 253-265, 2005
\bibitem{Diffusion}\href{http://en.wikipedia.org/wiki/Diffusion\_equation}{http://en.wikipedia.org/wiki/Diffusion\_equation}
\bibitem{Heat}\href{http://en.wikipedia.org/wiki/Heat\_equation}{http://en.wikipedia.org/wiki/Heat\_equation}
\bibitem{Dirichlet}\href{http://en.wikipedia.org/wiki/Dirichlet\_boundary\_condition}{http://en.wikipedia.org/wiki/Dirichlet\_boundary\_condition}
\bibitem{Poisson_eq}\href{http://en.wikipedia.org/wiki/Poisson\%27s\_equation}{http://en.wikipedia.org/wiki/Poisson\%27s\_equation}
\bibitem{Uniqueness}\href{http://en.wikipedia.org/wiki/Uniqueness\_theorem\_for\_Poisson\%27s\_equation}{http://en.wikipedia.org/wiki/Uniqueness\_theorem\_for\_Poisson\%27s\_equation}
\bibitem{Ansatz}\href{http://en.wikipedia.org/wiki/Ansatz}{http://en.wikipedia.org/wiki/Ansatz}
\bibitem{Kronecker}\href{http://en.wikipedia.org/wiki/Kronecker\_delta}{http://en.wikipedia.org/wiki/Kronecker\_delta}
\bibitem{Jacobi}\href{http://en.wikipedia.org/wiki/Jacobi\_method}{http://en.wikipedia.org/wiki/Jacobi\_method}
\bibitem{StochasticProcess}\href{http://en.wikipedia.org/wiki/Stochastic\_process}{http://en.wikipedia.org/wiki/Stochastic\_process}
\bibitem{Stochastic}\href{http://en.wikipedia.org/wiki/Independence\_\%28probability\_theory\%29}{http://en.wikipedia.org/wiki/Independence\_\%28probability\_theory\%29}
\bibitem{MarkovProperty}\href{http://en.wikipedia.org/wiki/Markov\_property}{http://en.wikipedia.org/wiki/Markov\_property}
\bibitem{MarkovProcess}\href{http://en.wikipedia.org/wiki/Markov\_process}{http://en.wikipedia.org/wiki/Markov\_process}
\bibitem{MarkovChain}\href{http://en.wikipedia.org/wiki/Markov\_chain}{http://en.wikipedia.org/wiki/Markov\_chain}
\bibitem{MasterEquation}\href{http://en.wikipedia.org/wiki/Master\_equation}{http://en.wikipedia.org/wiki/Master\_equation}
\bibitem{Metropolis}\href{http://en.wikipedia.org/wiki/Metropolis\%E2\%80\%93Hastings\_algorithm}{http://en.wikipedia.org/wiki/Metropolis\%E2\%80\%93Hastings\_algorithm}
\bibitem{MonteCarlo}\href{http://en.wikipedia.org/wiki/Monte\_Carlo\_algorithm}{http://en.wikipedia.org/wiki/Monte\_Carlo\_algorithm}
\bibitem{Isotropic}\href{http://en.wikipedia.org/wiki/Isotropy}{http://en.wikipedia.org/wiki/Isotropy}
\bibitem{Lax}\href{http://en.wikipedia.org/wiki/Lax\_equivalence\_theorem}{http://en.wikipedia.org/wiki/Lax\_equivalence\_theorem}
\bibitem{WellPosed}\href{http://en.wikipedia.org/wiki/Well-posed\_problem}{http://en.wikipedia.org/wiki/Well-posed\_problem}
\bibitem{Spectral_radius}\href{http://en.wikipedia.org/wiki/Spectral\_radius}{http://en.wikipedia.org/wiki/Spectral\_radius}
\end{thebibliography}

\end{flushleft}
\end{document}
