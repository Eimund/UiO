\documentclass[11pt,english,a4paper]{article}
\include{mitt_oppsett}
\usepackage{fullpage}

\renewcommand\title{FYS4150 - Computational Physics - Project 4}

\renewcommand\author{Eimund Smestad}
%\newcommand\adress{}
\renewcommand\date{\today}
\newcommand\email{\href{mailto:eimundsm@fys.uio.no}{eimundsm@fys.uio.no}}

%\lstset{language=[Visual]C++,caption={Descriptive Caption Text},label=DescriptiveLabel}
\lstset{language=c++}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{black}\bfseries}
\lstset{commentstyle=\itshape\color{black}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\lstset{tabsize=2}

\begin{document}
\maketitle
\begin{flushleft}

\begin{abstract}
In this report I will look at diffusion of neurotransmitter across synaptic cleft by solving the heat equation. I will compare the numerical result from the heat equation with an analytical solution of the diffusion problem. I have implemented the $\theta$ rule to solve the partial differential equation of the heat equation, and for the implicit schemes I will use a tridiagonal matrix solver of $4N$ FLOPS.
\end{abstract}

\section{Diffusion of neurotransmitters}

I will study diffusion as a transport process for neurotransmitters  across synaptic cleft separating the cell membrane of two neurons, for more detail see \cite{project4}. The diffusion equation is the partial differential equation 

\begin{align*}
\frac{\partial u\p{\textbf{x},t}}{\partial t} = \nabla \cdot\p{D\p{\textbf{x},t}\boldsymbol{\nabla} u\p{\textbf{x},t}}\,,
\end{align*}

where $u$ is the concentration of particular neurotransmitters at location $\textbf{x}$ and time $t$ with the diffusion coefficient $D$. In this study I consider the diffusion coefficient as constant, which simplify the diffusion equation to the heat equation

\begin{align*}
\frac{\partial u\p{\textbf{x},t}}{\partial t} = D\nabla^2 u\p{\textbf{x},t}\,.
\end{align*}

It is further assumed that the neurotransmitter concentration $u$ is only dependent on the distance $x$ in direction between the presynaptic to the postsynaptic across the synaptic cleft. Hence we have the differential equation

\begin{align}
\frac{\partial u\p{x,t}}{\partial t} = D\frac{\partial^2 u\p{x,t}}{\partial x^2}\,.
\label{eq_1}
\end{align}

The boundary and initial condition that I'm going to study is 

\begin{align}
\forall t \in\mathbb{R}_{0}: u\p{0,t} = u_0\,, \quad \forall  t\in\mathbb{R} : u\p{d, t} = 0 \quad \text{and} \quad \forall x\in\mathbb{R}_{0+}^{d-} \forall t \in \mathbb{R}^{0}:u\p{x,t}=0\,,
\label{eq_2}
\end{align}

where $u_0$ are kept constant at the presynaptic, $d$ is the distance between the presynaptic and the postsynaptic. Note that the notation $\forall x\in\mathbb{R}_{a+}^{b-} \Leftrightarrow a < x < b$, where as $\forall x\in\mathbb{R}_{a}^{b} \Leftrightarrow a \leq x \leq b$. Note also that these boundary conditions implies that the neurotransmitters are immediately absorbed at the postsynaptic, and for $t<0$ there are no neurotransmitters between the pre- and postsynaptic. \linebreak

To solve the differential equation \eqref{eq_1} with the boundary condition \eqref{eq_2} we start by separating the concentration $u\p{x,t}$ into two functions $u_1\p{x}$ and $u_2\p{x,t}$ in the time and space of the signal transmission of the neurotransmitters

\begin{align}
\forall x \in \mathbb{R}_0^d \forall t \in \mathbb{R}_{0}: u\p{x,t} = u_1\p{x} + u_2\p{x,t} \,,
\label{eq_3}
\end{align}

such that $u_2$ satisfies the Dirichlet boundary condition $u_2\p{0,t}=u_2\p{d,t}=0$, which forces $u_2\p{x,t}$ to be separated into two functions $u_3\p{x}$ and $u_4\p{t}$ as follows

\begin{align}
u_2\p{0,t} = u_2\p{d,t} = 0 \quad \Rightarrow \quad u_2\p{x,t} = u_3\p{x}u_4\p{t} \qquad \text{when $d\neq 0$ and $u_3\p{0}=u_3\p{d}=0$.}
\label{eq_4}
\end{align}

Now putting \eqref{eq_2} and \eqref{eq_3} into \eqref{eq_1} yields

\begin{align*}
u_3\p{x}\frac{\partial u_4\p{t}}{\partial t} = D\p{u_4\p{t}\frac{\partial^2 u_3\p{x}}{\partial x^2} + \frac{\partial^2 u_1\p{x}}{\partial x^2}} \,.
\end{align*}

We wish that $\frac{\partial^2 u_1\p{x}}{\partial x^2} =0$ because then we can separate this partial differential equation by variables, which puts the requirement $u_1\p{x} = a_0 + a_1 x$. Since the separation of $u\p{x,t}$ into $u_1\p{x}$ and $u_2\p{x,t}$ in \eqref{eq_3} can be done arbitrarily without changing the solution of $u\p{x,t}$, means that the requirement $u_1\p{x} = a_0 + a_1 x$ is allowed. However we also need to investigate that $u_1\p{x} = a_0 + a_1 x$ satisfies the boundary condition in \eqref{eq_2};

\begin{align*}
u\p{0,t} &= u_1\p{0} + u_2\p{0,t} = u_1\p{0} = u_0
\\
u\p{d,t} &= u_1\p{d} + u_2\p{d,t} = u_1\p{d} = 0\,,
\end{align*}

where the Dirichlet boundary condition in \eqref{eq_4} are used. And we see that the boundary condition can satisfy the requirement $u_1\p{x} = a_0 + a_1 x$ when

\begin{align}
u_1\p{x} = u_0\p{1-\frac{x}{d}}\,.
\label{eq_5}
\end{align} 

The differential equation in \eqref{eq_1} can now be written as

\begin{align*}
\frac{1}{Du_4\p{t}} \frac{\partial u_4\p{t}}{\partial t} = \frac{1}{u_3\p{x}}\frac{\partial^2 u_3\p{x}}{\partial x^2} = -\lambda^2 \,,
\end{align*}

where $\lambda$ is a constant, because $t$ and $x$ can vary independently. These two equations have the following solution

\begin{align*}
u_3\p{x} &= A\sin\p{\lambda x + \varphi} \qquad \text{and}
\\
u_4\p{t} &= C\e^{-D\lambda^2 t}\,.
\end{align*}

Applying the boundary conditions for $u_3$ in \eqref{eq_4} that $u_3\p{0}=u_4\p{d}=0$ gives

\begin{align*}
\lambda = \frac{n\pi}{d} \qquad \text{for } n \in \mathbb{N}/\kr{0}\,,
\end{align*}

where I let $\mathbb{N}=\mathbb{N}_{-\infty}^{\infty}$ represent all positive and negative integers. Hence

\begin{align*}
u_2\p{x,t} = u_3\p{x}u_4\p{t} = \sum_{n=1} A_n \sin\p{n\pi x} \exp\p{-D\p{\frac{n \pi}{d}}^2 t}\,,
\end{align*}

where the negative values of $n$ is absorbed into the coefficient $A_n$. Applying the initial condition from \eqref{eq_2}

\begin{align}
u\p{x,0} = u_0\p{1-\frac{x}{d}} + \sum_{n=1} A_n \sin\p{n\pi \frac{x}{d}} = 0 \qquad \text{for } x\in\mathbb{R}_0^d\,.
\label{eq_6}
\end{align}

We need to determine the coefficients $A_n$, and the trick is to do something with the equation above such that we isolate the $A_n$ coefficients. To achieve this we use the fact that $\sin\p{n\pi\frac{x}{d}}$ is orthogonal with $\sin\p{m\pi\frac{x}{d}}$ under integration 

\begin{align*}
&\int \sin\p{m\pi\frac{x}{d}}\sin\p{n\pi\frac{x}{d}}\,\mathrm{d}x = -\frac{d}{m\pi}\cos\p{m\pi\frac{x}{d}}\sin\p{n\pi\frac{x}{d}} + \frac{n}{m}\int \cos\p{m\pi\frac{x}{d}}\cos\p{n\pi\frac{x}{d}}\,\mathrm{d}x
\\
&\quad = -\frac{d}{m\pi}\cos\p{m\pi\frac{x}{d}}\sin\p{n\pi\frac{x}{d}} + \frac{n}{m}\p{\frac{d}{\pi m} \sin\p{m\pi\frac{x}{d}} \cos\p{n\pi\frac{x}{d}} + \frac{n}{m}\int \sin\p{m\pi\frac{x}{d}}\sin\p{n\pi\frac{x}{d}}\,\mathrm{d}x} \,,
\end{align*}

where I have used integration by parts $\int u\dot{v} = uv - \int \dot{u}v$. Solving this equation with regard to the integral we get

\begin{align*}
\int \sin\p{m\pi\frac{x}{d}}\sin\p{n\pi\frac{x}{d}}\,\mathrm{d}x = \frac{d}{\pi\p{n^2 - m^2}} \p{n \sin\p{m\pi\frac{x}{d}}\cos\p{n\pi\frac{x}{d}} - m \cos\p{m\pi\frac{x}{d}}\sin\p{n\pi\frac{x}{d}}} \,.
\end{align*}

We want to make the result of this integral zero for $n\neq m$, which is the result if $x=\frac{k d}{2}$ and $x=\frac{\ell d}{2}$ where $k,\ell\in\mathbb{N}$ (for zero,negative and positive integers), because then $\sin\cos$ parts above becomes zero. This means that we need to integrate from $x=\frac{k d}{2}$ and $x=\frac{\ell d}{2}$. However the result above is not defined for $n=m$ because we get $\frac{0}{0}$. So we redo the integration for $n=m$;

\begin{align*}
&\int_{\frac{k d}{2}}^{\frac{\ell d}{2}} \sin^2\p{n\pi\frac{x}{d}}\,\mathrm{d}x 
= -\kl{ \frac{d}{\pi n}\cos\p{n\pi\frac{x}{d}}\sin\p{n\pi\frac{x}{d}}}_{\frac{k d}{2}}^{\frac{\ell d}{2}} + \int_{\frac{k d}{2}}^{\frac{\ell d}{2}} \cos^2\p{n\pi\frac{x}{d}}\,\mathrm{d}x
= \int_{\frac{k d}{2}}^{\frac{\ell d}{2}}  \cos^2\p{n\pi\frac{x}{d}}\,\mathrm{d}x
\\ 
&\quad = \int_{\frac{k d}{2}}^{\frac{\ell d}{2}}  \p{1-\sin^2\p{n\pi\frac{x}{d}}}\,\mathrm{d}x 
= \kl{x}_{\frac{k d}{2}}^{\frac{\ell d}{2}} - \int_{\frac{k d}{2}}^{\frac{\ell d}{2}}  \sin^2\p{n\pi\frac{x}{d}}\,\mathrm{d}x 
= \frac{d}{2}\p{\ell-k} - \int_{\frac{k d}{2}}^{\frac{\ell d}{2}}  \sin^2\p{n\pi\frac{x}{d}}\,\mathrm{d}x 
= \frac{d}{4}\p{\ell-k} \,,
\end{align*}

where I solve the equation with regard to in $\int\frac{d}{2}\p{\ell-k} \sin^2\p{n\pi\frac{x}{d}}\,\mathrm{d}x$ in the last step. I have also used integration by parts $\int u\dot{v} = uv - \int \dot{u}v$ and the Pythagoras trigonometric relation $\sin^2 x+ \cos^2 x = 1$. So the solution of the following integral is

\begin{align*}
\int_{\frac{k d}{2}}^{\frac{\ell d}{2}} \sin\p{m\pi\frac{x}{d}}\sin\p{n\pi\frac{x}{d}}\,\mathrm{d}x = \frac{d}{4}\p{\ell-k}\delta_{mn}\,,
\end{align*}

where $\delta_{mn}$ is the Kronecker delta, and hence I have showed the orthogonality of the above integral. Applying this to \eqref{eq_6};

\begin{align*}
\sum_{n=1}\int_{\frac{k d}{2}}^{\frac{\ell d}{2}} A_n\sin\p{m\pi\frac{x}{d}}\sin\p{n\pi\frac{x}{d}}\,\mathrm{d}x = \int_{\frac{k d}{2}}^{\frac{\ell d}{2}}u_0\sin\p{m\pi\frac{x}{d}}\p{\frac{x}{d}-1}\,\mathrm{d}x\,,
\end{align*}

we can isolate $A_n$ at $n=m$ because of the Kronecker delta $\delta_{nm}$;

\begin{align*}
A_n & = \frac{4}{d\p{\ell-k}} \int_{\frac{k d}{2}}^{\frac{\ell d}{2}}u_0\sin\p{n\pi\frac{x}{d}}\p{\frac{x}{d}-1}\,\mathrm{d}x 
= \frac{4 u_0}{n\p{\ell-k}\pi} \p{-\kl{\p{\frac{x}{d}-1}\cos\p{n\pi\frac{x}{d}}}_{\frac{k d}{2}}^{\frac{\ell d}{2}} + \frac{1}{d}\int_{\frac{k d}{2}}^{\frac{\ell d}{2}}\cos\p{n\pi\frac{x}{d}}\,\mathrm{d}x }
\\
&= \frac{4 u_0}{n\p{\ell-k}\pi} \p{-\kl{\p{\frac{x}{d}-1}\cos\p{n\pi\frac{x}{d}}}_{\frac{k d}{2}}^{\frac{\ell d}{2}} + \frac{1}{n\pi} \kl{\sin\p{n\pi\frac{x}{d}}}_{\frac{k d}{2}}^{\frac{\ell d}{2}} } 
\\
&= \frac{4 u_0}{n\p{\ell-k}\pi}\p{\p{\frac{k}{2}-1}\cos\p{\frac{k n\pi}{2}} - \p{\frac{\ell}{2}-1}\cos\p{\frac{\ell n \pi}{2}} + \frac{1}{n\pi}\p{\sin\p{\frac{\ell n \pi}{2}}-\sin\p{\frac{k n \pi}{2}}}}
\\
&= \frac{4 u_0}{n\p{\ell-k}\pi}\begin{cases} \frac{k-\ell}{2} & \text{when $k n$ and $\ell n$ is even,} \\ \frac{k}{2}-1 + \frac{1}{n\pi} & \text{when $k n$ is even and $\ell n$ is odd,} \\ 1-\frac{\ell}{2} + \frac{1}{n\pi} & \text{when $k n$ is odd and $\ell n$ is even,} \\ 0 & \text{when $k n$ and $\ell n$ is odd.}\end{cases}
\end{align*}

Since $x\in\mathbb{R}_0^d$ in \eqref{eq_6} leads to $k,\ell\in\kr{0,2}$ (because of $\frac{kd}{2}$ and $\frac{\ell d}{2}$ in the integration interval) and $k\neq \ell$ for $A_n$ to apply to the whole interval of $x$, which means that $kn$ and $\ell n$ is even; hence there are only one possibility of the solution above

\begin{align}
A_n = -\frac{2 u_0}{n\pi} \qquad \text{for } x\in\mathbb{R}_0^d\,.
\label{eq_7}
\end{align}

Therefore the analytical solution for the concentration of neurotransmitters in \eqref{eq_3} is given by

\begin{align}
\forall x \in \mathbb{R}_0^d \forall t \in \mathbb{R}_{0}: u\p{x,t} = u_0\p{1-\frac{x}{d} - \sum_{n=1} \frac{2}{n\pi}\sin\p{n\pi x}\exp\p{-D\p{\frac{n\pi}{d}}^2 t}}\,.
\label{eq_8}
\end{align}

\section{Numerical methods}

\subsection{The $\theta$-rule}

The Taylor expansion is given by

\begin{align}
u\p{x} = \sum_{n=0} \frac{\T{u}{(n)}\p{x_0}}{n!}\p{x-x_0}^n
\label{eq_9}
\end{align}

where $\T{u}{(n)} = \frac{\mathrm{d}^n u}{\mathrm{d}t^n}$ and $x_0$ is a initial value where we step from to $x$ . If we now use the first order approximation

\begin{align*}
u\p{x} \approx u\p{x_0} + \T{u}{(1)}\p{x_0}\p{x-x_0} \,.
\end{align*}

The first order differential equation $\T{u}{(1)}\p{x} = f\p{x}$ is determined when we have the initial condition $u\p{x_0}$, however $\T{u}{(1)}\p{x_0}$ is not an initial condition, and it depends on how we calculate it numerically from the initial condition. Now note that $\T{u}{(1)}\p{x_0}$ is the same for different values of $x$ in the approximation above and lets say that we calculate it as given from the approximation above;

\begin{align}
\T{u}{(1)}\p{x_0} \approx \frac{u\p{x}-u\p{x_0}}{x-x_0}\,.
\label{eq_10}
\end{align}

So now use this in another point $x_{\theta} = \theta x + \p{1-\theta}x_0$ which we also approximate to the first order, and if we use the expression above for $\T{u}{(1)}\p{x_0}$ we get

\begin{align}
u\p{x_\theta} &\approx u\p{x_0} + \T{u}{(1)}\p{x_0}\p{x_{\theta}-x_0} = u\p{x_0} + \theta\,\T{u}{(1)}\p{x_0}\p{x-x_0} 
\nonumber\\
&\approx u\p{x_0} + \frac{u\p{x}-u\p{x_0}}{x-x_0}\theta\p{x-x_0} = \theta u\p{x} + \p{1-\theta}u\p{x_0} \,,
\label{eq_11}
\end{align}

this is known as the $\theta$-rule. The $\theta$-rule can be used to approximate the solution of the following first order differential equation

\begin{align}
\T{u}{(1)}\p{x} = f\p{u\p{x}} \,,
\label{eq_12}
\end{align}

where we use \eqref{eq_10} to approximate the expression $\T{u}{(1)}\p{x}$ and given an even better or worse approximation to the solution $u\p{x}$ by approximating $f\p{x}\approx f\p{x_{\theta}}$;

\begin{align*}
\frac{u\p{x}-u\p{x_0}}{x-x_0} \approx f\p{u\p{x_{\theta}}} = f\p{\theta u\p{x} + \p{1-\theta}u\p{x_0}}\,,
\end{align*}

which discretize to

\begin{align}
\frac{u_{i+1}-u_i}{x_{i+1}-x_i} = f\p{\theta u_{i+1} + \p{1-\theta}u_i} \qquad \text{where } i\in\mathbb{N}_0 \text{ and $u_0$ is an initial contidion.} 
\label{eq_13}
\end{align}

We can find the the next step in the numerical solution to \eqref{eq_12} by solving this difference equation with regard to $u_{i+1}$. Note the above discretization is known as Forward Euler scheme (Explicit) when $\theta = 0$, Backward Euler scheme (Implicit) when $\theta = 1$ and Crank-Nicolson scheme when $\theta = \frac{1}{2}$. \linebreak

The truncation error of the Forward and Backward Euler scheme can be found by an alternative derivation, where expand the Taylor series in \eqref{eq_9} around the point $x_0\pm\Delta x$ accordingly;

\begin{align}
u\p{x_0\pm\Delta x} = \sum_{n=0} \frac{\T{u}{(n)}\p{x_0}}{n!}\p{\pm\Delta x}^n\,,
\label{eq_14}
\end{align}

and solve it with regard to $\T{u}{(1)}\p{x_0}$

\begin{align*}
\T{u}{(1)}\p{x_0} = \frac{u\p{x_0\pm\Delta x}-u\p{x_0}}{\Delta x} + \mathcal{O}\p{\Delta x} \,,
\end{align*}

which means that we have a local truncation error of $\mathcal{O}\p{\Delta x}$ with the Forward and Backward Euler scheme. However the Crank-Nicolson scheme can be found by subtraction the Taylor expansion above for the two points

\begin{align*}
u\p{x_0+\Delta x} - u\p{x_0-\Delta x} = 2\sum_{n=1} \frac{\T{u}{\vspace{0.1cm}(2n-1)}\p{x_0}}{\p{2n-1}!}\Delta x^{2n-1}\,,
\end{align*}

and solve it with regard to $\T{u}{(1)}\p{x_0}$

\begin{align*}
\T{u}{(1)}\p{x_0} = \frac{u\p{x_0+\Delta x} - u\p{x_0-\Delta x}}{2\Delta x} + \mathcal{O}\p{\Delta x^2}\,,
\end{align*}

which means that we have a local truncation error of $\mathcal{O}\p{\Delta x^2}$. With the $\theta$-rule we can get even better or worse truncation error, because we can change the $\theta$ value to change the approximation.

\subsection{Second order derivative}

We approximated the first order derivative in \eqref{eq_10}, but we need to approximate the second order derivative to be able to solve the diffusion in \eqref{eq_1}.

Now adding the two expansions in \eqref{eq_14}

\begin{align*}
u\p{x_0+\Delta x} + u\p{x_0-\Delta x} = 2\sum_{n=0} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2n} = 2 u\p{x_0} + \T{u}{(2)}\p{x_0}\Delta x^2 + 2\sum_{n=2} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2n}\,,
\end{align*}

and solve it with

\begin{align*}
\T{u}{(2)}\p{x_0} &= \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2} - 2\sum_{n=2} \frac{\T{u}{(2n)}\p{x_0}}{(2!)}\Delta x^{2(n-1)}
\\ &=  \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2} + \mathcal{O}\p{\Delta x^2}\,,
\end{align*}

So the second order derivative can be approximated with

\begin{align}
\T{u}{(2)}\p{x_0} \approx   \frac{u\p{x_0+\Delta x} - 2 u\p{x_0} + u\p{x_0-\Delta x}}{\Delta x^2}
\label{eq_15}
\end{align}

with the local truncation error $\mathcal{O}\p{\Delta x^2}$.

\subsection{The heat equation} \label{heat_equation}

We want to discretize the dimensionless heat equation from \eqref{eq_1}, where we use $D=1$, $u_0=1$ and $d=1$, 

\begin{align*}
\frac{\partial u\p{x,t}}{\partial t} = \frac{\partial^2 u\p{x,t}}{\partial x^2}\,,
\end{align*}

to numerically solve diffusion of neurotransmitters. First we do the $\theta$-rule discretization in \eqref{eq_13} 

\begin{align*}
\frac{u_{i(j+1)}-u_{ij}}{\Delta t} = \frac{\partial^2 u_{i(j+\theta)}}{\partial x^2} = \frac{\partial^2 \p{\theta u_{i(j+1)}+\p{1-\theta}u_{ij}}}{\partial x^2} = \theta \frac{\partial u_{i(j+1)}}{\partial x^2} + \p{1-\theta}\frac{\partial u_{ij}}{\partial x^2}
\end{align*}

and then implement discretization of the second order in \eqref{eq_15}

\begin{align}
\frac{u_{i(j+1)}-u_{ij}}{\Delta t} = \frac{\theta}{\Delta x^2}\p{u_{(i+1)(j+1)} - 2 u_{i(j+1)} + u_{(i-1)(j+1)}} + \frac{1-\theta}{\Delta x^2}\p{u_{(i+1)j} - 2u_{ij} +u_{(i-1)j}} \,,
\label{eq_16}
\end{align}

where index $i$ is stepping of $x$ and $j$ is stepping of $t$. And from the discussion before we have the local truncation error $\mathcal{O}\p{\Delta t}$ and $\mathcal{O}\p{\Delta x^2}$ for the Forward and Backward Euler scheme, and $\mathcal{O}\p{\Delta t^2}$ and $\mathcal{O}\p{\Delta x^2}$ for Crank-Nicoloson scheme. \linebreak

The dimensionless initial condition from \eqref{eq_4} gives us

\begin{align*}
u_{i0} = \begin{cases} 1 & \text{when $i=0$,} \\ 0 & \text{elsewhere.} \end{cases} 
\end{align*}

The Forward Euler scheme is when $\theta = 0$ which gives explicit solution of \eqref{eq_16}

\begin{align}
u_{i(j+1)} = u_{ij} + \alpha\p{u_{(i+1)j} - 2 u_{ij} + u_{(i-1)j}} \,,
\label{eq_17}
\end{align} 

where

\begin{align*}
\alpha = \frac{\Delta t}{\Delta x^2} \qquad \text{and} \qquad n = \frac{1}{\Delta x} \,.
\end{align*}

When we step forward in time with $j$ Implicitly $\theta > 0$, we use \eqref{eq_16} to find the next time step $j+1$ which is unknown and collect the unknowns on one side of the equation;

\begin{align*}
\begin{array}{l} u_{0(j+1)} \\ -u_{(i-1)(j+1)} +\p{2+\frac{1}{\alpha \theta}}u_{i(j+1)} - u_{(i+1)(j+1)} \\ u_{n(j+1)}\end{array} = \begin{array}{ll} 1 & \text{boundary condition,} \\ \p{\frac{1}{\theta}-1}\p{u_{(i+1)j} - 2u_{ij} + u_{(i-1)j}} + \frac{u_{ij}}{\alpha\theta} & \text{when $i\in\mathbb{N}_1^{n-1}$,}\\ 0  & \text{boundary condition.}\end{array}
\end{align*} 

We can rewrite further to omit the boundary condition

\begin{align}
\begin{array}{l} \p{2+\frac{1}{\alpha \theta}}u_{1(j+1)} - u_{2(j+1)} \\  -u_{(i-1)(j+1)} +\p{2+\frac{1}{\alpha \theta}}u_{i(j+1)} - u_{(i+1)(j+1)} \\ -u_{(n-2)(j+1)} +\p{2+\frac{1}{\alpha \theta}}u_{(n-1)(j+1)} \end{array} = \begin{array}{ll} \p{\frac{1}{\theta}-1}\p{u_{2j} - 2u_{1j} + u_{0j}} + \frac{u_{1j}}{\alpha\theta} + u_{0j} & \text{when $i=1$,} \\ \p{\frac{1}{\theta}-1}\p{u_{(i+1)j} - 2u_{ij} + u_{(i-1)j}} + \frac{u_{ij}}{\alpha\theta} & \text{when $i\in\mathbb{N}_2^{n-2}$,}\\ \p{\frac{1}{\theta}-1}\p{u_{nj} - 2u_{(n-1)j} + u_{(n-2)j}} + \frac{u_{(n-1)j}}{\alpha\theta} + u_{nj} & \text{when $i=n-1$,}\end{array}
\label{eq_18}
\end{align}

where the left side constructs a tridiagonal matrix with the elements $\p{-1,2+\frac{1}{\alpha\theta},-1}$ when we extract the unknowns to a vector. Note that the boundary conditions are added in $i=1$ and $i=n-1$  in addition to the expression for $i\in\mathbb{N}_2^{n-2}$, which we get by taking the Gaussian elimination on row $i=1$ and $i=n-1$ with regard to the row $i=0$ and $i=n$ accordingly. \linebreak

The Explicit solver in \eqref{eq_17} requires $5N$ FLOPS to be solve for each timestep. The implicit solver requires a tridiagonal solver in addition \eqref{eq_18}, and the tridiagonal solver uses $4N$ FLOPS. For the Backward Euler scheme $\theta = 1$ so $\frac{1}{\theta}-1=0$ and we need only one FLOP to calculate \eqref{eq_18} since $\frac{1}{\alpha \theta}$ can be precalculated, and with the tridiagonal solver we then have $5N$ FLOPS. For a general $theta$-rule implicit scheme \eqref{eq_18} we need $6N$ FLOPS to calculate \eqref{eq_18} since $\frac{1}{\theta}-1$ and $\frac{1}{\alpha \theta}$ can be precalculated, and with the tridiagonal solver we then have $10N$ FLOPS.

\subsubsection{Tridiagonal matrix}

We found that heat equation in \eqref{eq_1} can be solved by solving a matrix problem on the form

\begin{align*}
\textbf{A}\textbf{u} = \textbf{v}
\end{align*} 

where $\textbf{u}$ is the unknowns that we want to find, $\textbf{v}$ are given values and $\textbf{A}$ is a tridiagonal matrix on the form $\p{-1,a,-1}$ which we represent with the elements

\begin{align*}
a_{ij} = \begin{cases} a & \text{when $i=j$.} \\ -1 &\text{when $j\in\kr{i-1,i+1}$ and $i,j\in\mathbb{N}_1^{n-1}$,} \\ 0 & \text{otherwise.}\end{cases}
\end{align*}

We then use Gaussian elimination to reduce the tridiagonal matrix $\textbf{A}$ to a upper triangular matrix $\check{\textbf{A}}$ in the following way

\begin{align*}
\check{\textbf{A}}\textbf{u} = \check{\textbf{v}}\,.
\end{align*} 

To find the upper tridiagonal matrix we need to eliminate the elements $a_{i(i-1)}$ so $\check{a}_{i(i-1)} = 0$, which means that we need to multiply the values (without changing them) in row $i-1$ with 

\begin{align*}
\frac{a_{i(i-1)}}{\check{a}_{(i-1)(i-1)}} = -\frac{1}{\check{a}_{(i-1)(i-1)}} 
\end{align*}

and subtract them with the values in row $i$, which leads obviously to

\begin{align*}
\check{a}_{i(i-1)} = a_{i(i-1)} - \frac{a_{i(i-1)}}{\check{a}_{(i-1)(i-1)}} \check{a}_{(i-1)(i-1)} = 0\,.
\end{align*}

We can also show that off-tridiagonal elements in the upper tridiagonal remains zero under this Gaussian elimination

\begin{align*}
\check{a}_{ij} = a_{ij} - \frac{a_{i(i-1)}}{\check{a}_{(i-1)(i-1)}} \check{a}_{(i-1)j} = a_{ij} = 0 \quad \text{when $i\in\mathbb{N}_2^{n-3}$ and $j\in\mathbb{N}_{i+2}^{n-1}$, because $\check{a}_{1j}=a_{1j} = 0$.}
\end{align*}

and similarly that the off-tridiagonal elements in the lower tridiagonal also remains zero

\begin{align*}
\check{a}_{ij} = a_{ij} - \frac{a_{i(i-1)}}{\check{a}_{(i-1)(i-1)}} \check{a}_{(i-1)j} = a_{ij} = 0 \quad \text{when $i\in\mathbb{N}_3^{n-1}$ and $j\in\mathbb{N}_{1}^{i-2}$, because $\check{a}_{i(i-1)}=a_{i(i-1)} = 0$.}
\end{align*}

We can also show that elements $a_{i(i+1)}=-1$ also remains unchanged under this Gaussian elimination

\begin{align*}
\check{a}_{i(i+1)} = a_{i(i+1)} - \frac{a_{i(i-1)}}{\check{a}_{(i-1)(i-1)}} \check{a}_{(i-1)(i+1)} = a_{i(i+1)} = -1 \,.
\end{align*}

This results in that the diagonal elements are then given by

\begin{align}
\check{a}_{ii} = a_{ii} - \frac{a_{i(i-1)}}{\check{a}_{(i-1)(i-1)}} \check{a}_{(i-1)i} = a_{ii} - \frac{1}{\check{a}_{(i-1)(i-1)}} \quad \text{when }\check{a}_{11} = a_{11} = a\,.
\label{eq_19}
\end{align} 

We have now calculated all the elements in the upper triangular matrix $\check{\textbf{A}}$, and we can summarize the the elements as

\begin{align*}
\check{a}_{ij} &= \begin{cases} a_{11} & \text{when $i=j=1$} \\ a_{ii} - \frac{1}{\check{a}_{(i-1)(i-1)}} & \text{when $i=j$ and  $i\in\mathbb{N}_2^{n-1}$} \\ -1 & \text{when $j=i+1$ and $i,j\in\mathbb{N}_1^{n-1}$} \\ 0 & \text{otherwise\,.}\end{cases}
\end{align*}

We also need to do the Gaussian elimination on the vector $\textbf{v}$ as well, which leads to

\begin{align}
\check{v}_i &= \begin{cases} v_1 & \text{when $i=1$} \\ v_i + \frac{\check{v}_{i-1}}{\check{a}_{(i-1)(i-1)}} & \text{when $i\in\mathbb{N}_2^{n-1}$}\,. \end{cases}
\label{eq_20}
\end{align}

We now want to eliminate the upper off-diagonal elements $\check{\textbf{A}}$ with Gaussian elimination so we get at diagonal matrix $\hat{\textbf{A}}$ in the following way

\begin{align*}
\hat{\textbf{A}}\textbf{u} = \hat{\textbf{v}}.
\end{align*}

To find the diagonal matrix we need to eliminate the elements $\check{a}_{i(i+1)}$ so $\hat{a}_{i(i+1)}=0$, which means that we need to multiply the values (without changing them) in row $i+1$ with

\begin{align*}
\frac{\check{a}_{i(i+1)}}{\hat{a}_{(i+1)(i+1)}} = - \frac{1}{\hat{a}_{(i+1)(i+1)}}
\end{align*} 

and subtract them with the values in row $i$, which leads obviously to

\begin{align*}
\hat{a}_{i(i+1)} = \check{a}_{i(i+1)} - \frac{\check{a}_{i(i+1)}}{\hat{a}_{(i+1)(i+1)}}\hat{a}_{(i+1)(i+1)} = 0\,.
\end{align*}

We can also show that off-tridiagonal elements in the upper tridiagonal remains zero under this Gaussian elimination

\begin{align*}
\hat{a}_{ij} = \check{a}_{ij} - \frac{\check{a}_{i(i+1)}}{\hat{a}_{(i+1)(i+1)}} \check{a}_{(i+1)j} = \check{a}_{ij} = 0 \quad \text{when $i\in\mathbb{N}_1^{n-3}$ and $j\in\mathbb{N}_{i+2}^{n-1}$, because $\hat{a}_{i(i+1)}=\check{a}_{i(i+1)} = 0$,}
\end{align*}

and similarly that the off-diagonal elements in the lower tridiagonal also remains zero

\begin{align*}
\hat{a}_{ij} = \check{a}_{ij} - \frac{\check{a}_{i(i+1)}}{\hat{a}_{(i+1)(i+1)}} \hat{a}_{(i+1)j} = a_{ij} = 0 \quad \text{when $i\in\mathbb{N}_2^{n-2}$ and $j\in\mathbb{N}_{1}^{i-1}$, because $\check{a}_{i(i+1)}=a_{i(i+1)} = 0$.}
\end{align*}

This results unchanged diagonal elements

\begin{align*}
\hat{a}_{ii} = \check{a}_{ii} - \frac{\check{a}_{i(i+1)}}{\hat{a}_{(i+1)(i+1)}}\check{a}_{(i+1)i} = \check{a}_{ii} = a_{ii} - \frac{1}{\check{a}_{(i-1)(i-1)}} \qquad \text{where }\check{a}_{11} = a_{11}\,.
\end{align*}

We have now calculated all the elements in the diagonal matrix $\hat{\textbf{A}}$, and we can summarize the elements as

\begin{align*}
\hat{a}_{ij} = \begin{cases} \check{a}_{ii}  & \text{when } i=j \text{ and } i\in\mathbb{N}_{1}^{n-1} \\ 0 & \text{otherwise.} \end{cases}
\end{align*}

We also need to do the Gaussian elimination on the vector $\textbf{v}$ as well, which leads to

\begin{align*}
\hat{v}_i &= \begin{cases} \check{v}_{n-1} & \text{when $i=n-1$} \\ \check{v}_i + \frac{\hat{v}_{i+1}}{\check{a}_{(i+1)(i+1)}} & \text{when $i\in\mathbb{N}_1^{n-2}$}\,. \end{cases}
\end{align*}

To find the unknowns in $\textbf{u}$ we need to make the diagonal matrix $\hat{\textbf{A}}$ to an identity matrix $\textbf{I}$ such that $\bar{\textbf{v}}$

\begin{align*}
\textbf{I}\textbf{u} = \bar{\textbf{v}}\,,
\end{align*}

which we achieve by dividing each row with the diagonal element $a_{ii}$. And the solution is given by

\begin{align}
u_i = \bar{v}_i = \frac{\hat{v}_i}{\check{a}_{ii}} \,.
\label{eq_21}
\end{align}

But we now see that when we calculate $\check{v}_i$ actually uses the previous solution $u_{i+1}$, and we can rewrite $\check{v}_i$ to

\begin{align}
\hat{v}_i &= \begin{cases} \check{v}_{n-1} & \text{when $i=n-1$} \\ \check{v}_i + u_{i+1} & \text{when $i\in\mathbb{N}_1^{n-2}$}\,. \end{cases}
\label{eq_22}
\end{align}

So what we need to calculate the solution in \eqref{eq_21} is first to calculate $\check{a}_{ii}$ in \eqref{eq_19}, then $\check{v}_i$ in \eqref{eq_20} followed by $\hat{v}_i$ \eqref{eq_22}. The calculation of $\check{a}_{ii}$ coefficient in \eqref{eq_19} is actually independent of the input values $v_i$, and can therefore be precalculated. Which means that we need 2N FLOPS to calculate $\check{v}_i$ in \eqref{eq_20}, 1N FLOP to calculate $u_i$ in \eqref{eq_21} and 1N FLOP to calculate $\hat{v}_i$ in \eqref{eq_22}. Hence we need 4N FLOPS to find the solution $u_i$.
\linebreak

\section{Implementation}

\subsection{Property class}

I made myself a \texttt{Property} class to manage public variables of a class in a better way. The purpose of the \texttt{Property} class is to protect public variables of a class, such that the address of the variable can only be changed by the owner of the variable, but still read and write to variable is allowed outside the class. This means that the \texttt{Property} class can't be overwritten outside the class either. For the  \texttt{Property} class to know how to do this it needs a owner type as a template argument in it's declaration, which is done by the template argument \texttt{C} in \texttt{
template<PropertyType A, typename C, typename T, typename... L> class Property}, and the class \texttt{C} is a friend of the \texttt{Property} class, which enables the owner to get access to private and protected instances inside the \texttt{Property} class that no one else gets. Type of the variable is determined by the template argument \texttt{T} and the owner allowed typecast of the variable is determined by template argument \texttt{L} (which is a varadic template argument which can be of an arbitrary length). The access to the variable is controlled by the template argument \texttt{A} which is of the following type

\begin{lstlisting}[title={\texttt{enum class PropertyType}}]
enum class PropertyType {
	ReadWrite,
	ReadOnly,
	WriteOnly
};
\end{lstlisting}

A variable that is protected by a \texttt{Property} class is accessed by read and write as if it's a regular variable, which is achieved by overloading the operators for typecasting and assignment. This is made possible by making \texttt{Property} inherent from \texttt{template<typename T, typename C, typename... L> class TypeCast<T,C,L...>}, which is again inherited by itself until all the typecasts are represented. However the compiler does not know what to do when a variable protected by a \texttt{Property} class is sent to a template function, unless you specify the template arguments explicitly when calling the function. This is due to that the it ambiguous which type is should cast to when it's not specified. Another way to resolve this to use the overloaded operator for \&, which unwraps the variable from the property class by making a copy of it self. \linebreak

The owner of a property variable may want to add user defined set and get function, to enable synchronization of data inside the owner class when a property variable is read or written to. This is made possible by using the classes \texttt{PropertyGet}, \texttt{PropertySet} or \texttt{PropertySetGet} as type to a \texttt{Property} class. This classes requires the use of a \texttt{Delegate} class which contains a function pointer an it's owner, which makes it easier to pass function pointers of class members, because the recipient does not need to know who owns the function pointer, it can just call it as a regular function. \linebreak

In addition I made \texttt{ArrayLength} class that has the length of an array as a variable, and automatically reallocate memory and reinitialize it by the owners wishes. This class does not contain the array it self but rather a pointer to the array. Which enables the array by be declared in regular way in it's owner class, and one don't have to deal with extra overhead code to access it. But it makes life much easier when dealing with arrays that changes sizes, because one don't have to think about reallocation of memory, one just sets the size of the array. \linebreak

This classes that I have described here are made to protected vital variables in a class that are made public, and ensure correct of the class use from outsiders by synchronize the variables. The implementation by the class of this protected variables is bit cumbersome, however when you have tested your class you can be a lot sure that outsiders can't case bugs inside your class. Which again makes it easier to locate bugs when you program, because you can exclude where the bugs may occur.

\subsection{Heat equation solver}

I made a my own class that solves the heat equation as described in section \ref{heat_equation} with  
\texttt{template<typename T, unsigned int D> class HeatEquation}. The template argument \texttt{T} is the variable type that the calculation is done with, and the template argument \texttt{D} is a preparation to support arbitrary dimension of the heat equation. However this implementation only support \texttt{D=1} for the time being. \texttt{HeatEquation} inherits from \texttt{template<typename T, unsigned int D> class Boundary} which handles the boundary conditions. The solver of the \texttt{HeatEquation} supports arbitrarily $\theta$ values in the $\theta$-rule, which is set by \texttt{HeatEquation::Theta} prior to calling the solver.

\begin{lstlisting}[title={\texttt{HeatEquation::Solve()}}]
void Solve() {
	T* _u[D], *_v[D]; // Preparations
	T* u, *v;
	T alpha;
	int n[D+1], _n;
	n[0] = this->n[0];
	for(int i = 1; i <= D; i++)
	n[i] = this->n[i]-2;
	for(int i = 0; i < D; i++) {
		_v[i] = new T[n[i+1]+2];
		_v[i][0] = this->u[i][0];
		_v[i][n[i+1]+1] = this->u[i][n[i+1]+1];
		_u[i] = this->u[i];
	}
	
	if(theta == 1.0) { // Backward Euler Implicit
		for(int i = 1, j, k; i < n[0]; i++) {
			for(j = 0; j < D; j++) {
				
				// Enable faster accessing of arrays
				u = this->u[j];		
				v = _v[j];
				alpha = this->alpha[j];
				_n = n[j+1];
				
				// Calculate eq_18 with theta=1
				v[1] = alpha*u[1] + u[0];
				for(k = 2; k < _n; k++)
					v[k] = alpha*u[k];
				v[_n] = alpha*u[_n] + u[_n+1];
				
				matrix[j]->Solve(&v[1], _n);	// Tridiagonal solver
				
				// Prepare for next timestep
				_v[j] = this->u[j];
				this->u[j] = v;
			}
		}
	} else if(theta) { // Theta Implicit
		for(int i = 1, j, k; i < n[0]; i++) {
			for(j = 0; j < D; j++) {
			
				// Enable faster accessing of arrays
				u = this->u[j];
				v = _v[j];
				alpha = this->alpha[j];
				_n = n[j+1];
				
				// Calculate eq_18
				v[1] = _1_theta*(u[2] - 2.0*u[1] + u[0]) + alpha*u[1] + u[0];
				for(k = 2; k < _n; k++)
					v[k] = _1_theta*(u[k+1] - 2.0*u[k] + u[k-1]) + alpha*u[k];
				v[_n] = _1_theta*(u[_n+1] - 2.0*u[_n] + u[_n-1]) + alpha*u[_n] + u[_n+1];
				
				matrix[j]->Solve(&v[1], _n);	// Tridiagonal solver
				
				// Prepare for next timestep
				_v[j] = this->u[j];
				this->u[j] = v;
			}
		}
	} else { // Forward Explicit
		for(int i = 1, j, k; i < n[0]; i++) {
			for(j = 0; j < D; j++) {
			
				// Enable faster accessing of arrays
				u = this->u[j];
				v = _v[j];
				alpha = this->alpha[j];
				_n = n[j+1];
				
				// Calculate eq_17				
				for(k = 1; k <= _n; k++)
					v[k] = u[k] + alpha*(u[k+1] - 2*u[k] + u[k-1]);
					
				// Prepare for next timestep				
				_v[j] = this->u[j];
				this->u[j] = v;
			}
		}
	}
	
	for(int i = 0; i < D; i++) // Clean up
		delete [] _v[i];
}
\end{lstlisting}

Number of timesteps are set by \texttt{HeatEquation::n[0]}, and number of grid points in $x$ direction is set by \texttt{HeatEquation::n[1]}. The start and end time are set by \texttt{HeatEquation::lower[0]} and \texttt{HeatEquation::upper[0]} accordingly, and the lower and upper boundary of $x$ are set by \texttt{HeatEquation::lower[1]} and \texttt{HeatEquation::upper[1]} accordingly. You are allowed full access to the array of $u\p{x,t}$ through \texttt{HeatEquation::u}, so you can set up the initial conditions prior to calling the solver, and you can read the result afterwards. The elements of \texttt{HeatEquation::u}  are set to zero by calling \texttt{HeatEquation::Initialize}.

\subsection{Tridiagonal solver}

The tridiagonal solver used by the heat equation is implemented in \texttt{template<class T> class Matrix<MatrixType::Tridiagonal\_m1\_C\_m1, T>}, where the template argument \texttt{T} is the type of variable that the calculation is done by. This matrix store only a constant for the main diagonal of the matrix, since this elements are constant, and the matrix looks like $\p{-1,C,-1}$ for each row where $C$ is located on the main diagonal and we have leading and succeeding zeros on the row. Which means that this matrix does not need any memory to represent it expect a constant $C$. However to enable the $4N$-solver we need to store an array of length $N$ (the size of the tridiagonal matrix) for the precalculated values \eqref{eq_19}, which is named \texttt{Matrix<MatrixType::Tridiagonal\_m1\_C\_m1, T>::factor}.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Tridiagonal\_m1\_C\_m1, T>::Solve}}]
bool Solve(T* f, int n) {
	if(this->n < n)		// Checks if the matrix size has changed 
		this->n = n;		// and if so new precalculated values are calculated
	
	n -= 1;
	int i = 0;
	while(i < n)
		f[i] += f[i]*factor[i++];	// eq_20
	while(i) {
		f[i] *= factor[i];				// eq_21
		f[i] += f[i--];						// eq_22
	}
	f[0] /= this->b;
	
	return true;
}
\end{lstlisting}

\section{Result}

Order of relative error of $\varv$ to $u$ is calculated by

\begin{align*}
\epsilon = \log_{10}\abs{\frac{\varv - u}{u}} \,.
\end{align*}

\begin{tabell}{|r|c|c|c|}{\small}{\input{build-project4-Desktop-Debug/time_header.dat}}{\input{build-project4-Desktop-Debug/time_forward_euler.dat}}{Calculation time for the Forward Euler scheme in seconds.}{tab_euler_forward}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{\input{build-project4-Desktop-Debug/time_header.dat}}{\input{build-project4-Desktop-Debug/time_backward_euler.dat}}{Calculation time for the Backward Euler scheme in seconds.}{tab_euler_backward}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{\input{build-project4-Desktop-Debug/time_header.dat}}{\input{build-project4-Desktop-Debug/time_crank_nicoloson.dat}}{Calculation time for the Crank-Nicoloson scheme in seconds.}{tab_crank_nicoloson}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{\input{build-project4-Desktop-Debug/time_header.dat}}{\input{build-project4-Desktop-Debug/time_forward_euler_error.dat}}{Order of relative error of Forward Euler scheme to the exact solution in \eqref{eq_8}, for $t=0.01$, $u_0 = 1$, $d = 1$ and $D = 1$. Relative error of calculated and exact value less than $10^{-6}$ are excluded.}{tab_euler_forward_error}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{\input{build-project4-Desktop-Debug/time_header.dat}}{\input{build-project4-Desktop-Debug/time_backward_euler_error.dat}}{Order of relative error of Backward Euler scheme to the exact solution in \eqref{eq_8}, for $t=0.01$, $u_0 = 1$, $d = 1$ and $D = 1$. Relative error of calculated and exact value less than $10^{-6}$ are excluded.}{tab_euler_backward_error}
\end{tabell}

\begin{tabell}{|r|c|c|c|}{\small}{\input{build-project4-Desktop-Debug/time_header.dat}}{\input{build-project4-Desktop-Debug/time_crank_nicoloson_error.dat}}{Order of relative error of Crank-Nicoloson scheme to the exact solution in \eqref{eq_8}, for $t=0.01$, $u_0 = 1$, $d = 1$ and $D = 1$. Relative error of calculated and exact value less than $10^{-6}$ are excluded.}{tab_crank_nicoloson_error}
\end{tabell}

\figur{0.8}{nx10_a0.49_t0.01.eps}{Exact solution calculated \eqref{eq_8}, FE is Forward Euler, BE is Back Euler and CN is Crank Nicholoson scheme. The following values is set as $u_0 = 1$, $d = 1$ and $D = 1$.}{fig1} 

\figur{0.8}{nx10_a0.1_t0.01.eps}{Exact solution calculated \eqref{eq_8}, FE is Forward Euler, BE is Back Euler and CN is Crank Nicholoson scheme. The following values is set as $u_0 = 1$, $d = 1$ and $D = 1$.}{fig2} 

\figur{0.8}{nx100_a0.49_t0.01.eps}{Exact solution calculated \eqref{eq_8}, FE is Forward Euler, BE is Back Euler and CN is Crank Nicholoson scheme. The following values is set as $u_0 = 1$, $d = 1$ and $D = 1$.}{fig3} 

\figur{0.8}{nx100_a0.51_t0.01.eps}{Exact solution calculated \eqref{eq_8}, FE is Forward Euler, BE is Back Euler and CN is Crank Nicholoson scheme. The following values is set as $u_0 = 1$, $d = 1$ and $D = 1$.}{fig4} 

\figur{0.8}{nx10_a0.49_t1.eps}{Exact solution calculated \eqref{eq_8}, FE is Forward Euler, BE is Back Euler and CN is Crank Nicholoson scheme. The following values is set as $u_0 = 1$, $d = 1$ and $D = 1$.}{fig5} 

\section{Conclusion}

We see from the results that the different solvers that I made for the heat equation gives good approximation to the analytical solution in \eqref{eq_8}. However if we look at the tables \reftab{tab_euler_forward_error}, \reftab{tab_euler_backward_error} and \reftab{tab_crank_nicoloson_error} the Crank-Nicoloson scheme is the most stable, where the solution converges the fastest. This is due that Crank-Nicoloson scheme has second order local truncation error in both time and space, whereas the the Euler schemes have only first order in time and second order in space. The Backward Euler scheme is also stable for all $\alpha$ values, as for Crank-Nicoloson. But the Forward Euler scheme is only stable for $\alpha < \frac{1}{2}$, which I have demonstrated in \reffig{fig3} and \reffig{fig4}. The different solvers converges faster to the stationary solution i \reffig{fig5} than a more dynamic solution in \reffig{fig1}. \linebreak

The Forward Euler scheme is about 3.5 times faster the implicit scheme, seen in \reftab{tab_euler_forward}, \reftab{tab_euler_backward} and \reftab{tab_crank_nicoloson}. This is not in agreement with number of FLOPS needed to solve with these schemes. The Forward and Backward scheme we found to both use $5N$ FLOPS for each timestep, but still the Backward Euler scheme is 3.5 times slower than the Forward Euler scheme. This is due to the tridiagonal matrix solver. Even though the tridiagonal matrix solver is taking into account in the number of FLOPS, it seems to run slower because it accesses two different arrays at the same instances. This also explains why the Crank-Nicoloson scheme is almost as fast as the Backward Euler scheme, even though the Crank-Nicoloson scheme use twice as many FLOPS. \linebreak

We can also see from \reftab{tab_euler_forward_error}, \reftab{tab_euler_backward_error} and \reftab{tab_crank_nicoloson_error} that if we want to have good approximation for small values for the concentration $u\p{x,t}$, we need to increase the number of grid points, where the number timestep does not affect this significantly. The error in these tables are dominated by the values close to zero of $u\p{x,t}$.

\section{Attachments}

The files produced in working with this project can be found at  \href{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%204}{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%204}\linebreak

The source files developed are

\begin{enumerate}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%204/project4/Array.h}{Array.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%204/project4/Boundary.h}{Boundary.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%204/project4/Delegate.h}{Delegate.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%204/project4/HeatEquation.h}{HeatEquation.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%204/project4/Matrix.h}{Matrix.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%204/project4/Property.h}{Property.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%204/project4/Type.h}{Type.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%204/project4/project4.cpp}{project4.cpp}}
\end{enumerate}


\section{Resources}

\begin{enumerate}
\item{\href{http://qt-project.org/downloads}{QT Creator 5.3.1 with C11}}
\item{\href{https://www.eclipse.org/downloads/}{Eclipse Standard/SDK  - Version: Luna Release (4.4.0) with PyDev for Python}}
\item{\href{http://www.ubuntu.com/download/desktop}{Ubuntu 14.04.1 LTS}}
\item{\href{http://shop.lenovo.com/no/en/laptops/thinkpad/w-series/w540/#tab-reseller}{ThinkPad W540 P/N: 20BG0042MN with 32 GB RAM}}
\end{enumerate}

\begin{thebibliography}{1}
\bibitem{project4}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/projects/project-4-deadline-november-10/project4\_2014.pdf}{\emph{FYS4150 - Project 4}} - \emph{Diffusion of neurotransmitters in the synaptic
cleft}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{lecture}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/Lecture\%20Notes/lecture2014.pdf}{\emph{Computational Physics - Lecture Notes Fall 2014}}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{Diffusion}\href{http://en.wikipedia.org/wiki/Diffusion\_equation}{http://en.wikipedia.org/wiki/Diffusion\_equation}
\bibitem{Heat}\href{http://en.wikipedia.org/wiki/Heat\_equation}{http://en.wikipedia.org/wiki/Heat\_equation}
\bibitem{Dirichlet}\href{http://en.wikipedia.org/wiki/Dirichlet\_boundary\_condition}{http://en.wikipedia.org/wiki/Dirichlet\_boundary\_condition}
\bibitem{IntegrationByParts}\href{http://en.wikipedia.org/wiki/Integration\_by\_parts}{http://en.wikipedia.org/wiki/Integration\_by\_parts}
\bibitem{Taylor}\href{http://en.wikipedia.org/wiki/Taylor\_series}{http://en.wikipedia.org/wiki/Taylor\_series}
\bibitem{Forward_Euler}\href{http://en.wikipedia.org/wiki/Euler\_method}{http://en.wikipedia.org/wiki/Euler\_method}
\bibitem{Backward_Euler}\href{http://en.wikipedia.org/wiki/Backward\_Euler\_method}{http://en.wikipedia.org/wiki/Backward\_Euler\_method}
\bibitem{Crank-Nicolson}\href{http://en.wikipedia.org/wiki/Crank\%E2\%80\%93Nicolson\_method}{http://en.wikipedia.org/wiki/Crank\%E2\%80\%93Nicolson\_method}
\bibitem{Gaussian}\href{http://en.wikipedia.org/wiki/Gaussian\_elimination}{http://en.wikipedia.org/wiki/Gaussian\_elimination}
\end{thebibliography}

\end{flushleft}
\end{document}
