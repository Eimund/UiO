\documentclass[11pt,english,a4paper]{article}
\include{mitt_oppsett}
\usepackage{fullpage}

\renewcommand\title{FYS4150 - Computational Physics - Project 1}

\renewcommand\author{Eimund Smestad}
%\newcommand\adress{}
\renewcommand\date{\today}
\newcommand\email{\href{mailto:eimundsm@fys.uio.no}{eimundsm@fys.uio.no}}

\lstset{language=[Visual]C++,caption={Descriptive Caption Text},label=DescriptiveLabel}

\begin{document}
\maketitle
\begin{flushleft}

\begin{abstract}
This report looks at different algorithms to solve the one-dimensional Poisson's equation with Dirichlet boundary condition. The algorithms are compared by speed and floating point error in the solution. The results show how different ways of writing code effect speed and floating point error, and discusses how the machines architecture and functionally are the reason for the performance difference.
\end{abstract}

\section{Theory}

We have the Poisson's equation from electromagnetism stated as

\begin{align*}
\nabla^2 \Phi = - 4 \pi \rho\p{\vec{r}}
\end{align*}

in three dimensional space with the electrostatic potential $\Phi$ generated by local charge distribution $\rho\p{\vec{r}}$. With a spherically symmetric $\Phi$ and $\rho\p{\vec{r}}$ the equations simplifies to a one-dimensional equation in $r$, namely

\begin{align*}
\frac{1}{r^2}\frac{\mathrm{d}}{\mathrm{d}r}\p{r^2\frac{\mathrm{d}\Phi}{\mathrm{d}r}} = -4 \pi \rho\p{r} \,,
\end{align*}

which can be rewritten via a substitution $\Phi\p{r} = \frac{\phi}{r}$ as 

\begin{align*}
\frac{\mathrm{d}^2\phi}{\mathrm{d}r^2} = - 4\pi \rho\p{r} \,,
\end{align*}

which again can be rewritten to

\begin{align*}
-u^{\prime\prime}\p{x} = f\p{x} \,.
\end{align*}

In this project I will solve the following case of the one-dimensional Poisson's equation with Dirichlet boundary condition

\begin{align}
-u^{\prime\prime}\p{x} = f\p{x}\,,  \qquad x\in\p{0,1}\,,\quad u\p{0} = u\p{1} = 0 \,.
\label{eq_1}
\end{align}

\section{Numerical approximation}

Equation \eqref{eq_1} can be approximated by discretizing $u$ as $\varv_i$ with grid points $x_i = i h$ with the interval $x_0 = 0$ and $x_{n+1}=1$;

\begin{align}
-\varv_{i+1} + 2 \varv_i - \varv_{i-1} = b_i \qquad \text{for $\varv_0 = 0$, $\varv_{n+1} = 0$ and $i\in\mathbb{N}_1^n$,}
\label{eq_2}
\end{align}

where $b_i = h^2 f\p{x_i}$ and the step length is given by $h = \frac{1}{n+1}$. This difference equation can be rewritten to matrix form

\begin{align*}
\vec{\vec{A}}\,\vec{\varv} = \vec{b} 
\end{align*}

which we can show to satisfy the difference equation \eqref{eq_2}

\begin{align*}
\begin{bmatrix} a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\ a_{2,1} & a_{2,2} & \cdots & a_{2,n}  \\ \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2} & \cdots & a_{n,n} \end{bmatrix} \begin{bmatrix} \varv_1 \\ \varv_2 \\ \vdots \\  \varv_n \end{bmatrix} 
= \begin{bmatrix} \sum_{j=1}^n a_{1,j} \varv_j \\ \sum_{j=1}^n a_{2,j} \varv_j \\ \vdots \\ \sum_{j=1}^n a_{n,j} \varv_j \end{bmatrix} 
= \begin{bmatrix} a_{1,1} \varv_1 + a_{1,2} \varv_2 \\ a_{2,1} \varv_1 + a_{2,2} \varv_2 + a_{2,3} \varv_3 \\ \vdots \\ a_{n,n-1} \varv_{n-1} + a_{n,n} \varv_n \end{bmatrix} = \begin{bmatrix} 2 \varv_1 - \varv_2 \\ -\varv_1 + 2 \varv_2 - \varv_3 \\ \vdots \\ -\varv_{n-1} + 2 \varv_n \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} \,,  
\end{align*}

where the matrix elements are given by $a_{i,i} = 2$, $a_{i,i\pm 1} = -1$ and $a_{i,i\pm j} = 0$ for $j \geq 2$, and this is called a tridiagonal matrix. Please note the boundary condition in \eqref{eq_1} gives $-\varv_0 + 2 \varv_1 - \varv_2 = 2 \varv_1 - \varv_2$ and $-\varv_{n-1}+ 2\varv_n - \varv_{n+1} = -\varv_{n-1} + 2 \varv_n$, which actually enables us to write the difference equation in \eqref{eq_2} on matrix form without getting in trouble in the corners of the matrix. 

\subsection{LU decomposition} \label{sec_LU}

Gaussian elimination is used to solve linear equations, but if we use the same matrix $A$ to solve for different source terms $b$ it is smarter to decompose the matrix $A$ to a upper and a lower triangle matrix with LU decomposition. The LU decomposition is independent of the source term $b$ and needs only forward and backward substitution to solve the linear equation. Both Gaussian elimination and LU decomposition use $\sim\frac{2}{3}n^3$ FLOPS to be solved; the difference is that the next solution with the same matrix $A$ the Gaussian elimination still needs $\frac{2}{3}n^3$ FLOPS, whereas we already have the LU decomposition and only need forward and backward substitution to find the solution, and forward and backward substitution need $\mathcal{O}\p{n^2}$ FLOPS to be solved (due to double looping). \linebreak

For the LU decomposition I have used \href{http://arma.sourceforge.net/}{armadillo} and made my own forward and backward substitution solver. \href{http://arma.sourceforge.net/}{Armadillo} uses the Doolittle algorithm which gives 1's on the diagonal $l_{i,i}$ on the lower triangle matrix;

\begin{align*}
\vec{\vec{A}} = \vec{\vec{L}}\vec{\vec{U}} = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ l_{2,1} & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ l_{n,1} & l_{n,2} & \cdots & 1 \end{bmatrix} \begin{bmatrix} u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\ 0 & u_{2,2} & \cdots & u_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & u_{n,n} \end{bmatrix} \,.
\end{align*}

The forward substitution is done by

\begin{align*}
\vec{\vec{U}}\vec{\varv} = \vec{\vec{L}}^{-1} \vec{b} &\sim \kl{\left.\begin{matrix} 1 & 0 & \cdots & 0 \\ l_{2,1} & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ l_{n,1} & l_{n,2} & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} b_1 \\ b_2 \\ \vdots \\ b_n\end{matrix}}
= \kl{\left.\begin{matrix} 1 & 0 & \cdots & 0 \\ l_{2,1} & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ l_{n,1} & l_{n,2} & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ b_2 \\ \vdots \\ b_n\end{matrix}}
\sim \kl{\left.\begin{matrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ l_{n,1} & l_{n,2} & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ b_2 - l_{2,1} \check b_1 \\ \vdots \\ b_n\end{matrix}}
\\
&= \kl{\left.\begin{matrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ l_{n,1} & l_{n,2} & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2 \\ \vdots \\ b_n\end{matrix}}
\sim \kl{\left.\begin{matrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2 \\ \vdots \\ b_n -\sum_{i=1}^{n-1} l_{n,i}\check b_i \end{matrix}}
= \kl{\left.\begin{matrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2 \\ \vdots \\ \check b_n \end{matrix}} \,,
\end{align*}

which gives the following algorithm for the forward substitution form Doolittle

\begin{align}
\check b_i = b_i - \sum_{j=1}^{i-1} l_{i,j}\check b_j \qquad \text{with $\check b_1 = b_1$.}
\label{eq_3}
\end{align}

The last step is to do the backward substitution

\begin{align*}
\vec{\varv} = \vec{\vec{U}}^{-1}\vec{\vec{L}}^{-1} \vec{b} &\sim \kl{\left.\begin{matrix}  u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\ 0 & u_{2,2} & \cdots & u_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & u_{n,n}  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2 \\ \vdots \\ \check b_n\end{matrix}} 
\sim \kl{\left.\begin{matrix}  u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\ 0 & u_{2,2} & \cdots & u_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2 \\ \vdots \\ \frac{\check b_n}{u_{n,n}}\end{matrix}} 
= \kl{\left.\begin{matrix}  u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\ 0 & u_{2,2} & \cdots & u_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2 \\ \vdots \\ \varv_n \end{matrix}} 
\\
&\sim\kl{\left.\begin{matrix}  u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \frac{\check b_2-\sum_{i=3}^n u_{2,i}\varv_i}{u_{2,2}} \\ \vdots \\ \varv_n \end{matrix}}
=\kl{\left.\begin{matrix}  u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \varv_2 \\ \vdots \\ \varv_n \end{matrix}}
\\ 
&\sim \kl{\left.\begin{matrix}  u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \frac{\check b_1-\sum_{i=2}^n u_{1,i} \varv_i}{u_{1,1}} \\ \varv_2 \\ \vdots \\ \varv_n \end{matrix}}
= \kl{\left.\begin{matrix}  1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{matrix}\,\,\right\vert\,\,\begin{matrix} \varv_1 \\ \varv_2 \\ \vdots \\ \varv_n \end{matrix}} \,,
\end{align*}

which gives the following algorithm for the backward substitution

\begin{align}
\varv_i = \frac{\check b_i - \sum_{j=i+1}^n u_{i,j}\varv_j}{u_{i,i}} \qquad \text{with $\varv_n = \check b_n$.} 
\label{eq_4}
\end{align}

\subsection{Tridiagonal matrix solver}\label{sec_Tridiagonal}

A tridiagonal matrix can be solved faster than LU decomposition method, even when only doing the forward and backward substitution. We apply the Gaussian elimination to first reduce the tridiagonal matrix to the upper triangle matrix;

\begin{align*}
&\kl{\left.\begin{matrix} a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ a_{2,1} & a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & a_{3,2} & a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & a_{n-1,n-2} & a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & a_{n,n-1} & a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_{n-2} \\ b_{n-1} \\ b_n\end{matrix}} 
\\
&\sim \kl{\left.\begin{matrix} a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 & a_{2,2} - \frac{a_{1,2}a_{2,1}}{a_{1,1}} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & a_{3,2} & a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & a_{n-1,n-2} & a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & a_{n,n-1} & a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} b_1 \\ b_2 - \frac{a_{1,2} b_1}{a_{1,1}} \\ b_3 \\ \vdots \\ b_{n-2} \\ b_{n-1} \\ b_n\end{matrix}}
\\
&=  \kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & a_{3,2} & a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & a_{n-1,n-2} & a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & a_{n,n-1} & a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ b_3 \\ \vdots \\ b_{n-2} \\ b_{n-1} \\ b_n\end{matrix}}
\\
&\sim  \kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ \check b_3 \\ \vdots \\ \check b_{n-2} \\ \check b_{n-1} \\ \check b_n\end{matrix}} \,.
\end{align*}

So the elements in the upper triangle matrix of reduced tridiagonal are given by the following two difference equations

\begin{align}
\check b_i &= b_i - \frac{a_{i,i-1}}{\check a_{i-1,i-1}}\check b_{i-1} && \text{for $\check b_{1} = b_{1}$ and $i\in\mathbb{N}_2^n$,} 
\label{eq_5} 
\\\check a_{i,i} &= a_{i,i} - \frac{a_{i,i-1}}{\check a_{i-1,i-1}}a_{i,i-1} && \text{for $\check a_{1,1} = a_{1,1}$ and $i\in\mathbb{N}_2^n$,}  
\label{eq_6}
\\
a_{i,i + j} &= 0 && \text{for $j \in \mathbb{Z}_{-n}^n / \mathbb{Z}_{-1}^0$.}
\label{eq_7}
\end{align}

If we now continue with Gaussian elimination to reduce tridiagonal further to a diagonal matrix

\begin{align*}
&\kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ \check b_3 \\ \vdots \\ \check b_{n-2} \\ \check b_{n-1} \\ \check b_n\end{matrix}} 
\\ &\sim \kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & 0 \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ \check b_3 \\ \vdots \\ \check b_{n-2} \\ \check b_{n-1} - \frac{a_{n-1,n}}{\check a_{n,n}} \check b_n\\ \check b_n\end{matrix}} 
\\ &= \kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & 0 \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ \check b_3 \\ \vdots \\ \check b_{n-2} \\ \hat b_{n-1} \\ \hat b_n\end{matrix}} 
\\ &\sim \kl{\left.\begin{matrix} \check a_{1,1} & 0 & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & 0 & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & 0 & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & 0 \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \hat b_1 \\ \hat b_2\\ \hat b_3 \\ \vdots \\ \hat b_{n-2} \\ \hat b_{n-1} \\ \hat b_n\end{matrix}} 
\end{align*}

which gives the difference equation

\begin{align}
\hat b_i = \check b_i - \frac{a_{i,i+1}}{\check a_{i+1,i+1}} \hat b_{i+1} = \check b_i - a_{i,i+1} \varv_{i+1} \qquad \text{for $\hat b_n = \check b_n$ and $i\in\mathbb{N}_1^{n-1}$.}
\label{eq_8}
\end{align}

where $\varv_i$ is the solution of the linear equation set represented by a tridiagonal matrix

\begin{align}
\varv_i = \frac{\hat b_i}{\check a_{i,i}} \qquad \text{for $i\in\mathbb{N}_1^n$.}
\label{eq_9}
\end{align}

So the solution for a linear equation set represented by a tridiagonal matrix in \eqref{eq_9} needs iteratively calculate first \eqref{eq_6} and then \eqref{eq_5} when $i$ goes $1\to n$, and then iteratively calculate \eqref{eq_8} when $i$ goes $n\to 1$. The difference equation \eqref{eq_6} requires 3 FLOPS to be calculated for each iteration, but \eqref{eq_5} requires only 2 FLOPS to be calculated for each iteration, because $\frac{a_{i-1,i}}{\check a_{i-1,i-1}}$ is used in both \eqref{eq_5} and \eqref{eq_6} and needs only to be calculated once. Then \eqref{eq_9} is calculated and requires a 1 FLOP, which is followed directly by \eqref{eq_8} which uses \eqref{eq_9} and therefore requires only 2 more FLOPS to be calculated. In total a general linear equation set of $n$ equations that are represented by a tridiagonal matrix requires approximately $8n$ FLOPS to be solved. Which is much faster than Gaussian elimination and LU decomposition. \linebreak

If we use the tridiagonal matrix generated by \eqref{eq_2} we can simplify \eqref{eq_5}, \eqref{eq_6}, \eqref{eq_8} and \eqref{eq_9} accordingly

\begin{align}
\check b_i &= b_i + \frac{\check b_{i-1}}{\check a_{i-1,i-1}} = b_i + \frac{i-1}{i} \, \check b_{i-1} && \text{for $\check b_1 = b_1$ and $i\in\mathbb{N}_2^n$,} 
\label{eq_10}
\\
\check a_{i,i} &= 2 - \frac{1}{\check a_{i-1,i-1}} = \frac{i+1}{i} && \text{for $\check a_{1,1} = 2$ and $i\in\mathbb{N}_2^n$,}
\label{eq_11}
\\
\hat b_i &= \check b_i + \varv_{i+1} && \text{for $\hat b_n = \check b_n$ and $i\in \mathbb{N}_1^{n-1}$,}
\label{eq_12}
\\
\varv_i &= \frac{\hat{b}_i}{\check{a}_{i,i}} = \frac{i}{i+1}\hat b_i && \text{for $i \in \mathbb{N}_1^n$.}
\label{eq_13}
\end{align}

We can show \eqref{eq_11} by induction. Clearly \eqref{eq_11} satisfy the initial condition $a_{1,1} = \lim_{i\to 1} \frac{i+1}{i} = 2$. The induction step can be shown as 

\begin{align*}
\check a_{i+1,i+1} = 2 - \frac{1}{\check a_{i,i}} = 2 - \frac{i}{i+1} = \frac{2\p{i+1}-i}{i+1} = \frac{i+2}{i+1} \,.
\end{align*}

Since \eqref{eq_11} is actually a constant that doesn't change with the source terms $b_i$, it can be pre calculated and reused for solving the tridiagonal matrix. In that case \eqref{eq_10} only needs 2 FLOPS to be calculated, \eqref{eq_11} is pre calculated, \eqref{eq_12} and \eqref{eq_13} needs 1 FLOP each, and the total is $4n$ FLOPS to solve the linear equation in \eqref{eq_2}. If we don't want to use memory to save the $\frac{i+1}{i}$ coefficient, we need to add 2 FLOPS; 1 FLOP  for calculate $\frac{i-1}{i}$ when $i$ goes $1\to n$, and another FLOP to calculate $\frac{i}{i+1}$ when $i$ goes $n\to 1$. And this gives a $6n$ FLOPS to solve.

\section{Implementation}\label{sec_Implementation}

In this section different algorithms are presented for solving the tridiagonal matrix generated of \eqref{eq_2}. Only the actual solver will be presented here, and not the pre calculation needed. These algorithms are compared to each other in section Result \ref{sec_Result}. The algorithms can be found in \href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project%201/A/project1/Matrix.h}{Matrix.h}.

\subsection{$8n$}

This method solves a general tridiagonal matrix. It requires one matrix of size $n$ and two matrix's of size $n-1$ to represent the tridiagonal matrix, but no extra memory of significance to solve the tridiagonal matrix.

\begin{lstlisting}[frame=single, caption={\texttt{Matrix<MatrixType::TridiagonalGeneral, T>::Solve}}]  
inline static bool Solve(T* a, T* b, T* c, T* f, unsigned int n) {
	T temp;
	T* start = f;
	T* end = &f[n-1];
	while(f != end) {
		temp = (*a++)/(*b++);
		*b -= temp*(*c++);          // Eq (6)
		*f -= temp*(*f++);          // Eq (5)
	}
	while(f != start) {
		*f /= (*b--);               // Eq (9)
		*f -= (*--c)*(*f--);        // Eq (8)
	}
	*f /= *b;
	return true;
}
\end{lstlisting}

\subsection{$6n$}\label{sec_6n}

This method uses actually $8n$ FLOPS, but it's based on the $6n$ algorithm derived in section \ref{sec_Tridiagonal}. The reason for number of FLOPS is speed consideration, which you can see in \reftab{tab_speed}. The reason for this is if we use integers for $i$ in $\frac{i+1}{i}$, we don't count the increment of $i$ as a FLOP. However in this algorithm $i$ is a floating point and therefore we need to count the increment of $i$ as a FLOP. This method needs no memory to represent the tridiagonal matrix, and need no extra memory of significance to solve the tridiagonal matrix.

\begin{lstlisting}[frame=single, caption={\texttt{Matrix<MatrixType::Tridiagonal\_minus1\_2\_minus1\_6n, T>::Solve}}]  
static bool Solve(T* f, unsigned int n) {
	T* start = f;
	T* end = &f[n-1];
	T i = 1;
	while(f != end)
		*f += i++/i*(*f++);	// Eq (10)
	i++;
	while(f != start) {
		*f *= 1/(i--)*i;// Eq (13) (faster than *f /= i--/i)
		*f += *f--;	// Eq (12)
	}
	*f /= 2;
	return true;
}
\end{lstlisting} 

\subsection{$6n$ cutoff}

If we look at the factor $\frac{i+1}{i} \to 1$ when $i\to \infty$, and it's tempting to approximate $\frac{i+1}{i} =1$ for $i \geq \mathrm{cutoff}$. However this approximation shows to give fatal error as shown in \reftab{tab_error}. Otherwise this method is the same as $6n$ algorithm in \ref{sec_6n}.

\begin{lstlisting}[frame=single, caption={\texttt{Matrix<MatrixType::Tridiagonal\_minus1\_2\_minus1\_6n, T>::Solve}}]  
static bool Solve(T* f, unsigned int n, unsigned int cutoff) {
	T* start = f;
	T* mid = &f[n < cutoff ? n-1 : cutoff-1];
	T* end = &f[n-1];
	T i = 1;
	while(f != mid)
		*f += i++/i*(*f++);	// Eq (10)
	while(f != end)
		*f += *f++;		// Eq (10) with (i+1)/i = 1
	i++;
	while(f != mid)
		*f += *f--;		// Eq (12) with (i+1)/i = 1
	while(f != start) {
		*f *= 1/(i--)*i;	// Eq (13)
		*f += *f--;		// Eq (12)
	}
	*f /= 2;
	return true;
}
\end{lstlisting} 

\subsection{$6n$ Int}

This method uses \texttt{unsigned int} as intrinsic data type for $i$ in the coefficient $\frac{i+1}{i}$ instead of a floating point, as discussed in section \ref{sec_6n}. Otherwise this method is the same as $6n$ algorithm in \ref{sec_6n}, and we see in \reftab{tab_speed} how this small change in code double the computing time.

\begin{lstlisting}[frame=single, caption={\texttt{Matrix<MatrixType::Tridiagonal\_minus1\_2\_minus1\_6n, T>::SolveInt}}]  
static bool SolveInt(T* f, unsigned int n) {
	T* start = f;
	T* end = &f[n-1];
	unsigned int i = 1;
	while(f != end)
		*f += (T)(i++)/i*(*f++);  // Eq (10)
	i++;
	while(f != start) {
		*f *= (T)1/(i--)*i;         // Eq (13) 
		*f += *f--;                 // Eq (12)
	}
	*f /= 2;
	return true;
}
\end{lstlisting}

\subsection{$6n$ True}

This method doesn't use induction proof in section \ref{sec_Tridiagonal} to get the factor $\frac{i+1}{i}$, but uses the difference equation in \eqref{eq_10}, \eqref{eq_11}, \eqref{eq_12} and \eqref{eq_13} directly, which gives $6n$ FLOP algorithm. This method does not need memory to represent the tridiagonal matrix, however it needs an array of size $n$ to store the values from \eqref{eq_11}. And we see in \reftab{tab_speed} how this extra memory accessing doubles the computing time. Another disadvantage is that this method is not as numerical stable for larger $n$. 

\begin{lstlisting}[frame=single, caption={\texttt{Matrix<MatrixType::Tridiagonal\_minus1\_2\_minus1\_6n, T>::SolveTrue}}]  
public: bool SolveTrue(T* f, unsigned int n) {
	if(_n < n)
		this->n = n;
	b[0] = 2;
	T temp;
	T* start = f;
	T* end = &f[n-1];
	while(f != end) {
		temp = (T)1/(*b++);
		*b = 2 - temp;              // Eq (11)
		*f += temp*(*f++);          // Eq (10)
	}
	while(f != start) {
		*f /= (*b--);               // Eq (13)
		*f += (*f--);               // Eq (12)
	}
	*f /= *b;
	return true;
}
\end{lstlisting}

\subsubsection{$4n$ and $5n$}

$5n$ is the same method as $4n$, only that $5n$ calculates the coefficient $\frac{i+1}{i}$ when solving the tridiagonal matrix, whereas $4n$ method pre calculates this values. These coefficients $\frac{i+1}{i}$ are stored in an array of size $n$, other wise this method is equal to $6n$ method in section \ref{sec_6n}, no memory need to represent the tridiagonal matrix. Even though $4n$ method has half the number of FLOPS needed to calculate as $6n$ (which is actually $8n$ FLOPS), we see in \reftab{tab_speed} that the speed is more or less the same. The reason is that $6n$ method has less memory access than the $4n$, and can do more FLOPS in CPU instead of waiting for memory.

\begin{lstlisting}[frame=single, caption={\texttt{Matrix<MatrixType::Tridiagonal\_minus1\_2\_minus1\_4n, T>::Solve}}]  
bool Solve(T* f, unsigned int n) {
	if(_n < n)
		this->n = n;
	T* start = f;
	T* end = &f[n-1];
	T* factor = this->factor;
	while(f != end)
		*f += (*f++)*(*factor++);   // Eq (10)
	while(f != start) {
		*f *= *factor--;            // Eq (13)
		*f += *f--;                 // Eq (12)
	}
	*f /= 2;
	return true;
}
\end{lstlisting}

\subsection{Armadillo LU}

This method uses the LU decomposition method discussed in section \ref{sec_LU}. \href{http://arma.sourceforge.net/}{Armadillo} is used to find the lower and upper triangle matrix's $L$ and $U$ by the function \texttt{lu(L, U, matrix)} prior to finding the solution, and are therefore not included in the timing in \reftab{tab_speed}. As we see from this table with $n = 1000$ the computation speed is approximately $1000$ slower than the other methods, which is in accordance with that the forward and backward substitution uses $\mathcal{O}\p{n^2}$ FLOPS. This method is very memory demanding, it needs three $n\times n$ arrays, on to represent the tridiagonal matrix and one each for both lower and upper triangle matrix. And for $n=100000$ we need 80 GB of memory to represent a matrix with intrinsic data type \texttt{double}, this is impossible since the matrix indexing is represented by the intrinsic data type \texttt{int}, which is 32 bit and can only represent a array size of $2^{31} \approx 2.1$ GB.

\begin{lstlisting}[frame=single, caption={\texttt{Matrix<MatrixType::LU\_decomposition, T>::Solve}}]  
bool Solve(T* f, unsigned int n) {
	if(n == _n) {
		T* f_;
		int i = 0, j;
		while(++i < n) {	// Forward solve Ly = f
			f_ = &f[i];
			for(j = 0; j < i; j++)
				*f_ -= L(i,j)*f[j];
		}
		n--;
		while(i--) {		// Backward solve Ux = y
			f_ = &f[i];
			for(j = n; j > i; j--)
				*f_ -= U(i,j)*f[j];
			*f_ /= U(i,i);
		}
		return true;
 	}
 	return false;
}
\end{lstlisting}

\section{Result} \label{sec_Result}

The algorithm described in section \ref{sec_Implementation} is tested with the source term

\begin{align}
f\p{x} = 100 \e^{-10 x}
\label{eq_14}
\end{align}

which has the analytic solution

\begin{align}
u\p{x} = 1 - \p{1-\e^{-10}}x - \e^{-10 x}\,,
\label{eq_15}
\end{align}

which one can easily see by taking the derivative of this expression two times, and we get \eq{eq_14} and it also satisfy the boundary conditions $u\p{0}=u\p{1}=0$. And all the algorithms are tested against this analytical solution in the following results. The relative error of the algorithms to the analytic solution \eqref{eq_15} is calculated by

\begin{align}
\epsilon_i = log_{10}\abs{\frac{\varv_i - u_i}{u_i}} \,.
\end{align}

\begin{tabell}{|r|c|c|c|c|c|c|c|c|}{}{Number & 8n & 6n & 6n cutoff & 6n int & 6n true & 5n & 4n & Armadillo\\ of steps & & & (10000) & & & & & LU\\}{\input{build-project1-Desktop-Debug/time.dat}}{Time used in seconds to solve the tridiagonal matrix with different solvers .}{tab_speed}
\end{tabell} 

\begin{tabell}{|r|c|c|c|c|c|c|c|c|}{}{Number & 8n & 6n & 6n cutoff & 6n int & 6n true & 5n & 4n & Armadillo\\ of steps & & & (10000) & & & & & LU\\}{\input{build-project1-Desktop-Debug/error.dat}}{Order of maximum relative error for the different solvers.}{tab_error}
\end{tabell}

\figur{0.8}{n_10.eps}{Result from different tridiagonal matrix solvers for $n=10$ steps compared to the closed form solution.}{}  

\figur{0.8}{n_100.eps}{Result from different tridiagonal matrix solvers for $n=100$ steps compared to the closed form solution.}{} 

\figur{0.8}{n_1000.eps}{Result from different tridiagonal matrix solvers for $n=1000$ steps compared to the closed form solution.}{}

\section{Conclusion}

This reports shows that there are many aspects to consider when optimising an algorithm for a specific problem. A generic algorithms for solving problems are often slow because lot of FLOPS are need to be calculated to solve a problem to be able to handle every possible case. However if a problem have simplified properties to the general case, very often a fast algorithm can be devised to solve the problem, because fewer considerations are need and hence less computation time is needed. An indeed a simplified algorithm may be able to handle larger datasets than the general algorithm, due to difference in memory usage. \linebreak

Number of FLOPS is often a good indication of computation time needed when we look at the order of FLOPS needed from $n$ data points. However when we have few FLOPS difference between algorithm for each data point, then it's not as obvious which algorithm is the fastest. Memory accessing, overhead code and data type conversion are additional considerations when comparing algorithms speed with few FLOPS difference. Avoiding memory accessing may be beneficial because the RAM runs on a lower speed than the CPU. \reftab{tab_speed} shows very well how this different aspects come into affect. \linebreak

But speed is not always the only consideration to take when evaluating algorithms, another important consideration is the error that the algorithms produce. Where certain algorithms are more numerical stable than others. \reftab{tab_error} shows these aspects. \linebreak

By looking at \reftab{tab_speed} and \reftab{tab_error} we find two algorithms that preforms better than the other algorithms when it comes to solving \eqref{eq_1}, namely $6n$ and $4n$. Both are approximately equal in speed and precision. From these results I will consider $6n$ superior to $4n$, because the $6n$ algorithm does not need extra memory to solve the problem, whereas the $4n$ requires an array of size $n$ extra. However computers with different architectures may favour these two algorithms differently, because the $6n$ algorithms uses more CPU power, whereas the $4n$ algorithms uses more memory power.

\section{Attachments}

The files produced in working with this project can be found at \href{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%201/A}{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%201/A} \linebreak

The source files developed are

\begin{enumerate}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%201/A/project1/Matrix.h}{Matrix.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%201/A/project1/project1.cpp}{project1.cpp}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%201/A/plot.py}{plot.py}}
\end{enumerate}

\section{Resources}

\begin{enumerate}
\item{\href{http://qt-project.org/downloads}{QT Creator 5.3.1 with C11}}
\item{\href{http://arma.sourceforge.net/}{Armadillo}}
\item{\href{https://www.eclipse.org/downloads/}{Eclipse Standard/SDK  - Version: Luna Release (4.4.0) with PyDev for Python}}
\item{\href{http://www.ubuntu.com/download/desktop}{Ubuntu 14.04.1 LTS}}
\item{\href{http://shop.lenovo.com/no/en/laptops/thinkpad/w-series/w540/#tab-reseller}{ThinkPad W540 P/N: 20BG0042MN with 32 GB RAM}}
\end{enumerate}

\begin{thebibliography}{9}
\bibitem{project1}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/project1_2014.pdf}{\emph{FYS4130 - Project 1 - Description}}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{lecture}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/Lecture\%20Notes/lecture2014.pdf}{\emph{Computational Physics - Lecture Notes Fall 2014}}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{gaussian}{\href{http://en.wikipedia.org/wiki/Gaussian\_elimination}{\texttt{http://en.wikipedia.org/wiki/Gaussian\_elimination}}} 
\bibitem{LU}{\href{http://en.wikipedia.org/wiki/LU\_decomposition}{\texttt{http://en.wikipedia.org/wiki/LU\_decomposition}}} 
\bibitem{gaussian}{\href{http://en.wikipedia.org/wiki/Triangular\_matrix\#Forward\_and\_back\_substitution}{\texttt{http://en.wikipedia.org/wiki/Triangular\_matrix\#Forward\_and\_back\_substitution}}} 
\end{thebibliography}
\end{flushleft}
\end{document}