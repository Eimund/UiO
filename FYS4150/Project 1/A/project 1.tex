\documentclass[11pt,english,a4paper]{article}
\include{mitt_oppsett}
\usepackage{fullpage}

\renewcommand\title{FYS4150 - Computational Physics - Project 1}

\renewcommand\author{Eimund Smestad}
%\newcommand\adress{}
\renewcommand\date{\today}
\newcommand\email{\href{mailto:eimundsm@fys.uio.no}{eimundsm@fys.uio.no}}

\begin{document}
\maketitle
\begin{flushleft}

\begin{abstract}

\end{abstract}

We have the Poisson's equation from electromagnetism stated as

\begin{align*}
\nabla^2 \Phi = - 4 \pi \rho\p{\vec{r}}
\end{align*}

in three dimensional space with the electrostatic potential $\Phi$ generated by local charge distribution $\rho\p{\vec{r}}$. With a spherically symmetric $\Phi$ and $\rho\p{\vec{r}}$ the equations simplifies to a one-dimensional equation in $r$, namely

\begin{align*}
\frac{1}{r^2}\frac{\mathrm{d}}{\mathrm{d}r}\p{r^2\frac{\mathrm{d}\Phi}{\mathrm{d}r}} = -4 \pi \rho\p{r} \,,
\end{align*}

which can be rewritten to via a substitution $\Phi\p{r} = \frac{\phi}{r}$ as 

\begin{align*}
\frac{\mathrm{d}^2\phi}{\mathrm{d}r^2} = - 4\pi \rho\p{r} \,,
\end{align*}

which again can be rewritten 

\begin{align*}
-u^{\prime\prime}\p{x} = f\p{x} \,.
\end{align*}

In this project I will solve the following case of the one-dimensional Poisson's equation with Dirichlet boundary condition

\begin{align}
-u^{\prime\prime}\p{x} = f\p{x}\,,  \qquad x\in\p{0,1}\,,\quad u\p{0} = u\p{1} = 0 \,.
\label{eq_1}
\end{align}

This equation is approximated by discretizing $u$ as $\varv_i$ with grid points $x_i = i h$ with the interval $x_0 = 0$ and $x_{n+1}=1$;

\begin{align}
-\varv_{i+1} + 2 \varv_i - \varv_{i-1} = b_i \qquad \text{for $\varv_0 = 0$, $\varv_{n+1} = 0$ and $i\in\mathbb{N}_1^n$,}
\label{eq_2}
\end{align}

where $b_i = h^2 f\p{x_i}$ and the step length is given by $h = \frac{1}{n+1}$. This difference equation can be rewritten to matrix form

\begin{align*}
\vec{\vec{A}}\,\vec{\varv} = \vec{b} 
\end{align*}

which we can show to satisfy the difference equation \eqref{eq_2}

\begin{align*}
\begin{bmatrix} a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\ a_{2,1} & a_{2,2} & \cdots & a_{2,n}  \\ \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2} & \cdots & a_{n,n} \end{bmatrix} \begin{bmatrix} \varv_1 \\ \varv_2 \\ \vdots \\  \varv_n \end{bmatrix} 
= \begin{bmatrix} \sum_{j=1}^n a_{1,j} \varv_j \\ \sum_{j=1}^n a_{2,j} \varv_j \\ \vdots \\ \sum_{j=1}^n a_{n,j} \varv_j \end{bmatrix} 
= \begin{bmatrix} a_{1,1} \varv_1 + a_{1,2} \varv_2 \\ a_{2,1} \varv_1 + a_{2,2} \varv_2 + a_{2,3} \varv_3 \\ \vdots \\ a_{n,n-1} \varv_{n-1} + a_{n,n} \varv_n \end{bmatrix} = \begin{bmatrix} 2 \varv_1 - \varv_2 \\ -\varv_1 + 2 \varv_2 - \varv_3 \\ \vdots \\ -\varv_{n-1} + 2 \varv_n \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} \,,  
\end{align*}

where the matrix elements are given by $a_{i,i} = 2$, $a_{i,i\pm 1} = -1$ and $a_{i,i\pm j} = 0$ for $j \geq 2$, and this is called a tridiagonal matrix. Please note the boundary condition in \eqref{eq_1} gives $-\varv_0 + 2 \varv_1 - \varv_2 = 2 \varv_1 - \varv_2$ and $-\varv_{n-1}+ 2\varv_n - \varv_{n+1} = -\varv_{n-1} + 2 \varv_n$, which actually enables us to write the difference equation in \eqref{eq_2} on matrix form without getting in trouble in the corners of the matrix. We can now apply the Gaussian elimination to first reduce the tridiagonal matrix to the upper triangle matrix;

\begin{align*}
&\kl{\left.\begin{matrix} a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ a_{2,1} & a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & a_{3,2} & a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & a_{n-1,n-2} & a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & a_{n,n-1} & a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_{n-2} \\ b_{n-1} \\ b_n\end{matrix}} 
\\
&\sim \kl{\left.\begin{matrix} a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 & a_{2,2} - \frac{a_{1,2}a_{2,1}}{a_{1,1}} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & a_{3,2} & a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & a_{n-1,n-2} & a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & a_{n,n-1} & a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} b_1 \\ b_2 - \frac{a_{1,2} b_1}{a_{1,1}} \\ b_3 \\ \vdots \\ b_{n-2} \\ b_{n-1} \\ b_n\end{matrix}}
\\
&=  \kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & a_{3,2} & a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & a_{n-1,n-2} & a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & a_{n,n-1} & a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ b_3 \\ \vdots \\ b_{n-2} \\ b_{n-1} \\ b_n\end{matrix}}
\\
&\sim  \kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ \check b_3 \\ \vdots \\ \check b_{n-2} \\ \check b_{n-1} \\ \check b_n\end{matrix}} \,.
\end{align*}

So the elements in the upper triangle matrix of reduced tridiagonal are given by the following two difference equations

\begin{align}
\check b_i &= b_i - \frac{a_{i,i-1}}{\check a_{i-1,i-1}}\check b_{i-1} && \text{for $\check b_{1} = b_{1}$ and $i\in\mathbb{N}_2^n$,} 
\label{eq_3} 
\\\check a_{i,i} &= a_{i,i} - \frac{a_{i,i-1}}{\check a_{i-1,i-1}}a_{i,i-1} && \text{for $\check a_{1,1} = a_{1,1}$ and $i\in\mathbb{N}_2^n$,}  
\label{eq_4}
\\
a_{i,i + j} &= 0 && \text{for $j \in \mathbb{Z}_{-n}^n / \mathbb{Z}_{-1}^0$.}
\label{eq_5}
\end{align}

If we now continue with Gaussian elimination to reduce tridiagonal further to a diagonal matrix

\begin{align*}
&\kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & a_{n-1,n} \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ \check b_3 \\ \vdots \\ \check b_{n-2} \\ \check b_{n-1} \\ \check b_n\end{matrix}} 
\\ &\sim \kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & 0 \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ \check b_3 \\ \vdots \\ \check b_{n-2} \\ \check b_{n-1} - \frac{a_{n-1,n}}{\check a_{n,n}} \check b_n\\ \check b_n\end{matrix}} 
\\ &= \kl{\left.\begin{matrix} \check a_{1,1} & a_{1,2} & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & a_{2,3} & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & a_{n-2,n-1} & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & 0 \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \check b_1 \\ \check b_2\\ \check b_3 \\ \vdots \\ \check b_{n-2} \\ \hat b_{n-1} \\ \hat b_n\end{matrix}} 
\\ &\sim \kl{\left.\begin{matrix} \check a_{1,1} & 0 & 0 & \cdots & 0 & 0 & 0 \\ 0 &  \check a_{2,2} & 0 & \cdots & 0 & 0 & 0\\ 0 & 0 & \check a_{3,3} & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & \check a_{n-2,n-2} & 0 & 0 \\ 0 & 0 & 0 & \cdots & 0 & \check a_{n-1,n-1} & 0 \\ 0 & 0 & 0 &\cdots & 0 & 0 & \check a_{n,n} \end{matrix}\,\,\right\vert\,\,\begin{matrix} \hat b_1 \\ \hat b_2\\ \hat b_3 \\ \vdots \\ \hat b_{n-2} \\ \hat b_{n-1} \\ \hat b_n\end{matrix}} 
\end{align*}

which gives the difference equation

\begin{align}
\hat b_i = \check b_i - \frac{a_{i,i+1}}{\check a_{i+1,i+1}} \hat b_{i+1} = \check b_i - a_{i,i+1} \varv_{i+1} \qquad \text{for $\hat b_n = \check b_n$ and $i\in\mathbb{N}_1^{n-1}$.}
\label{eq_6}
\end{align}

where $\varv_i$ is the solution of the linear equation set represented by a tridiagonal matrix

\begin{align}
\varv_i = \frac{\hat b_i}{\check a_{i,i}} \qquad \text{for $i\in\mathbb{N}_1^n$.}
\label{eq_7}
\end{align}

So the solution for a linear equation set represented by a tridiagonal matrix in \eqref{eq_7} needs iteratively calculate first \eqref{eq_4} and then \eqref{eq_3} when $i$ goes $1\to n$, and then iteratively calculate \eqref{eq_6} when $i$ goes $n\to 1$. The difference equation \eqref{eq_4} requires 3 FLOPS to be calculated for each iteration, but \eqref{eq_3} requires only 2 FLOPS to be calculated for each iteration, because $\frac{a_{i-1,i}}{\check a_{i-1,i-1}}$ is used in both \eqref{eq_3} and \eqref{eq_4} and needs only to be calculated once. Then \eqref{eq_7} is calculated and requires a 1 FLOP, which is followed directly by \eqref{eq_6} which uses \eqref{eq_7} and therefore requires only 2 more FLOPS to be calculated. In total a general linear equation set of $n$ equations that are represented by a tridiagonal matrix requires approximately $8n$ FLOPS to be solved. This is much faster than Gaussian elimination and LU decomposition which requires approximately $\frac{2}{3}n^3$ FLOPS. \linebreak

If we use the tridiagonal matrix generated by \eqref{eq_2} we can simplify \eqref{eq_3}, \eqref{eq_4}, \eqref{eq_6} and \eqref{eq_7} accordingly

\begin{align}
\check b_i &= b_i + \frac{\check b_{i-1}}{\check a_{i-1,i-1}} = b_i + \frac{i-1}{i} \, \check b_{i-1} && \text{for $\check b_1 = b_1$ and $i\in\mathbb{N}_2^n$,} 
\label{eq_8}
\\
\check a_{i,i} &= 2 - \frac{1}{\check a_{i-1,i-1}} = \frac{i+1}{i} && \text{for $\check a_{1,1} = 2$ and $i\in\mathbb{N}_2^n$,}
\label{eq_9}
\\
\hat b_i &= \check b_i + \varv_{i+1} && \text{for $\hat b_n = \check b_n$ and $i\in \mathbb{N}_1^{n-1}$,}
\label{eq_10}
\\
\varv_i &= \frac{i}{i+1}\hat b_i && \text{for $i \in \mathbb{N}_1^n$.}
\label{eq_11}
\end{align}

We can show \eqref{eq_9} by induction. Clearly \eqref{eq_9} satisfy the initial condition $a_{1,1} = \lim_{i\to 1} \frac{i+1}{i} = 2$. The induction step can be shown as 

\begin{align*}
\check a_{i+1,i+1} = 2 - \frac{1}{\check a_{i,i}} = 2 - \frac{i}{i+1} = \frac{2\p{i+1}-i}{i+1} = \frac{i+2}{i+1} \,.
\end{align*}

Since \eqref{eq_9} is actually a constant that doesn't change with the source terms $b_i$, it can be pre calculated and reused for solving the tridiagonal matrix. That means that \eqref{eq_9} might not contribute to number of FLOPS that needs to be calculated, and furthermore $\lim_{i\to \infty} \frac{i+1}{i} = 1$, so when $i$ is large we don't need to calculate $\check a_{i,i}$ because it is so close to 1 that we don't do any numerical error by setting $\check a_{i,i}$ to 1 for large $i$. So the calculation start by  \eqref{eq_8} which requires 2 FLOPS, and then \eqref{eq_11} requires 1 FLOP and \eqref{eq_10} also requires 1 FLOP. Hence the total number of flops to needed to solve \eqref{eq_2} is $4n$.

\begin{tabell}{|r|c|c|c|c|}{}{Number of steps & 8n & 6n & 5n & 4n \\}{\input{build-project1-Desktop-Debug/time.dat}}{Time used in seconds to solve problem for each of the tridiagonal matrix solvers}{}
\end{tabell}

\end{flushleft}
\end{document}