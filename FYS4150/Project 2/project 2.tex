\documentclass[11pt,english,a4paper]{article}
\include{mitt_oppsett}
\usepackage{fullpage}

\renewcommand\title{FYS4150 - Computational Physics - Project 2}

\renewcommand\author{Eimund Smestad}
%\newcommand\adress{}
\renewcommand\date{\today}
\newcommand\email{\href{mailto:eimundsm@fys.uio.no}{eimundsm@fys.uio.no}}

%\lstset{language=[Visual]C++,caption={Descriptive Caption Text},label=DescriptiveLabel}
\lstset{language=c++}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{black}\bfseries}
\lstset{commentstyle=\itshape\color{black}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}

\begin{document}
\maketitle
\begin{flushleft}

\begin{abstract}
This report looks at different algorithms to solve eigenvalue problems for matricies and apply them on Shr\"{o}dinger's equation for both a single electron and two electrons in a harmonic oscillator well with Coulumb repulsion between the electrons. I have investigated the Jacobi eigenvalue algorithm  and Francis QR algorithm in more detail. I have done different optimization to be able to solve the eigenvalue problem faster, and compare the different algorithms. The results were a Jacobi algorithm that is 10 times faster than the standard implementation for solving the eigenvalue problems for the Shr\"{o}dinger's equations in this report. Similarly I have made a QR algorithm for finding eigenvalues of symmetric tridiagonal matrix that solves the problem in this report significantly faster \texttt{tqli} in \texttt{lib.cpp} of the course site, without loss of precision in the cases investigated in this report.
\end{abstract}

\section{The radial Schr\"{o}dinger's equation and numerical implementation}

In this report I will study the radial part of Schr\"{o}dinger's equation for both one and two electrons, which is as follows for a single electron

\begin{align*}
-\frac{\hbar^2}{2m}\p{\frac{1}{r^2} \frac{\mathrm{d}}{\mathrm{d}r}r^2\frac{\mathrm{d}}{\mathrm{d}r}-\frac{\ell\p{\ell+1}}{r^2}}\mathrm{R}\p{r}+\mathrm{V}\p{r}\mathrm{R}\p{r} = E \mathrm{R}\p{r} \,,
\end{align*}

where $\mathrm{R}\p{r}$ is the radial wave function, $r\in\left[0,\infty\right)$ is the radial parameter, $\ell$ is the quantum number for the orbital momentum, $m$ is the mass, $\hbar$ is the Planck's constant, $\mathrm{V}\p{r}$ is the potential and $E$ is the energy of the system. In this report I will only investigate the case $\ell = 0$, and we can then rewrite the Schr\"{o}dinger's equation to this simplified form

\begin{align}
-\frac{\mathrm{d}^2}{\mathrm{d}\rho^2}\mathrm{u}\p{\rho} + \mathrm{V}\p{\rho}\mathrm{u}\p{\rho} = \lambda\mathrm{u}\p{\rho} \,,
\label{eq_1}
\end{align}

where $\mathrm{u}\p{r} = r \mathrm{R}\p{r}$, $r = \alpha \rho$, $\alpha^2 = - \frac{\hbar^2}{2m}$ and $\lambda = E$. This differential equation can be written on finite difference form

\begin{align*}
-\frac{u_{i+1}-2 u_i+u_{i-1}}{h^2} + V_i u_i = \lambda u_i \,,
\end{align*}

where subscript $i$ is a indication of dependency to $\rho_i$, which is given by

\begin{align*}
\rho_i = \rho_{\mathrm{min}} + i h \,,
\end{align*}

and $h$ is the step length

\begin{align*}
h = \frac{\rho_{\mathrm{max}}-\rho_{\text{min}}}{n_{\mathrm{step}}} 
\end{align*}

where $n_{\mathrm{step}}$ is the number of steps and $\rho\in\left[\rho_{\mathrm{min}},\rho_{\mathrm{max}}\right]$. This difference equation can again be rewritten to a simplified form

\begin{align}
e_{i+1} u_{i+1} + d_i u_i + e_{i-1}  u_{i-1} = \lambda u_i \,,
\label{eq_2}
\end{align}

where $e_i = - \frac{1}{h^2}$ and $d_i = \frac{2}{h^2} + V_i$. Introducing the Dirichlet boundary condition, $u_0 = u_{n_{\mathrm{step}}} = 0$, this difference equation becomes a matrix eigenvalue problem with dimension $n_{\mathrm{step}}-1$, where we have a tridiagonal matrix.

\subsection{Single electron in a harmonic oscillator well}

The harmonic oscillator potential is given by

\begin{align*}
\mathrm{V}\p{r} = \frac{1}{2}kr^2 \qquad \text{with } k = m\omega^2 \,,
\end{align*}

where $\omega$ is the oscillator frequency. The possible energies of a harmonic oscillator system are

\begin{align}
E_{n\ell} = \hbar\omega\p{2n+\ell+\frac{3}{2}} \qquad \text{with } n,\ell\in\mathbb{N}_0 \,,
\label{eq_3}
\end{align}

where $\mathbb{N}_0 = \mathbb{N}_0^{\infty}$ is all the integers from zero to infinity.  We can now write the Schr\"{o}dinger's equation in \eqref{eq_1} as 

\begin{align}
-\frac{\mathrm{d}^2}{\mathrm{d}\rho^2} \mathrm{u}\p{\rho} + \rho^2 \mathrm{u}\p{\rho} = \lambda\mathrm{u}\p{\rho} \,,
\label{eq_4}
\end{align}

where we $\alpha = \p{\frac{\hbar^2}{m k}}^{\frac{1}{4}}$ and $\lambda = \frac{2m\alpha^2}{\hbar^2} E$ instead.

\subsection{Two electrons in a harmonic oscillator well}

The potential for Coulomb repulsion is given by 

\begin{align*}
\mathrm{V}\p{r} = \frac{\beta e^2}{r} \,,
\end{align*}

where $r$ is the distance between the two electrons with $\beta e^2 = 1.44\unit{eVnm}$. From M. Taut's article \cite{TwoElectron} we have the following Shr\"{o}dinger's equation relative between two electrons with Coulomb repulsion in a harmonic oscillator well where $\ell = 0$;

\begin{align}
-\frac{\mathrm{d}^2}{\mathrm{d}\rho^2} \mathrm{u}\p{\rho} + {\omega_r}^2\mathrm{u}\p{\rho} + \frac{1}{\rho} = \lambda \mathrm{u}\p{\rho} 
\label{eq_5}
\end{align}

written on the form \eqref{eq_1}, where ${\omega_r}^2 = \frac{1}{4}\frac{mk}{\hbar^2} \alpha^4$, $\alpha = \frac{\hbar^2}{m\beta e^2}$, $\lambda = \frac{m\alpha^2}{\hbar^2} E_r$ and $E_r$ is the relative energy between the two electrons. The solution have the eigenvalues

\begin{align}
\lambda_n = 3\p{\frac{\omega_r}{2}}^{\frac{2}{3}} + 2\sqrt{3}\omega_r\p{n+\frac{1}{2}} \qquad \text{with } n \in \mathbb{N}_0 \,,
\label{eq_6}
\end{align}

with the ground state ($n=0$) wave function

\begin{align}
u_0\p{\rho} = \p{\frac{\sqrt{3}\omega_r}{\pi}}^{\frac{1}{4}}\exp\p{-\frac{\sqrt{3}}{2}\omega_r\p{\rho-\p{2{\omega_r}^2}^{-\frac{1}{3}}}^2} \,.
\label{eq_7}
\end{align}

\section{Eigenvalue algorithms}

\subsection{Diagonalize by matrix similarity}\label{sec_Diagonalize}

If we have a eigenvalue problem for a matrix $\textbf{A}$ with eigenvector $\mathbf{x}$ and eigenvalue $\lambda$

\begin{align*}
\mathbf{A}\mathbf{x} = \lambda \mathbf{x} \,,
\end{align*}

we can transform it by a basis matrix $\mathbf{S}$ to a similar matrix $\mathbf{B} = \mathbf{S}^{-1}\mathbf{A}\mathbf{S}$ with the same eigenvalue $\lambda$ but with a different eigenvector $\mathbf{S}^{-1} \mathbf{x}$;

\begin{align*}
\lambda \mathbf{S}^{-1} \mathbf{x} = \mathbf{S}^{-1}\mathbf{A}\mathbf{x} = \mathbf{S}^{-1}\mathbf{A}\mathbf{S} \mathbf{S}^{-1}\mathbf{x} = \mathbf{B}\mathbf{S}^{-1}\mathbf{x} \,.
\end{align*}

If we are able to diagonalize out matrix $\mathbf{A}$ the eigenvalue problem becomes trivial to solve. However to directly diagonalize matrix $\mathbf{A}$ of size $n\times n$ requires to solve a $n$-th polynomial problem, which is a very hard task. So we choose a easier way by iteratively zero out more and more non-diagonal elements of matrix $\mathbf{A}$, which will eventually give us the diagonal matrix

\begin{align*}
\mathbf{D} = \prod_{i=1}^n \mathbf{S}_{n-i}^{-1} \mathbf{A} \prod_{i=1}^n \mathbf{S}_i\,.
\end{align*}

To make the problem simple to solve we define a change of basis matrix $\textbf{S}_{i,j}$ with the elements

\begin{align}
s_{ijk\ell} = \begin{cases} s_{ij} & \text{when $k=i$ and $\ell=j$} \\ 1 & \text{when } k = \ell \in \mathbb{N}_1^n / \kr{i,j} \\ 0 & \text{elsewhere,} \end{cases}  
\label{eq_8}
\end{align}

where the strategy is for this change of basis matrix $\mathbf{S}$ will make $b_{ij} = b_{ji} = 0$ when we do a matrix similarity transformation of $\mathbf{A}$ to $\mathbf{B}$. The matrix elements of $\mathbf{S}_{i,j}^{-1}$ is then given by

\begin{align*}
s_{ijk\ell}^- = \begin{cases} \frac{s_{jj}}{s_{ii}s_{jj}-s_{ij}s_{ji}} & \text{for $k=\ell=i$} \\ -\frac{s_{ij}}{s_{ii}s_{jj}-s_{ij}s_{ji}} & \text{for $k=i$ and $\ell=j$} \\ -\frac{s_{ji}}{s_{ii}s_{jj}-s_{ij}s_{ji}} & \text{for $k=j$ and $\ell=i$} \\ \frac{s_{ii}}{s_{ii}s_{jj}-s_{ij}s_{ji}} & \text{for $k=\ell=j$} \\ 1 & \text{for $k=\ell\in\mathbb{N}_1^n/\kr{i,j}$} \\ 0 & \text{elsewhere.} \end{cases}
\end{align*}

To see how this matrix similarity transformation behaves we must do the matrix multiplication $\mathbf{B} = \mathbf{S}_{i,j}^{-1} \mathbf{A}\mathbf{S}_{i,j}$ of the transformation;

\begin{align}
b_{k\ell} &= \sum_{m,o=1}^n s_{ijkm}^- a_{mo} s_{ijo\ell} = \begin{cases} s_{ii}^- a_{ii} s_{ii} + s_{ij}^- a_{ji} s_{ii} + s_{ii}^- a_{ij} s_{ji} + s_{ij}^- a_{jj} s_{ji} & \text{for $k=\ell=i$} \\ s_{ii}^- a_{ii} s_{ij} + s_{ij}^- a_{ji} s_{ij} + s_{ii}^- a_{ij} s_{jj} + s_{ij}^- a_{jj} s_{jj} & \text{for $k=i$ and $\ell = j$} \\ s_{ji}^- a_{ii} s_{ii} + s_{jj}^- a_{ji} s_{ii} + s_{ji}^- a_{ij} s_{ji} + s_{jj}^- a_{jj} s_{ji} & \text{for $k=j$ and $\ell=i$} \\ s_{ji}^- a_{ii} s_{ij} + s_{jj}^- a_{ji} s_{ij} + s_{ji}^- a_{ij} s_{jj} + s_{jj}^- a_{jj} s_{jj} & \text{for $k=\ell=j$} \\ s_{ki}^- a_{i \ell} + s_{kj}^- a_{j\ell} & \text{for $k\in\kr{i,j}$ and $\ell\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{ki} s_{i\ell} + a_{kj}s_{j\ell} & \text{for $\ell\in\kr{i,j}$ and $k\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{k\ell} & \text{elsewhere.}\end{cases}
\label{eq_9}
\end{align}

What we want our matrix similarity transformation to do is to set $b_{ij} = b_{ji} = 0$, but we also need to ensure that the basis matrix $\mathbf{S}$ for the transformation is invertible. Hence we have the following three equation we want to solve 

\begin{align}
s_{ii} s_{jj} - s_{ij} s_{ji} &= 1 
\label{eq_10}\\
a_{ji} {s_{ij}}^2-\p{a_{ii}-a_{jj}}s_{jj}s_{ij} - a_{ij}{s_{jj}}^2 &= 0 \label{eq_11}\\
a_{ij} {s_{ji}}^2 - \p{a_{jj}-a_{ii}}s_{ii}s_{ji} - a_{ji}{s_{ii}}^2 &= 0 \,. \label{eq_12}
\end{align} 

If the matrix $\mathbf{A}$ is indeed diagonalizable then the above three equations above solvable. We further observer that we have four unknowns in the three equations, which means that we can choose one of them as we want, I suggest $s_{ii}=1$. I will not show the solution here, but it involves in principle in two second order equations $a x^2 + b x + c = 0$ which has a well known analytical solution $x = \frac{-b\pm\sqrt{b^2 - 4ac}}{2a}$. Be aware of that the coefficient $a$ may be zero which would make it a first order equations to solve instead. If the coefficient $b=0$ in addition to $a=0$, then the matrix $\mathbf{A}$ is not diagonalizable. \linebreak

\subsection{Jacobi's eigenvalue algorithm} \label{sec_Jacobi}

The Jacobi's eigenvalue algorithm is a special case of diagonalize by matrix similarity algorithm discussed in section \ref{sec_Diagonalize}, where I use a two dimensional rotation matrix $\textbf{R}_{ij}$ in $n$ dimensional space which has the following elements

\begin{align}
r_{ijk\ell} = \begin{cases} \cos\theta_{ij} & \text{when $k=i$ and $\ell=i$} \\ \sin\theta_{ij} & \text{when $k=i$ and $\ell=j$} \\ -\sin\theta_{ij} & \text{when $k=j$ and $\ell=i$} \\ \cos\theta_{ij} & \text{when $k=j$ and $\ell=j$}\\ 1 & \text{when } k=\ell\in\mathbb{N}_1^n / \mathbb{N}_j^{j+1}  \\ 0 & \text{otherwise.}\end{cases}
\label{eq_13}
\end{align} 

which satisfies the basis matrix $\mathbf{S}$ in \eqref{eq_8}. The rotation matrix can easily be shown to be a orthogonal matrix by realizing ${\mathbf{R}_{ij}}^{-1} = {\mathbf{R}_{ij}}^{\mathrm{T}}$, because when we rotate an angle $\theta$, we need to rotate an angle $-\theta$ to get back (or you could realize this by a more cumbersome way by actually do the matrix inversion). \linebreak

By doing the matrix similarity transformation $\mathbf{B} = \mathbf{R}_{ij}^{\text{T}}\mathbf{A}\mathbf{R}_{ij}$, we get the following similar matrix elements according to \eqref{eq_9},

\begin{align*}
b_{k\ell} = \begin{cases} a_{ii} \cos^2\theta_{ij}  + a_{jj}\sin^2 \theta_{ij} - \p{a_{ij} + a_{ji}}\sin\theta_{ij}\cos\theta_{ij} & \text{for $k=\ell=i$} \\ a_{ij}\cos^2\theta_{ij} - a_{ji}\sin^2\theta_{ij} + \p{a_{ii}-a_{jj}}\sin\theta_{ij}\cos\theta_{ij} & \text{for $k=i$ and $\ell = j$} \\ a_{ji}\cos^2\theta_{ij} - a_{ij}\sin^2\theta_{ij} + \p{a_{jj}-a_{ii}}\sin\theta_{ij}\cos\theta_{ij} & \text{for $k=j$ and $\ell=i$} \\ a_{jj} \cos^2\theta_{ij}  + a_{ii}\sin^2 \theta_{ij} + \p{a_{ij} + a_{ji}}\sin\theta_{ij}\cos\theta_{ij} & \text{for $k=\ell=j$} \\ a_{i \ell} \cos\theta_{ij} -  a_{j\ell} \sin\theta_{ij} & \text{for $k=i$ and $\ell\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{j\ell} \cos\theta_{ij} + a_{i\ell}\sin\theta_{ij} & \text{for $k=j$ and $\ell\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{ki} \cos\theta_{ij} - a_{kj}\sin\theta_{ij} & \text{for $\ell=i$ and $k\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{kj}\cos\theta_{ij} + a_{ki}\sin\theta_{ij} & \text{for $\ell=j$ and $k\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{k\ell} & \text{elsewhere.}\end{cases}
\end{align*}

The strategy is to set $b_{ij}$ and $b_{ji}$ to zero, which enables us by iteration to zero out every element in the matrix except the diagonal elements (as described in section \ref{sec_Diagonalize}). Unfortunately we have only one variable, namely $\theta_{ij}$, which makes it impossible to diagonalize a general matrix $\mathbf{A}$ by similarity. However when $\mathbf{A}$ is a symmetric matrix, $a_{ij}=a_{ji}$, which gives us only one equation to solve to set both $b_{ij}$ and $b_{ji}$ to zero. For a symmetric matrix $\mathbf{A}$ we have the following elements of the similarity matrix $\mathbf{B}$

\begin{align}
b_{k\ell} = \begin{cases} a_{ii} \cos^2\theta_{ij}  + a_{jj}\sin^2 \theta_{ij} - 2 a_{ij}\sin\theta_{ij}\cos\theta_{ij} & \text{for $k=\ell=i$} \\ a_{ij}\p{\cos^2\theta_{ij} - \sin^2\theta_{ij}} + \p{a_{ii}-a_{jj}}\sin\theta_{ij}\cos\theta_{ij} & \text{for $k=i$ and $\ell = j$, or $k=j$ and $\ell = i$} \\ a_{jj} \cos^2\theta_{ij}  + a_{ii}\sin^2 \theta_{ij} + 2 a_{ij}\sin\theta_{ij}\cos\theta_{ij} & \text{for $k=\ell=j$} \\ a_{i \ell} \cos\theta_{ij} -  a_{j\ell} \sin\theta_{ij} & \text{for $k=i$ and $\ell\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{j\ell} \cos\theta_{ij} + a_{i\ell}\sin\theta_{ij} & \text{for $k=j$ and $\ell\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{ki} \cos\theta_{ij} - a_{kj}\sin\theta_{ij} & \text{for $\ell=i$ and $k\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{kj}\cos\theta_{ij} + a_{ki}\sin\theta_{ij} & \text{for $\ell=j$ and $k\in\mathbb{N}_1^n / \kr{i,j}$} \\ a_{k\ell} & \text{elsewhere.}\end{cases}
\label{eq_14}
\end{align}

Now we are able to solve the following equation

\begin{align*}
b_{ij}=b_{ji} = a_{ij}\p{\cos^2\theta_{ij} - \sin^2\theta_{ij}}+\p{a_{ii}-a_{jj}}\sin\theta_{ij}\cos\theta_{ij} = 0 \,,
\end{align*}

by introducing $\tan\theta = \frac{\sin\theta}{\cos\theta}$ we can rewrite this equation to

\begin{align*}
\tan^2\theta_{ij} + 2\tau_{ij}\tan\theta_{ij} -1 = 0
\end{align*}

where $\tau_{ij} = \frac{a_{jj}-a_{ii}}{2a_{ij}}$, which gives the solution

or

\begin{align}
\tan\theta_{ij} = -\tau_{ij} \pm \sqrt{1+{\tau_{ij}}^2} = -\begin{cases} \frac{1}{\tau_{ij}-\sqrt{1+{\tau_{ij}}^2}} & \text{when } \tau_{ij}< 0 \\ \frac{1}{\tau_{ij}+\sqrt{1+{\tau_{ij}}^2}} & \text{otherwise,} \end{cases}
\label{eq_15}
\end{align}

where I have chosen the $\pm$ solution such that we avoid loss of numerical precision when subtracting almost equal parts. This makes $\abs{\tan\theta} \leq 1$, because of the chose $\abs{\tau_{ij}\pm\sqrt{1+{\tau_{ij}}^2}} \geq 1$, which makes $\theta_{ij} \in \kl{-\frac{\pi}{4}+n\pi,\frac{\pi}{4}+n\pi}$ where $n\in\mathbb{N}$. $\mathbb{N} = \mathbb{N}_{-\infty}^{\infty}$ is here all positive and negative integers including zero, which is usually indicated by $\mathbb{Z}$. However my notation $\mathbb{N}_n^m$ is the integers between $n$ and $m$, which may include negative integers, hence $\mathbb{Z}$ is a redundant notation that is not needed. We know that $\tan\theta = \cot^{-1}\theta$ and we see from \eqref{eq_15} that $-\tau_{ij}\pm\sqrt{1+\tau_{ij}} = \p{-\tau_{ij}\pm\sqrt{1+\tau_{ij}}}^{-1}$, which means that we can choose the following solution instead

\begin{align}
\cot\theta_{ij} = -\tau_{ij} \mp \sqrt{1+{\tau_{ij}}^2} = -\begin{cases} \frac{1}{\tau_{ij}-\sqrt{1+{\tau_{ij}}^2}} & \text{when } \tau_{ij}< 0 \\ \frac{1}{\tau_{ij}+\sqrt{1+{\tau_{ij}}^2}} & \text{otherwise,} \end{cases} \,.
\label{eq_16}
\end{align}

where we now have $\abs{\tau_{ij}\pm\sqrt{1+{\tau_{ij}}^2}} \geq 1$ which makes $\theta_{ij} \in \kl{\frac{\pi}{4}+n\pi,\frac{3\pi}{4}+n\pi}$ instead. Now using the fact that $1+\cot^2\theta = \frac{1}{\sin^2\theta}$ we can calculate

\begin{align}
\sin\theta_{ij} = \frac{1}{\sqrt{1+\cot^2\theta_{ij}}} \,,
\label{eq_17}
\end{align}

and $\cos\theta_{ij}$ is simply calculated $\cos\theta_{ij} = \sin\theta_{ij}\cot\theta_{ij}$. The reason for this change in solution of $\theta_{ij}$ is because we want $\sin^2\theta_{ij}$ when we calculate the diagonal elements in \eqref{eq_14} by rewriting them by using the trigonometrical relation $\cos^2\theta+\sin^2\theta = 1$;

\begin{align}
b_{ii} &= a_{ii}\cos^2\theta_{ij} + a_{jj}\sin^2\theta_{ij} - 2 a_{ij}\sin\theta_{ij}\cos\theta_{ij} 
= a_{ii}\p{1-\sin^2\theta_{ij}} + a_{jj}\sin^2\theta_{ij} - 2 a_{ij}\sin\theta_{ij}\cos\theta_{ij}
\nonumber\\
&= a_{ii} + \kl{\p{a_{jj}-a_{ii}}\sin^2\theta_{ij} - 2 a_{ij}\sin\theta_{ij}\cos\theta_{ij}}
\label{eq_18}
\\
b_{jj} &= a_{jj}\cos^2\theta_{ij} + a_{ii}\sin^2\theta_{ij} + 2a_{ij}\sin\theta_{ij}\cos\theta_{ij}
= a_{jj}\p{1-\sin^2\theta_{ij}} + a_{ii}\sin^2\theta_{ij} - 2a_{ij}\sin\theta_{ij}\cos\theta_{ij}
\nonumber\\
&= a_{jj} - \kl{\p{a_{jj}-a_{ii}}\sin^2\theta_{ij} - 2a_{ij}\sin\theta_{ij}\cos\theta_{ij}}
\label{eq_19}
\end{align}

Where the point here is that we calculate $\sin^2\theta_{ij}$ before $\sin\theta_{ij}$ in \eqref{eq_17} to reduce number of FLOPS we need. Note that it's better to use $\sin^2\theta$ instead of $\cos^2\theta$, because when we look at 

\begin{align*}
b_{ii} &= a_{jj} + \kl{\p{a_{ii}-a_{jj}}\cos^2\theta_{ij} - 2a_{ij}\cos\theta_{ij}\sin\theta_{ij}}
\\
b_{jj} &= a_{ii} - \kl{\p{a_{ii}-a_{jj}}\cos^2\theta_{ij} - 2a_{ij}\cos\theta_{ij}\sin\theta_{ij}} \,,
\end{align*}

we see that $a_{jj}$ corresponds to $b_{ii}$ and $a_{ii}$ corresponds to $b_{jj}$, which means that we need to backup either $a_{ii}$ or $a_{jj}$ before we do the calculation, so we don't overwrite the value before we have used in the calculation. However for the $\sin^2\theta_{ij}$ expressions we have that $a_{ii}$ corresponds to $b_{ii}$ and $a_{jj}$ corresponds to $b_{jj}$, which means that we don't need to backup $a_{ii}$ or $a_{jj}$, and we can  use the program operator \texttt{+=} directly on the diagonal elements. \linebreak

We see from the terms $a_{xy}\cos\theta_{ij}\pm a_{zw}\sin\theta_{ij}$ in \eqref{eq_14} that elements that was zero in $\textbf{A}$ isn't necessarily zero in $\textbf{B}$. And we therefore seeks to minimize the off diagonal elements by searching for the largest of diagonal elements and zero it out by the Jacobi method. This minimize the number of rotation transformation we need to do, however it requires $\mathcal{O}\p{n^2}$ to search for the maximum off diagonal element.

\subsection{Francis QR algorithm}\label{sec_QR}

We want to find the eigenvalues of a tridiagonal symmetric matrix $\textbf{A}_0$ of $n\times n$ with QR decomposition on

\begin{align*}
\textbf{A}_0 = \textbf{S}_0\textbf{U}_0 \,,
\end{align*}

where $\text{S}_0$ is the orthogonal matrix $\textbf{Q}$ and $\textbf{U}_0$ is the upper triangle matrix $\textbf{R}$ in the QR decomposition. We can show that the similar matrix $\textbf{A}_1$ to $\textbf{A}_0$ is linked to the QR decomposition as follows

\begin{align*}
\textbf{A}_1 = \textbf{S}_0^{\text{T}} \textbf{A}_0 \textbf{S}_0 
=\textbf{S}_0^{\text{T}} \textbf{S}_0\textbf{U}_0 \textbf{S}_0 = \textbf{U}_0 \textbf{S}_0 \,,
\end{align*}

where I have used the orthogonal property of $\textbf{S}_0$ such that $\textbf{S}_0^{\text{T}} \textbf{S}_0 = \textbf{I}$ ($\textbf{I}$ is the identity martrix). Now imagine that we use QR decomposition on $\textbf{A}_1$ and find the similarity matrix $\textbf{A}_2$ in the same manner. We can now show by induction that this process for step $i+1$ by using the QR decomposition on $\textbf{A}_i$;

\begin{align}
\textbf{A}_i = \textbf{S}_i \textbf{U}_i \,,
\label{eq_20}
\end{align}

and we now find the similarity matrix $\textbf{A}_{i+1}$ to $\textbf{A}_i$ as follows

\begin{align}
\textbf{A}_{i+1} = \textbf{S}_i^{\text{T}} \textbf{A}_i \textbf{S}_i 
=\textbf{S}_i^{\text{T}} \textbf{S}_i\textbf{U}_i \textbf{S}_i = \textbf{U}_i \textbf{S}_i \,.
\label{eq_21}
\end{align}

We assume that $\textbf{A}_i$ is a symmetric tridiagonal matrix with the elements 

\begin{align*}
a_{ik\ell} = \begin{cases} a_{ik(k+\abs{\ell-k})} & \text{when } \ell \in \mathbb{N}_{k-1}^{k+1} \\ 0 & \text{otherwise,}\end{cases}
\end{align*}

which I will show by induction. To find the upper matrix we use successively rotation matrices $\textbf{R}_{ij}$ to eliminate the elements in the lower triangle part of the tridiagonal matrix $\textbf{A}_i$ 

\begin{align}
\textbf{U}_i = \prod_{j=1}^{n-1} \textbf{R}_{i(n-j)}^{\text{T}} \textbf{A}_i \,,
\label{eq_22}
\end{align}

where the rotation matrix $\textbf{R}_{ij}$ is defined as

\begin{align*}
r_{ijk\ell} = \begin{cases} \cos\theta_{ij} & \text{when } k,\ell \in\mathbb{N}_j^{j+1} \\ \sin\theta_{ij} & \text{when } k=\ell-1= j \\ -\sin\theta_{ij} & \text{when } k-1=\ell= j \\ 1 & \text{when } k,\ell\in\mathbb{N}_1^n/\mathbb{N}_j^{j+1} \\ 0 & \text{elsewhere.} \end{cases}  
\end{align*}

Note that the rotation matrix is orthogonal which means that the elements of $\textbf{R}_{ij}^{\text{T}}$ is given by $r_{ijk\ell}^{\text{T}} = r_{ij\ell k}$. I continue by defining the matrix

\begin{align*}
\textbf{A}_{i(j+1)} = \begin{cases}\textbf{A}_i &\text{when } j=0 \\ \textbf{R}_{ij}^{\text{T}}\textbf{A}_{ij} & \text{when } j\in\mathbb{N}_1^{n-1}\,, \end{cases}
\end{align*}

which means that we are going to show that $\textbf{U}_i = \textbf{A}_{in}$, where the matrix $\textbf{A}_{ij}$ has the elements $a_{ijk\ell}$. I start by doing the matrix multiplication $\textbf{A}_{i2} = \textbf{R}_{i1}^{\text{T}}\textbf{A}_{i1}$;

\begin{align*}
a_{i2k\ell} &= \sum_{m=1}^n r_{i1km}^{\text{T}} a_{i1m\ell} = \sum_{m=-1}^1 r_{i1k(\ell+m)}^{\text{T}} a_{i1(\ell+m)\ell}
= \begin{cases} \sum_{m=-1}^1 r_{i1k(\ell+m)}^{\text{T}} a_{i1(\ell+m)\ell} & \text{when } \p{k,\ell+m}\in{\leftidx{^2}{\mathbb{N}}{_1^2}}\\ a_{i1k\ell} & \text{otherwise.}\end{cases} 
\\
&= \begin{cases} \sum_{m=-1}^1 r_{i1k(\ell+m)}^{\text{T}} a_{i1(\ell+m)\ell} & \text{when } \p{k,\ell+m}\in{\leftidx{^2}{\mathbb{N}}{_1^2}} \text{ and } \ell \in\mathbb{N}_1^3\\ a_{i1k\ell} & \text{otherwise.}\end{cases}
\end{align*}

The notation $\leftidx{^2}{\mathbb{N}}{_1^2} = \mathbb{N}_1^2 \times \mathbb{N}_1^2$, which means all the permutations of the natural number from 1 to 2; $(1,1)$, $(1,2)$, $(2,1)$ and $(2,2)$. We want our transformation to zero out the lower triangle element $(2,1)$

\begin{align*}
a_{i221} = \sum_{m=-1}^1 r_{i12(m+1)}^{\text{T}} a_{i1(m+1)1} = \sum_{m=0}^1 r_{i12(m+1)}^{\text{T}} a_{i1(m+1)1} = 0\,,
\end{align*}

which means that 

\begin{align*}
a_{i2k\ell} = \begin{cases} \sum_{m=-1}^1 r_{i1k(\ell+m)}^{\text{T}} a_{i1(\ell+m)\ell} & \text{when } \p{k,\ell+m}\in{\leftidx{^2}{\mathbb{N}}{_1^2}}, (k,\ell)\neq(2,1) \text{ and } \ell \in\mathbb{N}_1^3\\ 0 & \text{when } (k,\ell)=(2,1) \\ a_{i1k\ell} & \text{otherwise.}\end{cases}
\end{align*}

Note that all the lower off diagonal elements  $a_{i2k\ell}$ is the same as $a_{i1k\ell}$, expect $a_{i221} = 0$. Note also that $a_{i213}$ is the only upper triangle element that isn't necessarily zero that was zero in matrix $\textbf{A}_{i1}$. If we now continue the matrix multiplication to step $j$, $\textbf{A}_{i(j+1)} = \textbf{R}_{ij}^{\text{T}} \textbf{A}_{ij}$;

\begin{align*}
a_{i(j+1)k\ell} &= \sum_{m=1}^n r_{ijkm}^{\text{T}} a_{ijm\ell} = \sum_{m=-1}^1 r_{ijk(\ell+m)}^{\text{T}} a_{ij(\ell+m)\ell} 
= \begin{cases} \sum_{m=-1}^1 r_{ijk(\ell+m)}^{\text{T}} a_{ij(\ell+m)\ell} & \text{when } \p{k,\ell+m}\in{\leftidx{^2}{\mathbb{N}}{_j^{j+1}}}\\ a_{ijk\ell} & \text{otherwise.}\end{cases}
\\
&= \begin{cases} \sum_{m=-1}^1 r_{ijk(\ell+m)}^{\text{T}} a_{ij(\ell+m)\ell} & \text{when } \p{k,\ell+m}\in{\leftidx{^2}{\mathbb{N}}{_j^{j+1}}} \text{ and } \ell \in\mathbb{N}_{j-1}^{j+2}\\ a_{ijk\ell} & \text{otherwise.}\end{cases}
\\
&= \begin{cases} \sum_{m=-1}^1 r_{ijk(\ell+m)}^{\text{T}} a_{ij(\ell+m)\ell} & \text{when } \p{k,\ell+m}\in{\leftidx{^2}{\mathbb{N}}{_j^{j+1}}} \text{ and } \ell \in\mathbb{N}_{j}^{j+2}\\ a_{ijk\ell} & \text{otherwise,}\end{cases}
\end{align*}

where I have used that $a_{ijj(j-1)}=0$ due to zero out of the previous rotation. We now want our transformation to zero out the lower triangle element $(j+1,j)$

\begin{align*}
a_{i(j+1)(j+1)j} = \sum_{m=-1}^1 r_{i,j,j+1,j+m}^{\text{T}} a_{ij(j+m)j} = \sum_{m=0}^1 r_{ij(j+1)(j+m)}^{\text{T}} a_{ij(j+m)j} = 0
\end{align*}

where I have used the fact that $r_{ij(j+1)(j-1)}^{\text{T}} = 0$. This means that

\begin{align}
a_{i(j+1)k\ell} = \begin{cases} \sum_{m=-1}^1 r_{ijk(\ell+m)}^{\text{T}} a_{ij(\ell+m)\ell} & \text{when } \p{k,\ell+m}\in{\leftidx{^2}{\mathbb{N}}{_j^{j+1}}}, (k,\ell)\neq (j+1,j) \text{ and } \ell \in\mathbb{N}_{j}^{j+2}\\ 0 & \text{when } (k,\ell)=(j+1,j) \\ a_{ijk\ell} & \text{otherwise.}\end{cases} 
\label{eq_23}
\end{align}

I have now shown that all the lower off diagonal elements $a_{i(j+1)k\ell}$ is the same as $a_{ijk\ell}$, expect $a_{i(j+1)(j+1)j}=0$. Then using the assumption that $\textbf{A}_i$ is a tridiagonal matrix, induction gives us that $\textbf{A}_{in} = \textbf{U}_i$ is a upper triangle matrix. Furthermore $a_{i(j+1)j(j+2)}$ is the only upper triangle element that isn't zero that was zero in the matrix $\textbf{A}_{ij}$, hence induction give us the following upper triangle matrix elements

\begin{align}
u_{ik\ell} = \begin{cases} a_{ink\ell} & \text{when } \ell\in\mathbb{N}_k^{k+2} \\ 0 & \text{otherwise.} \end{cases}
\label{eq_24}
\end{align}

Since $\textbf{A}_i = \textbf{S}_i \textbf{U}_i$ and \eqref{eq_22} means that 

\begin{align*}
\textbf{S}_i^{\text{T}} = \prod_{j=1}^{n-1} \textbf{R}_{i(n-j)}^{\text{T}} \,,
\end{align*}

and due to the similarity transformation $\textbf{A}_{i+1} = \textbf{S}_i^{\text{T}} \textbf{A}_i \textbf{S}_i = \prod_{j=1}^{n-1}\textbf{R}_{i(n-j)}^{\text{T}} \textbf{A}_i \prod_{i=1}^{n-1}\textbf{R}_{ij}$ we have that

\begin{align*}
\textbf{S}_i = \prod_{j=1}^{n-1} \textbf{R}_{ij} \,.
\end{align*}

To find the similarity matrix $\textbf{A}_{i+1}$ to $\textbf{A}_i$ we use successively rotation matrices $\textbf{R}_{ij}$ by defining the matrix

\begin{align*}
\textbf{U}_{i(j+1)} = \begin{cases} \textbf{U}_i & \text{when } j = 0 \\ \textbf{U}_{ij}\textbf{R}_{ij} & \text{when } j\in \ \mathbb{N}_1^{n-1}\,, \end{cases}
\end{align*}

where $\textbf{U}_{in} = \textbf{A}_{i+1}$. Doing the matrix multiplication $\textbf{U}_{i2} = \textbf{U}_{i1} \textbf{R}_{i1}$;

\begin{align*}
u_{i2k\ell} &= \sum_{m=1}^n u_{i1km} r_{i1m\ell} = \sum_{m=0}^2 u_{i1k(k+m)} r_{i1(k+m)\ell} 
= \begin{cases} \sum_{m=0}^2 u_{i1k(k+m)} r_{i1(k+m)\ell} & \text{when } (k+m,\ell)\in{\leftidx{^2}{\mathbb{N}}{_1^2}} \\ u_{i1k\ell} & \text{otherwise} \end{cases}
\\
&= \begin{cases} \sum_{m=0}^1 u_{i1k(k+m)} r_{i1(k+m)\ell} & \text{when } (k+m,\ell)\in{\leftidx{^2}{\mathbb{N}}{_{1}^2}} \text{ and } k\in\mathbb{N}_1^2  \\ u_{i1k\ell} & \text{otherwise.} \end{cases}
\end{align*} 

Note that all lower off diagonal elements $u_{i2k\ell}$ is the same as $u_{i1k\ell}$, except $u_{i221}$ which now may be different than zero. If we now continue the matrix multiplication to step $j$, $\textbf{U}_{i(j+1)} = \textbf{U}_{ij}\textbf{R}_{ij}$;

\begin{align}
u_{i(j+1)k\ell} &= \sum_{m=1}^n u_{ijkm} r_{ijm\ell} = \sum_{m=0}^2 u_{ijk(k+m)} r_{ij(k+m)\ell} 
= \begin{cases} \sum_{m=0}^2 u_{ijk(k+m)} r_{ij(k+m)\ell} & \text{when } (k+m,\ell)\in{\leftidx{^2}{\mathbb{N}}{_{j}^{j+1}}} \\ u_{ijk\ell} & \text{otherwise} \end{cases}
\nonumber\\
&= \begin{cases} \sum_{m=0}^2 u_{ijk(k+m)} r_{ij(k+m)\ell} & \text{when } (k+m,\ell)\in{\leftidx{^2}{\mathbb{N}}{_{j}^{j+1}}} \text{ and } k\in\mathbb{N}_{j-2}^{j+1}  \\ u_{ijk\ell} & \text{otherwise,} \end{cases}
\label{eq_25}
\end{align}

and we see that the lower off diagonal elements $u_{i(j+1)k\ell}$ is the same as $u_{ijk\ell}$, except $u_{i(j+1)(j+1)j}$ which now may be different than zero. Hence by induction I shown that the only non-zero lower off diagonal elements in $\textbf{A}_{i+1}=\textbf{U}_{in}$ is the elements $u_{ink(k-1)}$;

\begin{align}
a_{(i+1)k\ell} = \begin{cases} u_{ink\ell} & \text{when } \ell\in\mathbb{N}_{k-1}^n \\ 0 & \text{otherwise.} \end{cases}
\label{eq_26}
\end{align}

But we can actually show that $\textbf{A}_{i+1}$ is tridiagonal matrix by using the assumption that $\textbf{A}_i$ is a symmetric matrix. This we can do by showing that when $\textbf{A}_i$ is symmetric then $\textbf{A}_{i+1}$ is also symmetric by doing the matrix multiplication of the similarity transformation $\textbf{A}_{i+1}^{\text{T}} = \p{\textbf{S}_i^{\text{T}}\textbf{A}_i \textbf{S}_i}^\text{T}$;

\begin{align*}
a_{(i+1)k\ell}^{\text{T}} = \p{\sum_{m,o=1}^n s_{ikm}^{\text{T}}a_{imo} s_{io\ell}}^{\text{T}} = \sum_{m,o=1}^n s_{i\ell m}^{\text{T}}a_{imo} s_{iok} = a_{(i+1)\ell k} \,.
\end{align*}

Since we have shown that $\textbf{A}_{i+1}$ is symmetric due to that $\textbf{A}_i$ is symmetric, and that $\textbf{A}_{i+1}$ has only non-zero elements in the lower off diagonal $a_{ik(k-1)}$, means that $a_{ik(k+1)}$ is the only higher off diagonal elements that are non-zero as well;

\begin{align}
a_{(i+1)k\ell} = \begin{cases} a_{(i+1)k(k+\abs{\ell-k})} & \text{when } \ell \in \mathbb{N}_{k-1}^{k+1} \\ 0 & \text{otherwise.}\end{cases}
\label{eq_27}
\end{align}

And we have indeed shown by induction that $\textbf{A}_i$ is symmetric tridiagonal matrix because the initial matrix $\textbf{A}_0$ is symmetric tridiagonal matrix. Now if the off diagonal elements decreases for each QR transformation, then the similarity matrices $\textbf{A}_i$ will converge to a diagonal matrix, and hence we have found the eigenvalues. \linebreak

Lastly I will show to calculate the rotation matrix $\textbf{R}_{ij}$ from the already established relation

\begin{align*}
a_{i(j+1)(j+1)j} = \sum_{m=0}^1 r_{ij(j+1)(j+m)}^{\text{T}} a_{ij(j+m)j} = a_{ij(j+1)j}\cos\theta_{ij}-a_{ijjj}\sin\theta_{ij} = 0 \,,
\end{align*}

which gives

\begin{align}
\cos\theta_{ij} &= \frac{a_{ijjj}}{\sqrt{{a_{ijjj}}^2+{a_{ij(j+1)j}}^2}}
\label{eq_28}
\\
\sin\theta_{ij} &= \frac{a_{ij(j+1)j}}{\sqrt{{a_{ijjj}}^2+{a_{ij(j+1)j}}^2}} \,.
\label{eq_29}
\end{align}

The method described here suggest to calculate the upper tridiagonal matrix $\textbf{U}_i$ and then similarity matrix $\textbf{A}_{i+1}$, but this way of doing it would require we must store the calculated angles $\theta_{ij}$. However from \eqref{eq_23} we see  that elements $a_{i(j+1)k\ell}$ with $k < j$ is not changed, and from \eqref{eq_25} we see that elements $u_{i(j+1)k\ell}$ with $k >j+1$, which means that we can calculate $u_{ijk\ell}$ after $a_{i(j+1)k\ell}$. This require us only to store one set of $\sin\theta_{ij}$ and $\cos\theta_{ij}$ values, because we need $\sin\theta_{ij}$ and $\cos\theta_{ij}$ after we have calculated $\sin\theta_{i(j+1)}$ and $\cos\theta_{i(j+1)}$, but for the steps afterwards we don't need the $\sin\theta_{ij}$ and $\cos\theta_{ij}$ anymore. Hence the only memory needed to solve the symmetric tridiagonal matrix eigenvalue problem is the memory of the symmetric tridiagonal matrix it self.

\subsection{Eigenvectors}\label{sec_Eigenvectors}

When we do a similarity transformation with the change of basis matrix $\textbf{S}$, the eigenvector $\textbf{x}$ changes to $\textbf{S}^{-1}\textbf{x}$, as discussed in section \ref{sec_Diagonalize}. Which means that you would have to update your approximation to the eigenvectors for each similarity transform. Since an matrix of dimensionality $n$ has $n$ eigenvalues and therefore $n$ eigenvectors, we would actually need to calculate new values for the eigenvectors $n^2$ times, which is very costly. Granted, for a simple rotation matrix there are only $2n$ elements we need to update. Never the less this has a huge impact on preformance on the Francis QR algorithm for instance, which has $n-1$ rotations for each QR transformation. \linebreak

However if we assume that we where able to calculate a very good approximation of the eigenvalues, we can calculate the eigenvectors afterwards. For symmetric tridiagonal matrix this becomes very simply. The idea is to use Gaussian elimination on \eqref{eq_2}  to eliminate all $e_{i+1}$ elements (lower off diagonal elements). In our case of the radial Schr\"{o}dinger equation, we can rewrite \eqref{eq_2} such that the elements $e_i = -1$, which is very beneficial with regard to number of FLOPS needed. So I will continue with $e_i=-1$.\linebreak

I start by using the Gaussian elimination to eliminate the element $a_{21}$ in symmetric tridiagonal matrix $\text{A}$ of dimensionality $n$ with $e_i=-1$. This gives

\begin{align*}
\check{a}_{22} = a_{22} - \lambda -\frac{1}{\check{a}_{11}} \,,
\end{align*}

where $\check{a}_{11} = a_{11}$ and $\lambda$ is our approximated eigenvalue. And we can show by induction that

\begin{align}
\check{a}_{ii} = a_{ii} - \lambda -\frac{1}{\check{a}_{(i-1)(i-1)}} \qquad \text{for } i\in\mathbb{N}_2^{n-1} \text{ with } \check{a}_{11} = a_{11}\,.
\label{eq_30}
\end{align}

If the $\lambda$ is an good approximation to a eigenvalue to $\textbf{A}$, we will then get that $\p{a_{nn}-\lambda}x_{n}-x_{n-1} \approx c\p{\check{a}_{(n-1)(n-1)} x_{n-1} - x_n}$, where $c$ is a constant. By assuming that this approximation is good, we can exclude row $n$, and we can easily show that the eigenvector elements $x_i$ are related as follows

\begin{align}
x_{i+1} = \check{a}_{ii} x_i \qquad \text{for } i\in\mathbb{N}_1^{n-1} \,.
\label{eq_31}
\end{align}

As you can see, we can now quite easily iteratively calculate an approximate eigenvector after we have calculated the approximated eigenvalue, and the performance cost is very low. Note that one has to start with choosing an value for $x_1$, where a natural chose would be $x_1 = 1$.  And then you would have to normalize the eigenvector after all the elements are calculated, if you want it normalized. \linebreak

The disadvantage with this method is that \eqref{eq_31} is approximately correct, which means that $\check{a}_{ii}$ has an error. By doing the calculation in accordance with \eqref{eq_31}, we accumulate more an more errors for each iterations. This means that the eigenvector elements $x_i$ early in the iteration has a very good approximation to the real eigenvector, but will have a very bad approximation at the end. When you calculate the new eigenvector $\textbf{S}^{-1}\textbf{x}$ for each rotation instead, you have distrusted the error between the eigenvector elements, and therefore don't have this problem.

However in our case when we study the Schr\"{o}dinger equation for a electron, we let $\rho_{\text{max}}\to intry$, but we are only interested at the start of the interval of $\rho$. Which means we are free to cut off the end elements of the eigenvector and still get what we want, and then this Gaussian method of finding the eigenvector will give a good approximation. And indeed you can start your Gaussian method of iteration at an arbitrary element, to target a specific are of the eigenvector that you want a good approximation.

\section{Numerical implementation}

\subsection{Matrices}

Since the properties of a matrices is important for choosing the most effective method for solving a problem with regard to speed and memory usage, have I made partial specializations of a template class \texttt{template<MatrixType Type, class T> class Matrix}, where \texttt{MatrixType} it the partial specialization and $\texttt{class T}$ specializes the matrix class to the type \texttt{T}.

\begin{lstlisting}[title={Matrix partial specialization}]
enum class MatrixType {
    Square,
    SquareT,
    Symmetric,
    Tridiagonal,
    TridiagonalSymmetric,
    Tridiagonal_m1_X_m1,
    Tridiagonal_m1_2_m1,
    Tridiagonal_m1_2_m1_6n,
    Tridiagonal_m1_2_m1_4n,
    LU_decomposition,
    tqli
};
\end{lstlisting}

This enables enables us to use \texttt{MatrixType} to tell the compiler which type of matrix we want, and the compiler will then link up the solvers and functionality that is tailored for the desired matrix. In addition the program does not need to allocate more memory that are needed to represent the properties of the matrix, which will save a lot of memory for tridiagonal matrices for instance. This is all well and fine, but actually not needed, we could just name the different matrices classes differently and we would achieve the the same thing. But it's still nicer to call all the different matrix class for \texttt{Matrix}, and not \texttt{Matrix1}, \texttt{Matrix2}, ..., \texttt{Matrixn} for instance. However the true power of partial specialization comes to light when a function takes a matrix as input, but don't care what kind of matrix it is but only that it's matrix. An example of such a function is a function that print out the content of a matrix;

\begin{lstlisting}[title={Printing a matrix}]
template<MatrixType Type, class T> void MatrixCout(Matrix<Type, T>& matrix) {
    for(unsigned int i = 0; i < matrix.n; i++) {
        for(unsigned int j = 0; j < matrix.n; j++)
            cout << matrix(i,j) << "\t";
        cout << '\n';
    }
    cout << '\n';
}

template <class M, class T> T& MatrixElements<M,T>::operator() (const unsigned int& row, const unsigned int& col) { 
	return matrix[row][col];
}

template <class T> T& MatrixElements<Matrix<MatrixType::Symmetric, T>, T>::operator() (const unsigned int& row, const unsigned int& col) {
	return row < col ? matrix[col-row][row] : matrix[row-col][col];
}

template <class T> T& MatrixElements<Matrix<MatrixType::TridiagonalSymmetric, T>, T>::operator() (const unsigned int row, const unsigned int col) {    
   	if(row == col)
    		return b[row];
   	else if(row-1 == col)
    		return a[col];
    	else if(row == col-1)
		return a[row];
	other = 0;
	return other;
}
\end{lstlisting}

Furthermore you can use the partial specialization to tell the compiler how you want two different partial specialized class to interact. An example is if you want to take matrix multiplication of a tridiagonal matrix and a square matrix, you could let the compiler figure out how this should be linked up without you actually needing to know what kind of matrices you are multiplying. The compiler will always keep track of the matrix type, even if it's almost impossible for you to know it. This gives you a very high level feature on a low programming level, which makes it easy to optimize your code for speed and memory usage without having to do the hard work to link up optimized code, the compiler does it for you. The disadvantage is that it requires some fancy programming skills to write such partial specialized classes that tells how the compiler should link up the different cases. And if you do something wrong with the syntax, the compiler is usually less then helpful to tell you what is wrong. But when the difficult work of actually writing such classes the application of them becomes quite robust and easy to use in combination with C11 features \texttt{auto} and \texttt{decltype}, which enables you to hide the cumbersome and long names of the classes with the partial specialization. One should also keep in mind that such a code put a lot of work load on the compiler and the different possibilities of combination of partial specialization is written as code at compile time by the compiler, which makes your program substantially larger in size. \linebreak

Sometimes you want a partial specialization of a function, but unfortunately this is not supported, and you would have to do a work around to achieve it. An example is a base class of $\texttt{Matrix}$ that sets value on a diagonal in the matrix. Sometimes you want only to set the same value on the entire diagonal, and other times you want to set an array on the diagonal. Make such a function a template function that calls the partial specialized classes static function with the template argument, and then you have actually partially specialized your function;

\begin{lstlisting}[title={The base class \texttt{MatrixDiagonal<M, T>}}]
template<class M, class T> class MatrixDiagonal {
    private: M* owner;
    private: template<class P> class Type {
        public: static inline void Diagonal(M* owner, const int diagonal, const P value, const unsigned int n) {
            unsigned int d = abs(diagonal);
            owner->n = n;
            unsigned int nmax = n - d;
            if(diagonal > 0) {
                for(unsigned int i = 0; i < nmax; i++)
                    owner->operator()(i,i+d) = value;
            } else {
                for(unsigned int i = 0; i < nmax; i++)
                    owner->operator()(i+d,i) = value;
            }
        }
    };
    private: template<class P> class Type<P*> {
        public: static inline void Diagonal(M* owner, const int diagonal, const P* value, const unsigned int n) {
            unsigned int d = abs(diagonal);
            owner->n = n;
            unsigned int nmax = n - d;
            if(diagonal > 0) {
                for(unsigned int i = 0; i < nmax; i++)
                    owner->operator()(i,i+d) = value[i];
            } else {
                for(unsigned int i = 0; i < nmax; i++)
                    owner->operator()(i+d,i) = value[i];
            }
        }
    };
    protected: MatrixDiagonal(M* owner) : owner(owner) {
    }
    public: template<class P> inline void Diagonal(const int diagonal, const P value) {
        Type<P>::Diagonal(owner, diagonal, value, owner->n);
    }
    public: template<class P> inline void Diagonal(const int diagonal, const P value, const unsigned int n) {
        Type<P>::Diagonal(owner, diagonal, value, n);
    }
};
\end{lstlisting} 

And now you can call the \anf{same} function \texttt{Diagonal} when you have only one value or an array of values.  

\begin{lstlisting}[title={Example of usage of partial specialized \texttt{MatrixDiagonal<M, T>::Diagonal}}]
double* d = new double[10];
auto matrix = Matrix<MatrixType::TridiagonalSymmetric, double>(10);
matrix.Diagonal(0, d);
matrix.Diagonal(1,-1);
\end{lstlisting} 

\subsection{Jacobi's eigenvalue algorithm}

This algorithms solve only the eigenvalue problem, which means that the eigenvectors are not calculated here.

\subsubsection{Jacobi lecture} \label{sec_Jacobi_lecture}

This is mostly a direct implementation of the Jacobi algorithm given in the lecture notes \cite[p. 218]{lecture}. Note that this implementation uses a complete square matrix even though it solves for a symmetric matrix. This means that half of the memory is redundant.

\begin{lstlisting}[title={\texttt{template<class T> class Matrix<MatrixType::Square, T>::JacobiMethod}}]
void JacobiMethod(T error, unsigned int &num /**/) {
	MatrixIndex I;
	T s, c, t, tau, a_kk, a_ll, a_ik, a_il;
	num = 0;
	while(error < MaxAbs(this->matrix, this->_n, I)) {
		tau = (this->matrix[I.j][I.j] - this->matrix[I.i][I.i]) / (2*this->matrix[I.i][I.j]);
		if (tau > 0)
			t = 1.0/(tau + sqrt(1.0 + tau*tau));
		else
			t = -1.0/( -tau + sqrt(1.0 + tau*tau));
		c = 1/sqrt(1+t*t);
		s = c*t;
		
		a_kk = this->matrix[I.i][I.i];
		a_ll = this->matrix[I.j][I.j];
		// changing the matrix elements with indices k and l
		this->matrix[I.i][I.i] = c*c*a_kk - 2.0*c*s*this->matrix[I.i][I.j] + s*s*a_ll;
		this->matrix[I.j][I.j] = s*s*a_kk + 2.0*c*s*this->matrix[I.i][I.j] + c*c*a_ll;
		this->matrix[I.i][I.j] = 0.0; // hard-coding of the zeros
		this->matrix[I.j][I.i] = 0.0;
		
		// and then we change the remaining elements
		for (int i = 0; i < this->_n; i++ ) {
			if ( i != I.i && i != I.j ) {
				a_ik = this->matrix[i][I.i];
				a_il = this->matrix[i][I.j];
				this->matrix[i][I.i] = c*a_ik + s*a_il;
				this->matrix[I.i][i] = this->matrix[i][I.i];
				this->matrix[i][I.j] = c*a_il - s*a_ik;
				this->matrix[I.j][i] = this->matrix[i][I.j];
			}
		}
		num++;
	}
}
\end{lstlisting}

And the search function for the maximum absolute value in the off diagonal elements.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Square, T>::MaxAbs}}]
T MaxAbs(T** matrix, unsigned int n, MatrixIndex& I) {
	T max = 0, abs1;
	for(unsigned int i = 0, j; i < n; i++) {
		for(j = i+1; j < n; j++) {
			abs1 = abs(matrix[i][j]);
			if(abs1 > max) {
				max = abs1;
				I.i = i;            
				I.j = j;            
			}
		}
	}
	return max;
}
\end{lstlisting}

\subsubsection{My Jacobi algorithms} \label{sec_my_Jacobi}

My implementation of Jacobi's algorithm uses the class \texttt{Matrix<MatrixType::Symmetric, T>} which uses the fact that we are solving a symmetric matrix, and therefore store only the upper triangle matrix in memory. The indexing of this matrix is done diagonal number \texttt{d} and row number \texttt{r} (\texttt{matrix[d][r]}), to better exploit the symmetry of the matrix. 

\begin{lstlisting}[title={Matrix indexing of \texttt{Matrix<MatrixType::Symmetric, T>}}]
T& operator() (const unsigned int& row, const unsigned int& col) {
	return row < col ? matrix[col-row][row] : matrix[row-col][col];
}
\end{lstlisting}

This is especially useful when we use it in combination with \texttt{tqli} in \texttt{lib.cpp}, where you want to send the main diagonal and the side diagonal of the tridiagonal matrix. By indexing by diagonal number we can easily obtain a pointer to the diagonals in the matrix, without doing remapping of memory.

I made a function that does rotation transformation in the Jacobi algorithm, which is basically the same as done in the lecture note \cite{lecture}, but with the optimization discussed in section \ref{sec_Jacobi}.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Symmetric, T>::Rotate}}]
void Rotate(unsigned int i, unsigned int j, unsigned int& num) {
	T b = 2*this->matrix[j][i];
	this->matrix[j][i] = 0;     // bij=bji=0
	
	j += i;	// Convert j from diagonalnumber to column number
	
	// Calculate cos and sin
	T a = this->matrix[0][j] - this->matrix[0][i];
	T t = a/b;
	if(t > 0)
		t = (T)1/(t+sqrt(1+t*t));   // Calculate -cot !
	else
		t = (T)1/(t-sqrt(1+t*t));   // Calculate -cot !
	T s2 = (T)1/(1+t*t);
	T s = sqrt(s2);
	T c = t*s;
	
	a = a*s2 + b*s*c; 	// s*c = - sin*cos because of t=-cot
	this->matrix[0][i] += a;       // bii eq(18)
	this->matrix[0][j] -= a;       // bjj eq(19)
	
	unsigned int k, l, n;
	// Calculate bik and bjk, row < i and row < j
	for(k = i, l = j, n = 0; n < i; k--, l--, n++) {
		a = this->matrix[k][n];
		b = this->matrix[l][n];
		this->matrix[k][n] = a*c - b*s;
		this->matrix[l][n] = b*c + a*s;
	}
         
   	// row > i and row < j
   	for(k++, l--, n++; n < j; k++, l--, n++) {
   		a = this->matrix[k][i];
   		b = this->matrix[l][n];
   		this->matrix[k][i] = a*c - b*s;
   		this->matrix[l][n] = b*c + a*s;
   	}
        
   	// row > i and row > j
   	for(k++, l++, n = this->_n-j; l < n; k++, l++) {
   		a = this->matrix[k][i];
   		b = this->matrix[l][j];
   		this->matrix[k][i] = a*c - b*s;
   		this->matrix[l][j] = b*c + a*s;
   	}
   	num++;	// To be removed
}
\end{lstlisting}

The last three for loops does the same, but the reason for separating them in such a way is due to the fact that we have a symmetric matrix, where only the upper triangle of the matrix is allocated in memory. So the three different for loops handles the indexing differently to make sure we operate in the correct element in the matrix.

\subsubsubsection{Jacobi}

This implementation does basically the same as the lecture implementation shown in section \ref{sec_Jacobi_lecture}, where the algorithm searches for the maximum absolute value of the off diagonal elements and zero it out by rotation transformation in the Jacobi algorithm as shown in \ref{sec_my_Jacobi}.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Symmetric, T>::JacobiMethod}}]
T* JacobiMethod(T error, unsigned int& num /**/) {
	MatrixIndex I;
	T** offdiagonal = &this->matrix[1];
	unsigned int offn = this->_n-1;
	num = 0;                                    
	while(error < MaxAbs(offdiagonal, offn, I)) 
		Rotate(I.j, ++I.i, num);
	return this->matrix[0];
}
\end{lstlisting}

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Symmetric, T>::MaxAbs}}]
T MaxAbs(T** matrix, unsigned int n, MatrixIndex& I) {
	T max = 0, abs1;
	for(unsigned int i = 0, j; n; i++, n--) {
		for(j = 0; j < n; j++) {
			abs1 = abs(matrix[i][j]);
			if(abs1 > max) {
				max = abs1;
				I.i = i; 	// I.i is the diagonal number = col-row
				I.j = j;    // I.j is the row number
			}
		}
	}
	return max;
}
\end{lstlisting}

\subsubsubsection{Jacobi FD}

The standard Jacobi algorithm searches for the maximum absolute value off diagonal element, which is quit time expensive to use since it's goes as $\mathcal{O}\p{n^2}$. But by studying the behavior of the off diagonal elements during the Jacobi algorithm a pattern emerges regarding the approximate position of the maximum diagonal elements for a symmetric  tridiagonal matrix. Hence one can develop a Jacobi algorithm that doesn't do the search for maximum absolute value off diagonal elements, but rather iterate over the elements in a certain way. I will present different ways of doing this and compare the results from the different methods. \linebreak

The first method I made is the Jacobi Forward Diagonal iteration, which first does the Jacobi transformation on all the elements in diagonal next to the main diagonal (symmetry diagonal) of the matrix. And then goes to the next diagonal and do the Jacobi transformation, and continues to the last diagonal of one element in the corner of the matrix is reached. Then the process starts from the beginning again, till all the off diagonal elements is less than a given value.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Symmetric, T>::JacobiMethodFD}}]
T* JacobiMethodFD(T error, unsigned int& num /**/) {
	bool run = true;
	num = 0;
	while(run) {
		run = false;
		// Run through the diagonals
		for(unsigned int d = 1, r, n = this->_n-1; n; d++, n--) {   
			for(r = 0; r < n; r++) {	// Run through the rows
				if(error < abs(this->matrix[d][r])) {
					run = true;
					Rotate(r, d, num);
				}
			}
		}
	}
	return this->matrix[0];
}
\end{lstlisting}

\subsubsubsection{Jacobi RD}

This is the Jacobi Reverse Diagonal iteration, which is the same as Jacobi FD but the process is reversed.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Symmetric, T>::JacobiMethodRD}}]
T* JacobiMethodRD(T error, unsigned int& num /**/) {
	bool run = true;
	num = 0;
	while(run) {
		run = false;
		// Run through the diagonals
		for(unsigned int d = this->_n-1, r, p = 1; d; d--, p++) {   
			for(r = 0; r < p; r++) { // Run through the rows
				if(error < abs(this->matrix[d][r])) {
					run = true;
					Rotate(r, d, num);
				}
			}
		}
	}
	return this->matrix[0];
}
\end{lstlisting}

\subsubsubsection{Jacobi FC}

This is the Jacobi Forward Column iteration, which starts at the lowest column number and does the Jacobi transformation on all it's elements before moving on to the next. Otherwise similar behavior to Jacobi FD.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Symmetric, T>::JacobiMethodFC}}]
T* JacobiMethodFC(T error, unsigned int& num /**/) {
	bool run;
	num = 0;
	do {
		run = false;
		for(unsigned int r = 0, d, n = this->_n; r < this->_n; r++, n--) {
			for(d = 1; d < n; d++) {
				if(error < abs(this->matrix[d][r])) {
					run = true;
					Rotate(r, d, num);
				}
			}
		}
	} while(run);
	return this->matrix[0];
}
\end{lstlisting}

\subsubsubsection{Jacobi RC}

This is the Jacobi Reverse Column iteration, which is the reverse of Jacobi FC.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Symmetric, T>::JacobiMethodRC}}]
T* JacobiMethodRC(T error, unsigned int& num /**/) {
	bool run;
	num = 0;
	do {
		run = false;
		for(unsigned int r = 0, d, n = this->_n-1; r < this->_n; r++, n--) {
			for(d = n; d; d--) {
				if(error < abs(this->matrix[d][r])) {
					run = true;
					Rotate(r, d, num);
				}
			}
		}
	} while(run);
	return this->matrix[0];
}
\end{lstlisting}

\subsubsubsection{Jacobi FR}

This is the Jacobi Forward Row iteration, which starts at the lowest row number and does the Jacobi transformation on all it's elements before moving on to the next.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Symmetric, T>::JacobiMethodFR}}]
T* JacobiMethodFR(T error, unsigned int& num /**/) {
	bool run;
	num = 0;                                                        // To be removed
	do {
		run = false;
		for(unsigned int r, d = 1, c; d < this->_n; d++) {
			for(r = 0, c = d; c; r++, c--) {
				if(error < abs(this->matrix[c][r])) {
					run = true;
					Rotate(r, c, num);
				}
			}
		}
	} while(run);
	return this->matrix[0];
}
\end{lstlisting}

\subsubsubsection{Jacobi RR}

This is the Jacobi Reverse Row iteration, which is the reverse of Jacobi FR.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Symmetric, T>::JacobiMethodRR}}]
T* JacobiMethodRR(T error, unsigned int& num /**/) {
	bool run;
	num = 0;
	do {
		run = false;
		for(unsigned int r, d = 1, c; d < this->_n; d++) {
			for(r = d-1, c = 1; c <= d; r--, c++) {
				if(error < abs(this->matrix[c][r])) {
					run = true;
					Rotate(r, c, num);
				}
			}
		}
	} while(run);
	return this->matrix[0];
}
\end{lstlisting}

\subsection{Francis QR algorithm}

This algorithm is designed for solving a symmetric tridiagonal matrix, and is solved as described in section \ref{sec_QR}. Surprisingly this algorithm is quit slow, which is due to the fact that it needs $n_{\text{step}}-1$ rotation transformation for each QR transformation, which you can see in \reftab{tab_num}. \linebreak

However closer investigation showed that the QR algorithm gave vary big difference in absolute value of the off-diagonal elements. Many elements became small very fast, whereas others didn't. To speed up the algorithm I introduced a if check that determines whether the off-diagonal element is already smaller than a given value before doing the rotation transformation on it. If the off-diagonal element in question is smaller than the given value, it skips the rotation transformation. This reduces the number of rotation transformation that is needed to preform the QR algorithm significantly, rough estimates indicated a speed up of 25 times at tridiagonal matrix of dimensionality 100. \linebreak

The first and last rotation in the QR transformation needs to be handled a little bit differently than the other rotation transformations, to be able to handle the corners of the matrix. Therefore the QR algorithm that I made has three section that does almost the same thing, but handles the three different stages of the QR transformation, namely first, intermediate and last rotation. This to avoid if checks that would be run frequently.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::TridiagonalSymmetric, T>::QRalgorithm}}]
T* QRalgorithm(T error, unsigned int& num) {
	unsigned int i, j, n = this->_n-2;
	T c, s, _c, _s, temp, temp2, max, a, b;

	num = 0;
	do {
		// First rotation in QR transformation
		max = 0;
		a = this->a[0];
		b = this->b[0];

		temp = sqrt((T)1/(a*a+b*b));	// Calculate sin and cos
		c = b*temp;
		s = a*temp;
		
		temp = a*s;                  	// Eigenvalues
		this->b[0] = b*c*c + this->b[1]*s*s + 2*temp*c;
		this->b[1] = this->b[1]*c - temp;
		
		num++;
		
		// The intermediate rotations in QR transformation
		i = 1;
		while(i < n) {
			if(abs(this->a[i]) > error) {	// Offdiagonal element still to large?

				_c = c;
				_s = s;
				a = this->a[i];
				b = this->b[i];
				
				// Calculate sin and cos
				temp = (T)1/sqrt(a*a + b*b);	
				c = b*temp;
				s = a*temp;
				
				temp = a*s;		// Eigenvalues
				b = b*c + temp;
				this->a[i-1] = b*_s;
				temp2 = abs(this->a[i-1]);
				if(max < temp2)
					max = temp2;
				temp *= _c;
				this->b[i] = b*_c*c + temp*c + this->b[++i]*s*s;
				this->b[i] = this->b[i]*c - temp;
				
				num++;
			
			} else {    // Offdiagonal element less then error
				if(s) { // If previous sin not zero, then update elements
					
					_c = c;
					_s = s;
					
					this->a[i-1] = this->b[i]*_s;   // Eigenvalues
					this->b[i] = this->b[i]*_c;
					temp2 = abs(this->a[i-1]);
					if(max < temp2)
						max = temp2;
					
					c = 1;
					s = 0;
				}
				i++;
			}
		}
		
		// Last rotation in QR transformation
		a = this->a[i];
		b = this->b[i];
		
		temp = (T)1/sqrt(a*a + b*b);    // Calculate sin and cos
		_c = b*temp;
		_s = a*temp;
		
		temp = a*_s;	// Eigenvalues
		b = b*_c + temp;
		this->a[i-1] = b*s;
		temp2 = abs(this->a[i-1]);
		if(max < temp2)
			max = temp2;
		temp *= c;
		this->b[i] = b*_c*c + temp*_c + this->b[++i]*_s*_s;
		b = this->b[i]*_c - temp;
		this->a[i-1] = b*_s;
		temp2 = abs(this->a[i-1]);
		if(max < temp2)
			max = temp2;
		this->b[i] = b*_c;
		
		num++;

	} while(max > error);

    return this->b;
}
\end{lstlisting}

\subsection{Eigenvectors}

Here I will present the numerical implementation of finding the eigenvectors as discussed in section \ref{sec_Eigenvectors}. This method is what I have used to calculate the eigenvectors together with the QR algorithm for finding the eigenvalues.

\begin{lstlisting}[title={\texttt{Matrix<MatrixType::Tridiagonal\_m1\_X\_m1, T>::Eigenvector}}]
T* Eigenvector(T value) {
	T temp = 1, temp2 = this->b[0] - value;
	this->b[0] = 1;
	for(unsigned int i = 1; i < this->_n; i++) {
		temp  *= temp2;
		temp2 = this->b[i] - value - (T)1/temp2;
		this->b[i] = temp;
	}
	return this->b;
}
\end{lstlisting}

This algorithm calculates only one eigenvector for a given eigenvalue.

\newpage
\section{Results}

\subsection{Comparing different solvers}\label{sec_compare_solvers}

\begin{tabell}{|r|c|c|c|c|c|c|c|c|c|c|}{\scriptsize}{Number & tqli & QR & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi\\ & (lib.cpp) & & (lecture) &  & (FD) & (RD) & (FC) & (RC) & (FR) & (RR)\\}{\input{build-project2-Desktop-Debug/time.dat}}{Time used in seconds to solve the different eigenvalue and eigenvector solvers. The Jacobi solvers and QR algorithm terminates when off-diagonal elements are less than $10^{-6}$. Used the case of single electron and $\rho_{\mathrm{max}}=10$.} {tab_speed}
\end{tabell} 

\begin{tabell}{|r|c|c|c|c|c|c|c|c|c|}{\small}{Number & QR & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi\\ & & (lecture) &  & (FD) & (RD) & (FC) & (RC) & (FR) & (RR)\\}{\input{build-project2-Desktop-Debug/error.dat}}{Order of relative error to result of eigenvalues to the tqli in lib.cpp. The Jacobi solvers and QR algorithm terminates when off-diagonal elements are less than $10^{-6}$. Used the case of single electron and $\rho_{\mathrm{max}}=10$.}{tab_error}
\end{tabell}

\begin{tabell}{|r|c|c|c|c|c|c|c|c|c|}{\small}{Number & QR & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi & Jacobi\\ & & (lecture) &  & (FD) & (RD) & (FC) & (RC) & (FR) & (RR)\\}{\input{build-project2-Desktop-Debug/numberoftransformations.dat}}{Number of rotations. The Jacobi solvers and QR algorithm terminates when off-diagonal elements are less than $10^{-6}$. Used the case of single electron and $\rho_{\mathrm{max}}=10$.}{tab_num}
\end{tabell}

\newpage
\subsection{Single electron harmonic oscillator}\label{sec_single_electron}

\begin{tabell}{|r|c|c|c|c|c|c|}{\small}{\input{build-project2-Desktop-Debug/error2header.dat}}{\input{build-project2-Desktop-Debug/errorQR.dat}}{Order of relative error of the three lower eigenvalues with $\ell = 0$ for a single electron trapped in a harmonic oscillator well. The relative error is compared to the exact eigenvalues  $\lambda_0 = 3$, $\lambda_1 = 7$ and $\lambda_2 = 11$. Solved with my QR algorithm with off-diagonal elements are less than $10^{-6}$.}{tab_one_electron}
\end{tabell}

\begin{tabell}{|r|c|c|c|c|c|c|}{\small}{\input{build-project2-Desktop-Debug/error2header.dat}}{\input{build-project2-Desktop-Debug/errortqli.dat}}{Order of relative error of the three lower eigenvalues with $\ell = 0$ for a single electron trapped in a harmonic oscillator well. The relative error is compared to the exact eigenvalues  $\lambda_0 = 3$, $\lambda_1 = 7$ and $\lambda_2 = 11$. Solved with my tqli in lib.cpp}{tab_one_electron_2}
\end{tabell}

\subsection{Two electrons harmonic oscillator with repulsive Coulomb interaction}\label{sec_two electrons}

\begin{tabell}{|r|c|c|c|c|c|c|c|}{\small}{\input{build-project2-Desktop-Debug/error2elheader.dat}}{\input{build-project2-Desktop-Debug/errorQRw0.dat}}{Order of relative error of the ground state with $\ell = 0$ and $\omega_r = 0.01$ for two electrons trapped in a harmonic oscillator well with repulsive Coulomb interaction. The relative error is compared to the exact eigenvalues  $\lambda_0 = 0.105041$. Solved with my QR algorithm with off-diagonal elements less than $10^{-6}$.}{tab_two_electron_w0}
\end{tabell}

\begin{tabell}{|r|c|c|c|c|c|c|c|}{\small}{\input{build-project2-Desktop-Debug/error2elheader.dat}}{\input{build-project2-Desktop-Debug/errorQRw1.dat}}{Order of relative error of the ground state with $\ell = 0$ and $\omega_r = 0.5$ for two electrons trapped in a harmonic oscillator well with repulsive Coulomb interaction. The relative error is compared to the exact eigenvalues  $\lambda_0 = 2.05658$. Solved with my QR algorithm with off-diagonal elements less than $10^{-6}$.}{tab_two_electron_w1}
\end{tabell}

\begin{tabell}{|r|c|c|c|c|c|c|c|}{\small}{\input{build-project2-Desktop-Debug/error2elheader.dat}}{\input{build-project2-Desktop-Debug/errorQRw2.dat}}{Order of relative error of the ground state with $\ell = 0$ and $\omega_r = 1$ for two electrons trapped in a harmonic oscillator well with repulsive Coulomb interaction. The relative error is compared to the exact eigenvalues  $\lambda_0 = 3.62193$. Solved with my QR algorithm with off-diagonal elements less than $10^{-6}$.}{tab_two_electron_w2}
\end{tabell}

\begin{tabell}{|r|c|c|c|c|c|c|c|}{\small}{\input{build-project2-Desktop-Debug/error2elheader.dat}}{\input{build-project2-Desktop-Debug/errorQRw3.dat}}{Order of relative error of the ground state with $\ell = 0$ and $\omega_r = 5$ for two electrons trapped in a harmonic oscillator well with repulsive Coulomb interaction. The relative error is compared to the exact eigenvalues  $\lambda_0 = 14.1863$. Solved with my QR algorithm with off-diagonal elements less than $10^{-6}$.}{tab_two_electron_w3}
\end{tabell}

\figur{0.8}{Exact.eps}{The normalize exact solution of the wave function for two electrons in a harmonic oscillator well with Coulomb repulsion \eqref{eq_7}.}{fig_exact} 

\figur{0.8}{QR.eps}{The normalize solution of the wave function from my own eigenvector algorithm for two electrons in a harmonic oscillator well with Coulomb repulsion, where Francis QR algorithm has been used to find the eigenvalues.}{fig_QR} 

\figur{0.8}{tqli.eps}{The normalize solution of the wave function from tqli in lib.cpp for two electrons in a harmonic oscillator well with Coulomb repulsion, where Francis QR algorithm has been used to find the eigenvalues.}{fig_tqli} 

\newpage

\section{Conclusion}

In this report I have shown that there are many different methods for solving a eigenvalue problem for a matrix, and the different solvers I have compared in section \ref{sec_compare_solvers}. \reftab{tab_speed} show big difference in computation time for different Jacobi solvers where we iterate over the off-diagonal elements in different ways. We see that both forward and reverse diagonal Jacobi method is very efficient to solve the eigenvalue problems in this report, which is 10 times faster than the standard Jacobi method where you search for the maximum absolute value off diagonal element. But \reftab{tab_speed} also show that certain iteration schemes for the Jacobi method is very inefficient, like the forward row and column iteration. All the solvers seems to go with $\mathcal{O}\p{n^3}$ except the QR algorithm that seems to go with $\mathcal{O}\p{n^2}$. \linebreak

One should note that the \texttt{tqli} function calculates the eigenvectors as well, whereas the other solvers does not. This puts the \texttt{tqli}  at a disadvantage in the speed comparison in \reftab{tab_speed}, because the eigenvector calculation is quite expensive. However the implementation of the eigenvector algorithm that I made in \label{sec_Eigenvectors} that calculates the eigenvectors after the eigenvalue has been found, is very speed efficient and does not impact the results in \reftab{tab_speed}. Though this eigenvector algorithm is not as numerical stable as the implementation in \texttt{tqli}, where you get a good approximation of the eigenvector at certain areas of the eigenvector. But taking care and guidelines as depicted in \label{sec_Eigenvectors} one can still get very good results with my eigenvector algorithm, which \reffig{fig_exact}, \reffig{fig_QR} and \reffig{fig_tqli} shows. So my eigenvector algorithm with the QR algorithm that I made enables one to study heavy eigenvalue problems in matter of few minutes than rather of hours, which may be the difference between you getting insight to the problem or not, because you can run many more runs with a fast algorithm. \linebreak

An unanswered question in this report is the results in \reftab{tab_error} where all the solvers I made seems to diverge by the same amount relative to the $\texttt{tqli}$ with higher numbers of steps. And decreasing the maximum absolute value of the off-diagonal elements did not seem to effect the results, which seems to indicate that the error is $\texttt{tqli}$. However \reftab{tab_one_electron}, \reftab{tab_one_electron_2}, \reftab{tab_two_electron_w0}, \reftab{tab_two_electron_w1}, \reftab{tab_two_electron_w2} and \reftab{tab_two_electron_w3} shows no significant difference in accuracy to known eigenvalues. The difference is that  \reftab{tab_error} compares all the eigenvalues and shows the largest difference, where as the other results shows only for few specific eigenvalues. \linebreak

The algorithms that I made in this report terminates when the off-diagonal elements are less than a certain absolute value. This is however not an obvious indication of the accuracy of the results, and therefore a better way to input the desired accuracy of the solution would be beneficial.

\section{Attachments}

The files produced in working with this project can be found at \href{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%202}{https://github.com/Eimund/UiO/tree/master/FYS4150/Project\%202} \linebreak

The source files developed are

\begin{enumerate}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%202/project2/Matrix.h}{Matrix.h}}
\item{\href{https://github.com/Eimund/UiO/blob/master/FYS4150/Project\%202/project2/project2.cpp}{project2.cpp}}
\end{enumerate}

\newpage

\section{Resources}

\begin{enumerate}
\item{\href{http://qt-project.org/downloads}{QT Creator 5.3.1 with C11}}
\item{\href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/Programs\%20\%20(as\%20tar.gz\%20file,\%20all\%20languages)/programs.tar.gz}{Morten Hjorth-Jensen's \texttt{lib.cpp}}}
\item{\href{https://www.eclipse.org/downloads/}{Eclipse Standard/SDK  - Version: Luna Release (4.4.0) with PyDev for Python}}
\item{\href{http://www.ubuntu.com/download/desktop}{Ubuntu 14.04.1 LTS}}
\item{\href{http://shop.lenovo.com/no/en/laptops/thinkpad/w-series/w540/#tab-reseller}{ThinkPad W540 P/N: 20BG0042MN with 32 GB RAM}}
\end{enumerate}

\begin{thebibliography}{9}
\bibitem{project1}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/projects/project-2-deadline-september-22/project2_2014.pdf}{\emph{FYS4150 - Project 2}} - \emph{Schr\"{o}dinger's equation for two electrons in a three-dimensional harmonic oscillator well}, \href{http://www.uio.no}{University of Oslo}, 2014} 
\bibitem{lecture}{\href{mailto:morten.hjorth-jensen@fys.uio.no}{Morten Hjorth-Jensen}, \href{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h14/undervisningsmateriale/Lecture\%20Notes/lecture2014.pdf}{\emph{Computational Physics - Lecture Notes Fall 2014}}, \href{http://www.uio.no}{University of Oslo}, 2014}  
\bibitem{TwoElectron}{M. Taut, \emph{\href{http://prola.aps.org/abstract/PRA/v48/i5/p3561\_1}{Two electrons in an external oscillator potential:} Particular analytic solutions of a Coulomb correlation problem}, Physical Review A Volume 48 Number 5, 1993}
\bibitem{Griffiths}{David J. Griffiths, \emph{\href{http://www.amazon.com/Introduction-Quantum-Mechanics-David-Griffiths/dp/0131118927/ref=sr_1_1?ie=UTF8&qid=1412067343&sr=8-1&keywords=introduction+to+quantum+mechanics+griffiths}{Introduction to Quantum Mechanics}}, 2.ed, Pearson Education Inc., 2005}
\bibitem{Eigenvaule}{\href{http://en.wikipedia.org/wiki/Eigenvalues\_and\_eigenvectors}{http://en.wikipedia.org/wiki/Eigenvalues\_and\_eigenvectors}}
\bibitem{JacobiMethod}{\href{http://en.wikipedia.org/wiki/Jacobi\_eigenvalue\_algorithm}{http://en.wikipedia.org/wiki/Jacobi\_eigenvalue\_algorithm}}
\bibitem{MatrixSimularity}{\href{http://en.wikipedia.org/wiki/Matrix\_similarity}{http://en.wikipedia.org/wiki/Matrix\_similarity}}
\bibitem{RotationMatrix}{\href{http://en.wikipedia.org/wiki/Rotation\_matrix}{http://en.wikipedia.org/wiki/Rotation\_matrix}}
\bibitem{OrthogonalMatrix}{\href{http://en.wikipedia.org/wiki/Orthogonal\_matrix}{http://en.wikipedia.org/wiki/Orthogonal\_matrix}}
\bibitem{PlancksKonstant}{\href{http://en.wikipedia.org/wiki/Planck\_constant}{http://en.wikipedia.org/wiki/Planck\_constant}}
\bibitem{Electron}{\href{http://en.wikipedia.org/wiki/Electron}{http://en.wikipedia.org/wiki/Electron}}
\bibitem{QR}{\href{http://en.wikipedia.org/wiki/QR\_algorithm}{http://en.wikipedia.org/wiki/QR\_algorithm}}
\bibitem{Gaussian_elimination}{\href{http://en.wikipedia.org/wiki/Gaussian\_elimination}{http://en.wikipedia.org/wiki/Gaussian\_elimination}}
\end{thebibliography}

\end{flushleft}
\end{document}
